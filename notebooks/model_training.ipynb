{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from model.dataset import OpenCloseDataset\n",
    "from model.gnn_model import GCN, GATv2\n",
    "import os\n",
    "from model.utils import train, device, train_epoch, eval_epoch, cross_val\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import lr_scheduler\n",
    "from torch_geometric.loader import DataLoader\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import global_mean_pool, GCNConv, GATConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "96"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset = OpenCloseDataset(datafolder='../data', reload=False, k_degree=10).shuffle()\n",
    "len(os.listdir('../data/processed'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "Data(x=[420, 420], edge_index=[2, 5924], edge_attr=[5924], y=0)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[  0,   0,   0,  ..., 419, 419, 419],\n        [  1,   2,   3,  ..., 406, 407, 418]])"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset[0].edge_index"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "Data(x=[420, 420], edge_index=[2, 5966], edge_attr=[5966], y=1)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[  0,   0,   0,  ..., 419, 419, 419],\n        [  1,   2,   3,  ..., 406, 407, 418]])"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset[0].edge_index"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "train_size = int(0.94 * len(full_dataset))\n",
    "train_dataset, val_dataset = full_dataset[:train_size], full_dataset[train_size:]\n",
    "#val_dataset, test_dataset = val_dataset[:-3], val_dataset[-3:]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-03.\n"
     ]
    },
    {
     "data": {
      "text/plain": "116858"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GCN(full_dataset.num_features, channels=[256, 32, 8], dropout=0.1).to(device())\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "epochs = 40\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=epochs//3, gamma=0.1, last_epoch=-1, verbose=True)\n",
    "sum(p.numel() for p in model.parameters())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/40 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6513b0d6610a413481396ee5f750ca51"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Loss: 0.0235, Test Loss 0.1189, Train Acc: 0.5114, Test Acc: 0.3333\n",
      "Epoch: 002, Train Loss: 0.0234, Test Loss 0.1184, Train Acc: 0.5114, Test Acc: 0.3333\n",
      "Epoch: 003, Train Loss: 0.0231, Test Loss 0.1172, Train Acc: 0.5341, Test Acc: 0.3333\n",
      "Epoch: 004, Train Loss: 0.0229, Test Loss 0.1209, Train Acc: 0.5227, Test Acc: 0.3333\n",
      "Epoch: 005, Train Loss: 0.0225, Test Loss 0.1203, Train Acc: 0.5682, Test Acc: 0.3333\n",
      "Epoch: 006, Train Loss: 0.0220, Test Loss 0.1181, Train Acc: 0.7500, Test Acc: 0.3333\n",
      "Epoch: 007, Train Loss: 0.0213, Test Loss 0.1291, Train Acc: 0.6591, Test Acc: 0.3333\n",
      "Epoch: 008, Train Loss: 0.0205, Test Loss 0.1313, Train Acc: 0.7727, Test Acc: 0.3333\n",
      "Epoch: 009, Train Loss: 0.0200, Test Loss 0.1192, Train Acc: 0.9205, Test Acc: 0.5000\n",
      "Epoch: 010, Train Loss: 0.0189, Test Loss 0.1423, Train Acc: 0.7159, Test Acc: 0.3333\n",
      "Epoch: 011, Train Loss: 0.0180, Test Loss 0.1295, Train Acc: 0.9659, Test Acc: 0.3333\n",
      "Epoch: 012, Train Loss: 0.0166, Test Loss 0.1285, Train Acc: 0.9659, Test Acc: 0.3333\n",
      "Epoch: 013, Train Loss: 0.0154, Test Loss 0.1230, Train Acc: 0.9545, Test Acc: 0.5000\n",
      "Epoch: 014, Train Loss: 0.0139, Test Loss 0.1391, Train Acc: 0.9659, Test Acc: 0.3333\n",
      "Epoch: 015, Train Loss: 0.0129, Test Loss 0.1293, Train Acc: 0.9886, Test Acc: 0.3333\n",
      "Epoch: 016, Train Loss: 0.0121, Test Loss 0.1644, Train Acc: 0.9773, Test Acc: 0.1667\n",
      "Epoch: 017, Train Loss: 0.0111, Test Loss 0.1354, Train Acc: 1.0000, Test Acc: 0.3333\n",
      "Epoch: 018, Train Loss: 0.0097, Test Loss 0.1932, Train Acc: 1.0000, Test Acc: 0.1667\n",
      "Epoch: 019, Train Loss: 0.0080, Test Loss 0.1547, Train Acc: 1.0000, Test Acc: 0.3333\n",
      "Epoch: 020, Train Loss: 0.0074, Test Loss 0.1612, Train Acc: 1.0000, Test Acc: 0.3333\n",
      "Epoch: 021, Train Loss: 0.0066, Test Loss 0.1898, Train Acc: 1.0000, Test Acc: 0.3333\n",
      "Epoch: 022, Train Loss: 0.0056, Test Loss 0.1920, Train Acc: 1.0000, Test Acc: 0.1667\n",
      "Epoch: 023, Train Loss: 0.0049, Test Loss 0.1924, Train Acc: 1.0000, Test Acc: 0.1667\n",
      "Epoch: 024, Train Loss: 0.0044, Test Loss 0.1990, Train Acc: 1.0000, Test Acc: 0.1667\n",
      "Epoch: 025, Train Loss: 0.0040, Test Loss 0.2122, Train Acc: 1.0000, Test Acc: 0.1667\n",
      "Epoch: 026, Train Loss: 0.0033, Test Loss 0.2168, Train Acc: 1.0000, Test Acc: 0.3333\n",
      "Epoch: 027, Train Loss: 0.0028, Test Loss 0.2180, Train Acc: 1.0000, Test Acc: 0.3333\n",
      "Epoch: 028, Train Loss: 0.0027, Test Loss 0.2590, Train Acc: 1.0000, Test Acc: 0.1667\n",
      "Epoch: 029, Train Loss: 0.0022, Test Loss 0.2562, Train Acc: 1.0000, Test Acc: 0.1667\n",
      "Epoch: 030, Train Loss: 0.0019, Test Loss 0.2318, Train Acc: 1.0000, Test Acc: 0.3333\n",
      "Epoch: 031, Train Loss: 0.0017, Test Loss 0.2434, Train Acc: 1.0000, Test Acc: 0.3333\n",
      "Epoch: 032, Train Loss: 0.0017, Test Loss 0.2822, Train Acc: 1.0000, Test Acc: 0.1667\n",
      "Epoch: 033, Train Loss: 0.0011, Test Loss 0.2642, Train Acc: 1.0000, Test Acc: 0.3333\n",
      "Epoch: 034, Train Loss: 0.0011, Test Loss 0.2547, Train Acc: 1.0000, Test Acc: 0.3333\n",
      "Epoch: 035, Train Loss: 0.0011, Test Loss 0.2907, Train Acc: 1.0000, Test Acc: 0.1667\n",
      "Epoch: 036, Train Loss: 0.0010, Test Loss 0.2978, Train Acc: 1.0000, Test Acc: 0.1667\n",
      "Epoch: 037, Train Loss: 0.0009, Test Loss 0.2713, Train Acc: 1.0000, Test Acc: 0.3333\n",
      "Epoch: 038, Train Loss: 0.0008, Test Loss 0.2937, Train Acc: 1.0000, Test Acc: 0.3333\n",
      "Epoch: 039, Train Loss: 0.0008, Test Loss 0.3082, Train Acc: 1.0000, Test Acc: 0.1667\n",
      "Epoch: 040, Train Loss: 0.0006, Test Loss 0.2879, Train Acc: 1.0000, Test Acc: 0.3333\n"
     ]
    }
   ],
   "source": [
    "history = train(model, epochs, train_loader, val_loader, loss, optimizer, scheduler=None)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loss, val_loss = zip(*history)\n",
    "# k = None no dropuot\n",
    "plt.plot(train_loss)\n",
    "plt.plot(val_loss);"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "220722"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GATv2(full_dataset.num_features, 128, 8).to(device())\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "epochs = 40\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=epochs//4, gamma=0.1, last_epoch=-1, verbose=False)\n",
    "sum(p.numel() for p in model.parameters())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/40 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dd684b0d9b454d3daaa118d4d64b114b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Loss: 0.0235, Test Loss 0.1103, Train Acc: 0.4886, Test Acc: 0.6667\n",
      "Epoch: 002, Train Loss: 0.0226, Test Loss 0.1197, Train Acc: 0.6591, Test Acc: 0.3333\n",
      "Epoch: 003, Train Loss: 0.0217, Test Loss 0.1306, Train Acc: 0.6477, Test Acc: 0.3333\n",
      "Epoch: 004, Train Loss: 0.0205, Test Loss 0.1211, Train Acc: 0.7159, Test Acc: 0.5000\n",
      "Epoch: 005, Train Loss: 0.0209, Test Loss 0.1129, Train Acc: 0.5795, Test Acc: 0.6667\n",
      "Epoch: 006, Train Loss: 0.0171, Test Loss 0.1398, Train Acc: 0.8750, Test Acc: 0.3333\n",
      "Epoch: 007, Train Loss: 0.0167, Test Loss 0.1238, Train Acc: 0.7386, Test Acc: 0.5000\n",
      "Epoch: 008, Train Loss: 0.0152, Test Loss 0.1208, Train Acc: 0.7955, Test Acc: 0.5000\n",
      "Epoch: 009, Train Loss: 0.0124, Test Loss 0.1614, Train Acc: 0.9205, Test Acc: 0.3333\n",
      "Epoch: 010, Train Loss: 0.0105, Test Loss 0.1592, Train Acc: 0.9659, Test Acc: 0.5000\n",
      "Epoch: 011, Train Loss: 0.0084, Test Loss 0.1703, Train Acc: 0.9886, Test Acc: 0.5000\n",
      "Epoch: 012, Train Loss: 0.0071, Test Loss 0.1517, Train Acc: 0.9659, Test Acc: 0.6667\n",
      "Epoch: 013, Train Loss: 0.0045, Test Loss 0.1826, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 014, Train Loss: 0.0032, Test Loss 0.2240, Train Acc: 1.0000, Test Acc: 0.5000\n",
      "Epoch: 015, Train Loss: 0.0028, Test Loss 0.1990, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 016, Train Loss: 0.0019, Test Loss 0.2210, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 017, Train Loss: 0.0016, Test Loss 0.2919, Train Acc: 1.0000, Test Acc: 0.5000\n",
      "Epoch: 018, Train Loss: 0.0019, Test Loss 0.3517, Train Acc: 1.0000, Test Acc: 0.5000\n",
      "Epoch: 019, Train Loss: 0.0008, Test Loss 0.3261, Train Acc: 1.0000, Test Acc: 0.5000\n",
      "Epoch: 020, Train Loss: 0.0007, Test Loss 0.2985, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 021, Train Loss: 0.0005, Test Loss 0.3158, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 022, Train Loss: 0.0005, Test Loss 0.3789, Train Acc: 1.0000, Test Acc: 0.5000\n",
      "Epoch: 023, Train Loss: 0.0013, Test Loss 0.4599, Train Acc: 1.0000, Test Acc: 0.5000\n",
      "Epoch: 024, Train Loss: 0.0003, Test Loss 0.4063, Train Acc: 1.0000, Test Acc: 0.5000\n",
      "Epoch: 025, Train Loss: 0.0004, Test Loss 0.3520, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 026, Train Loss: 0.0003, Test Loss 0.3682, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 027, Train Loss: 0.0002, Test Loss 0.4074, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 028, Train Loss: 0.0002, Test Loss 0.4326, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 029, Train Loss: 0.0001, Test Loss 0.4364, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 030, Train Loss: 0.0001, Test Loss 0.4230, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 031, Train Loss: 0.0002, Test Loss 0.4087, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 032, Train Loss: 0.0002, Test Loss 0.4036, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 033, Train Loss: 0.0001, Test Loss 0.4097, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 034, Train Loss: 0.0001, Test Loss 0.4259, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 035, Train Loss: 0.0001, Test Loss 0.4392, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 036, Train Loss: 0.0001, Test Loss 0.4370, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 037, Train Loss: 0.0001, Test Loss 0.4389, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 038, Train Loss: 0.0001, Test Loss 0.4398, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 039, Train Loss: 0.0001, Test Loss 0.4334, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 040, Train Loss: 0.0001, Test Loss 0.4350, Train Acc: 1.0000, Test Acc: 0.6667\n"
     ]
    }
   ],
   "source": [
    "history = train(model, epochs, train_loader, val_loader, loss, optimizer, scheduler=None)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loss, val_loss = zip(*history)\n",
    "\n",
    "plt.plot(train_loss)\n",
    "plt.plot(val_loss);"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '../weights/gat_2heads_128_8_knn30.pt')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7b301190b54244c4b410d148fbadcd6f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'lr'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_17000\\2619165759.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m cross_val(full_dataset, GCN, lr=0.001, step_size=50//4, gamma=0.1, last_epoch=-1, verbose=False,\n\u001B[1;32m----> 2\u001B[1;33m           num_features=full_dataset.num_features, channels=[256, 32, 8], dropout=0.3)\n\u001B[0m",
      "\u001B[1;32m~\\PycharmProjects\\Open_Close_GNN\\model\\utils.py\u001B[0m in \u001B[0;36mcross_val\u001B[1;34m(data, model_name, n_splits, epochs, batch_size, **kwargs)\u001B[0m\n\u001B[0;32m     69\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     70\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mn_fold\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mtrain_idx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtest_idx\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mtqdm\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0menumerate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mskf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlabels\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlabels\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 71\u001B[1;33m         \u001B[0mmodel\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel_name\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     72\u001B[0m         \u001B[0moptimizer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moptim\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mAdam\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mparameters\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     73\u001B[0m         \u001B[0mcriterion\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mCrossEntropyLoss\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: __init__() got an unexpected keyword argument 'lr'"
     ]
    }
   ],
   "source": [
    "cross_val(full_dataset, GCN, lr=0.001, step_size=50//4, gamma=0.1, last_epoch=-1, verbose=False,\n",
    "          num_features=full_dataset.num_features, channels=[256, 32, 8], dropout=0.3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e79c08e398d24141950bddd238dbc5f9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Train Loss: 0.0865, Test Loss 0.2112, Train Acc: 0.4545, Test Acc: 0.6667\n",
      "Epoch: 001, Train Loss: 0.0863, Test Loss 0.2283, Train Acc: 0.4886, Test Acc: 0.3333\n",
      "Epoch: 002, Train Loss: 0.0859, Test Loss 0.2202, Train Acc: 0.5114, Test Acc: 0.6667\n",
      "Epoch: 003, Train Loss: 0.0856, Test Loss 0.2265, Train Acc: 0.5227, Test Acc: 0.6667\n",
      "Epoch: 004, Train Loss: 0.0854, Test Loss 0.2065, Train Acc: 0.5568, Test Acc: 1.0000\n",
      "Epoch: 005, Train Loss: 0.0848, Test Loss 0.2246, Train Acc: 0.5455, Test Acc: 0.6667\n",
      "Epoch: 006, Train Loss: 0.0840, Test Loss 0.2156, Train Acc: 0.6136, Test Acc: 0.6667\n",
      "Epoch: 007, Train Loss: 0.0831, Test Loss 0.2139, Train Acc: 0.6364, Test Acc: 0.6667\n",
      "Epoch: 008, Train Loss: 0.0820, Test Loss 0.2101, Train Acc: 0.6477, Test Acc: 0.6667\n",
      "Epoch: 009, Train Loss: 0.0819, Test Loss 0.2072, Train Acc: 0.6364, Test Acc: 0.6667\n",
      "Epoch: 010, Train Loss: 0.0820, Test Loss 0.1909, Train Acc: 0.6136, Test Acc: 1.0000\n",
      "Epoch: 011, Train Loss: 0.0778, Test Loss 0.2059, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 012, Train Loss: 0.0778, Test Loss 0.1972, Train Acc: 0.7045, Test Acc: 0.6667\n",
      "Epoch: 013, Train Loss: 0.0768, Test Loss 0.2073, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 014, Train Loss: 0.0763, Test Loss 0.2080, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 015, Train Loss: 0.0753, Test Loss 0.2019, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 016, Train Loss: 0.0746, Test Loss 0.1985, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 017, Train Loss: 0.0740, Test Loss 0.2023, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 018, Train Loss: 0.0733, Test Loss 0.1936, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 019, Train Loss: 0.0727, Test Loss 0.1914, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 020, Train Loss: 0.0720, Test Loss 0.1976, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 021, Train Loss: 0.0717, Test Loss 0.2040, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 022, Train Loss: 0.0710, Test Loss 0.1873, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 023, Train Loss: 0.0704, Test Loss 0.1975, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 024, Train Loss: 0.0703, Test Loss 0.1954, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 025, Train Loss: 0.0702, Test Loss 0.1941, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 026, Train Loss: 0.0702, Test Loss 0.1934, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 027, Train Loss: 0.0701, Test Loss 0.1940, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 028, Train Loss: 0.0701, Test Loss 0.1928, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 029, Train Loss: 0.0700, Test Loss 0.1917, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 030, Train Loss: 0.0700, Test Loss 0.1913, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 031, Train Loss: 0.0699, Test Loss 0.1918, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 032, Train Loss: 0.0699, Test Loss 0.1906, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 033, Train Loss: 0.0698, Test Loss 0.1903, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 034, Train Loss: 0.0698, Test Loss 0.1894, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 035, Train Loss: 0.0697, Test Loss 0.1898, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 036, Train Loss: 0.0697, Test Loss 0.1898, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 037, Train Loss: 0.0697, Test Loss 0.1898, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 038, Train Loss: 0.0697, Test Loss 0.1899, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 039, Train Loss: 0.0697, Test Loss 0.1899, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 040, Train Loss: 0.0697, Test Loss 0.1899, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 041, Train Loss: 0.0697, Test Loss 0.1900, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 042, Train Loss: 0.0697, Test Loss 0.1900, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 043, Train Loss: 0.0697, Test Loss 0.1900, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 044, Train Loss: 0.0697, Test Loss 0.1899, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 045, Train Loss: 0.0697, Test Loss 0.1900, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 046, Train Loss: 0.0697, Test Loss 0.1899, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 047, Train Loss: 0.0697, Test Loss 0.1899, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 048, Train Loss: 0.0697, Test Loss 0.1899, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 049, Train Loss: 0.0697, Test Loss 0.1900, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 000, Train Loss: 0.0863, Test Loss 0.2153, Train Acc: 0.5000, Test Acc: 1.0000\n",
      "Epoch: 001, Train Loss: 0.0860, Test Loss 0.2284, Train Acc: 0.5455, Test Acc: 0.3333\n",
      "Epoch: 002, Train Loss: 0.0859, Test Loss 0.2244, Train Acc: 0.5682, Test Acc: 0.6667\n",
      "Epoch: 003, Train Loss: 0.0857, Test Loss 0.2200, Train Acc: 0.5455, Test Acc: 0.6667\n",
      "Epoch: 004, Train Loss: 0.0855, Test Loss 0.2143, Train Acc: 0.5455, Test Acc: 1.0000\n",
      "Epoch: 005, Train Loss: 0.0853, Test Loss 0.2111, Train Acc: 0.6023, Test Acc: 1.0000\n",
      "Epoch: 006, Train Loss: 0.0847, Test Loss 0.2223, Train Acc: 0.6023, Test Acc: 0.6667\n",
      "Epoch: 007, Train Loss: 0.0842, Test Loss 0.2197, Train Acc: 0.6591, Test Acc: 0.6667\n",
      "Epoch: 008, Train Loss: 0.0838, Test Loss 0.2053, Train Acc: 0.6364, Test Acc: 1.0000\n",
      "Epoch: 009, Train Loss: 0.0824, Test Loss 0.2195, Train Acc: 0.6364, Test Acc: 0.6667\n",
      "Epoch: 010, Train Loss: 0.0825, Test Loss 0.1957, Train Acc: 0.6136, Test Acc: 1.0000\n",
      "Epoch: 011, Train Loss: 0.0801, Test Loss 0.2043, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 012, Train Loss: 0.0800, Test Loss 0.2017, Train Acc: 0.6818, Test Acc: 1.0000\n",
      "Epoch: 013, Train Loss: 0.0796, Test Loss 0.2037, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 014, Train Loss: 0.0793, Test Loss 0.2061, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 015, Train Loss: 0.0790, Test Loss 0.2098, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 016, Train Loss: 0.0784, Test Loss 0.2043, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 017, Train Loss: 0.0779, Test Loss 0.1996, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 018, Train Loss: 0.0775, Test Loss 0.2045, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 019, Train Loss: 0.0771, Test Loss 0.2047, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 020, Train Loss: 0.0767, Test Loss 0.2005, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 021, Train Loss: 0.0763, Test Loss 0.2012, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 022, Train Loss: 0.0765, Test Loss 0.1869, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 023, Train Loss: 0.0756, Test Loss 0.2018, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 024, Train Loss: 0.0755, Test Loss 0.2016, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 025, Train Loss: 0.0755, Test Loss 0.2011, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 026, Train Loss: 0.0754, Test Loss 0.1998, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 027, Train Loss: 0.0754, Test Loss 0.1999, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 028, Train Loss: 0.0753, Test Loss 0.1993, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 029, Train Loss: 0.0753, Test Loss 0.1992, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 030, Train Loss: 0.0752, Test Loss 0.1978, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 031, Train Loss: 0.0752, Test Loss 0.1982, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 032, Train Loss: 0.0752, Test Loss 0.1984, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 033, Train Loss: 0.0751, Test Loss 0.1975, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 034, Train Loss: 0.0751, Test Loss 0.1986, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 035, Train Loss: 0.0750, Test Loss 0.1981, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 036, Train Loss: 0.0750, Test Loss 0.1983, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 037, Train Loss: 0.0750, Test Loss 0.1982, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 038, Train Loss: 0.0750, Test Loss 0.1982, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 039, Train Loss: 0.0750, Test Loss 0.1982, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 040, Train Loss: 0.0750, Test Loss 0.1982, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 041, Train Loss: 0.0750, Test Loss 0.1981, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 042, Train Loss: 0.0750, Test Loss 0.1981, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 043, Train Loss: 0.0750, Test Loss 0.1981, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 044, Train Loss: 0.0750, Test Loss 0.1981, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 045, Train Loss: 0.0750, Test Loss 0.1981, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 046, Train Loss: 0.0750, Test Loss 0.1981, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 047, Train Loss: 0.0750, Test Loss 0.1980, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 048, Train Loss: 0.0750, Test Loss 0.1981, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 049, Train Loss: 0.0750, Test Loss 0.1980, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 000, Train Loss: 0.0869, Test Loss 0.2471, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 001, Train Loss: 0.0866, Test Loss 0.2323, Train Acc: 0.4432, Test Acc: 0.3333\n",
      "Epoch: 002, Train Loss: 0.0865, Test Loss 0.2306, Train Acc: 0.5568, Test Acc: 0.6667\n",
      "Epoch: 003, Train Loss: 0.0862, Test Loss 0.2260, Train Acc: 0.5114, Test Acc: 0.6667\n",
      "Epoch: 004, Train Loss: 0.0861, Test Loss 0.2290, Train Acc: 0.6364, Test Acc: 0.6667\n",
      "Epoch: 005, Train Loss: 0.0863, Test Loss 0.2177, Train Acc: 0.5114, Test Acc: 0.6667\n",
      "Epoch: 006, Train Loss: 0.0857, Test Loss 0.2294, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 007, Train Loss: 0.0851, Test Loss 0.2198, Train Acc: 0.5341, Test Acc: 0.6667\n",
      "Epoch: 008, Train Loss: 0.0844, Test Loss 0.2124, Train Acc: 0.5795, Test Acc: 1.0000\n",
      "Epoch: 009, Train Loss: 0.0841, Test Loss 0.2324, Train Acc: 0.6023, Test Acc: 0.3333\n",
      "Epoch: 010, Train Loss: 0.0852, Test Loss 0.2402, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 011, Train Loss: 0.0844, Test Loss 0.2322, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 012, Train Loss: 0.0843, Test Loss 0.2316, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 013, Train Loss: 0.0840, Test Loss 0.2297, Train Acc: 0.5114, Test Acc: 0.3333\n",
      "Epoch: 014, Train Loss: 0.0836, Test Loss 0.2273, Train Acc: 0.5227, Test Acc: 0.3333\n",
      "Epoch: 015, Train Loss: 0.0828, Test Loss 0.2249, Train Acc: 0.5227, Test Acc: 0.3333\n",
      "Epoch: 016, Train Loss: 0.0820, Test Loss 0.2288, Train Acc: 0.5455, Test Acc: 0.3333\n",
      "Epoch: 017, Train Loss: 0.0817, Test Loss 0.2318, Train Acc: 0.5455, Test Acc: 0.3333\n",
      "Epoch: 018, Train Loss: 0.0816, Test Loss 0.2173, Train Acc: 0.6818, Test Acc: 1.0000\n",
      "Epoch: 019, Train Loss: 0.0811, Test Loss 0.2178, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 020, Train Loss: 0.0805, Test Loss 0.2243, Train Acc: 0.6023, Test Acc: 0.3333\n",
      "Epoch: 021, Train Loss: 0.0802, Test Loss 0.2208, Train Acc: 0.6477, Test Acc: 0.3333\n",
      "Epoch: 022, Train Loss: 0.0799, Test Loss 0.2211, Train Acc: 0.6364, Test Acc: 0.3333\n",
      "Epoch: 023, Train Loss: 0.0798, Test Loss 0.2146, Train Acc: 0.6818, Test Acc: 0.6667\n",
      "Epoch: 024, Train Loss: 0.0797, Test Loss 0.2152, Train Acc: 0.6932, Test Acc: 0.6667\n",
      "Epoch: 025, Train Loss: 0.0796, Test Loss 0.2161, Train Acc: 0.6932, Test Acc: 0.6667\n",
      "Epoch: 026, Train Loss: 0.0796, Test Loss 0.2161, Train Acc: 0.6932, Test Acc: 0.6667\n",
      "Epoch: 027, Train Loss: 0.0795, Test Loss 0.2165, Train Acc: 0.6932, Test Acc: 0.6667\n",
      "Epoch: 028, Train Loss: 0.0795, Test Loss 0.2168, Train Acc: 0.6818, Test Acc: 0.6667\n",
      "Epoch: 029, Train Loss: 0.0794, Test Loss 0.2174, Train Acc: 0.6818, Test Acc: 0.6667\n",
      "Epoch: 030, Train Loss: 0.0794, Test Loss 0.2176, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 031, Train Loss: 0.0794, Test Loss 0.2173, Train Acc: 0.6818, Test Acc: 0.6667\n",
      "Epoch: 032, Train Loss: 0.0793, Test Loss 0.2172, Train Acc: 0.6818, Test Acc: 0.6667\n",
      "Epoch: 033, Train Loss: 0.0793, Test Loss 0.2175, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 034, Train Loss: 0.0793, Test Loss 0.2173, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 035, Train Loss: 0.0793, Test Loss 0.2175, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 036, Train Loss: 0.0793, Test Loss 0.2175, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 037, Train Loss: 0.0793, Test Loss 0.2175, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 038, Train Loss: 0.0793, Test Loss 0.2174, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 039, Train Loss: 0.0792, Test Loss 0.2174, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 040, Train Loss: 0.0792, Test Loss 0.2174, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 041, Train Loss: 0.0792, Test Loss 0.2174, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 042, Train Loss: 0.0792, Test Loss 0.2174, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 043, Train Loss: 0.0792, Test Loss 0.2174, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 044, Train Loss: 0.0792, Test Loss 0.2174, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 045, Train Loss: 0.0792, Test Loss 0.2174, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 046, Train Loss: 0.0792, Test Loss 0.2174, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 047, Train Loss: 0.0792, Test Loss 0.2173, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 048, Train Loss: 0.0792, Test Loss 0.2173, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 049, Train Loss: 0.0792, Test Loss 0.2173, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 000, Train Loss: 0.0867, Test Loss 0.2206, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 001, Train Loss: 0.0865, Test Loss 0.2273, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 002, Train Loss: 0.0863, Test Loss 0.2260, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 003, Train Loss: 0.0862, Test Loss 0.2211, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 004, Train Loss: 0.0860, Test Loss 0.2263, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 005, Train Loss: 0.0859, Test Loss 0.2191, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 006, Train Loss: 0.0863, Test Loss 0.2157, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 007, Train Loss: 0.0860, Test Loss 0.2184, Train Acc: 0.5114, Test Acc: 0.6667\n",
      "Epoch: 008, Train Loss: 0.0862, Test Loss 0.2302, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 009, Train Loss: 0.0857, Test Loss 0.2373, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 010, Train Loss: 0.0854, Test Loss 0.2252, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 011, Train Loss: 0.0853, Test Loss 0.2308, Train Acc: 0.6591, Test Acc: 0.6667\n",
      "Epoch: 012, Train Loss: 0.0851, Test Loss 0.2289, Train Acc: 0.6932, Test Acc: 0.6667\n",
      "Epoch: 013, Train Loss: 0.0847, Test Loss 0.2260, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 014, Train Loss: 0.0843, Test Loss 0.2246, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 015, Train Loss: 0.0839, Test Loss 0.2243, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 016, Train Loss: 0.0836, Test Loss 0.2198, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 017, Train Loss: 0.0833, Test Loss 0.2245, Train Acc: 0.6818, Test Acc: 0.6667\n",
      "Epoch: 018, Train Loss: 0.0830, Test Loss 0.2167, Train Acc: 0.7614, Test Acc: 1.0000\n",
      "Epoch: 019, Train Loss: 0.0828, Test Loss 0.2215, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 020, Train Loss: 0.0825, Test Loss 0.2209, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 021, Train Loss: 0.0823, Test Loss 0.2176, Train Acc: 0.7614, Test Acc: 1.0000\n",
      "Epoch: 022, Train Loss: 0.0821, Test Loss 0.2176, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 023, Train Loss: 0.0821, Test Loss 0.2079, Train Acc: 0.6818, Test Acc: 1.0000\n",
      "Epoch: 024, Train Loss: 0.0820, Test Loss 0.2085, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 025, Train Loss: 0.0819, Test Loss 0.2094, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 026, Train Loss: 0.0819, Test Loss 0.2102, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 027, Train Loss: 0.0818, Test Loss 0.2106, Train Acc: 0.7614, Test Acc: 1.0000\n",
      "Epoch: 028, Train Loss: 0.0818, Test Loss 0.2108, Train Acc: 0.7614, Test Acc: 1.0000\n",
      "Epoch: 029, Train Loss: 0.0818, Test Loss 0.2112, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 030, Train Loss: 0.0817, Test Loss 0.2117, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 031, Train Loss: 0.0817, Test Loss 0.2124, Train Acc: 0.8068, Test Acc: 1.0000\n",
      "Epoch: 032, Train Loss: 0.0817, Test Loss 0.2123, Train Acc: 0.8068, Test Acc: 1.0000\n",
      "Epoch: 033, Train Loss: 0.0817, Test Loss 0.2122, Train Acc: 0.8068, Test Acc: 1.0000\n",
      "Epoch: 034, Train Loss: 0.0816, Test Loss 0.2123, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 035, Train Loss: 0.0816, Test Loss 0.2130, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 036, Train Loss: 0.0816, Test Loss 0.2130, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 037, Train Loss: 0.0816, Test Loss 0.2130, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 038, Train Loss: 0.0816, Test Loss 0.2130, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 039, Train Loss: 0.0816, Test Loss 0.2130, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 040, Train Loss: 0.0816, Test Loss 0.2130, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 041, Train Loss: 0.0816, Test Loss 0.2130, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 042, Train Loss: 0.0816, Test Loss 0.2130, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 043, Train Loss: 0.0816, Test Loss 0.2130, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 044, Train Loss: 0.0816, Test Loss 0.2130, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 045, Train Loss: 0.0816, Test Loss 0.2130, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 046, Train Loss: 0.0816, Test Loss 0.2130, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 047, Train Loss: 0.0816, Test Loss 0.2130, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 048, Train Loss: 0.0816, Test Loss 0.2130, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 049, Train Loss: 0.0816, Test Loss 0.2130, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 000, Train Loss: 0.0864, Test Loss 0.2116, Train Acc: 0.5568, Test Acc: 1.0000\n",
      "Epoch: 001, Train Loss: 0.0860, Test Loss 0.2201, Train Acc: 0.5114, Test Acc: 0.6667\n",
      "Epoch: 002, Train Loss: 0.0861, Test Loss 0.2337, Train Acc: 0.5114, Test Acc: 0.3333\n",
      "Epoch: 003, Train Loss: 0.0854, Test Loss 0.2180, Train Acc: 0.5114, Test Acc: 0.6667\n",
      "Epoch: 004, Train Loss: 0.0852, Test Loss 0.2149, Train Acc: 0.5682, Test Acc: 0.6667\n",
      "Epoch: 005, Train Loss: 0.0847, Test Loss 0.2122, Train Acc: 0.5909, Test Acc: 1.0000\n",
      "Epoch: 006, Train Loss: 0.0845, Test Loss 0.2303, Train Acc: 0.5568, Test Acc: 0.3333\n",
      "Epoch: 007, Train Loss: 0.0836, Test Loss 0.2120, Train Acc: 0.6023, Test Acc: 0.6667\n",
      "Epoch: 008, Train Loss: 0.0832, Test Loss 0.2296, Train Acc: 0.5909, Test Acc: 0.3333\n",
      "Epoch: 009, Train Loss: 0.0820, Test Loss 0.2148, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 010, Train Loss: 0.0829, Test Loss 0.2136, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 011, Train Loss: 0.0822, Test Loss 0.2070, Train Acc: 0.6364, Test Acc: 1.0000\n",
      "Epoch: 012, Train Loss: 0.0820, Test Loss 0.2078, Train Acc: 0.6364, Test Acc: 0.6667\n",
      "Epoch: 013, Train Loss: 0.0818, Test Loss 0.2091, Train Acc: 0.6364, Test Acc: 0.6667\n",
      "Epoch: 014, Train Loss: 0.0815, Test Loss 0.2140, Train Acc: 0.7159, Test Acc: 0.6667\n",
      "Epoch: 015, Train Loss: 0.0813, Test Loss 0.2164, Train Acc: 0.7045, Test Acc: 0.6667\n",
      "Epoch: 016, Train Loss: 0.0811, Test Loss 0.2134, Train Acc: 0.7159, Test Acc: 0.6667\n",
      "Epoch: 017, Train Loss: 0.0809, Test Loss 0.2134, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 018, Train Loss: 0.0808, Test Loss 0.2163, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 019, Train Loss: 0.0805, Test Loss 0.2096, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 020, Train Loss: 0.0801, Test Loss 0.2099, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 021, Train Loss: 0.0799, Test Loss 0.2125, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 022, Train Loss: 0.0797, Test Loss 0.2099, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 023, Train Loss: 0.0795, Test Loss 0.2097, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 024, Train Loss: 0.0794, Test Loss 0.2091, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 025, Train Loss: 0.0794, Test Loss 0.2087, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 026, Train Loss: 0.0794, Test Loss 0.2089, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 027, Train Loss: 0.0794, Test Loss 0.2090, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 028, Train Loss: 0.0793, Test Loss 0.2092, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 029, Train Loss: 0.0793, Test Loss 0.2092, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 030, Train Loss: 0.0793, Test Loss 0.2088, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 031, Train Loss: 0.0793, Test Loss 0.2093, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 032, Train Loss: 0.0793, Test Loss 0.2094, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 033, Train Loss: 0.0792, Test Loss 0.2091, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 034, Train Loss: 0.0792, Test Loss 0.2088, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 035, Train Loss: 0.0792, Test Loss 0.2089, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 036, Train Loss: 0.0792, Test Loss 0.2088, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 037, Train Loss: 0.0792, Test Loss 0.2089, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 038, Train Loss: 0.0792, Test Loss 0.2088, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 039, Train Loss: 0.0792, Test Loss 0.2089, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 040, Train Loss: 0.0792, Test Loss 0.2089, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 041, Train Loss: 0.0792, Test Loss 0.2089, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 042, Train Loss: 0.0792, Test Loss 0.2089, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 043, Train Loss: 0.0792, Test Loss 0.2089, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 044, Train Loss: 0.0792, Test Loss 0.2089, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 045, Train Loss: 0.0792, Test Loss 0.2089, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 046, Train Loss: 0.0792, Test Loss 0.2089, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 047, Train Loss: 0.0792, Test Loss 0.2089, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 048, Train Loss: 0.0792, Test Loss 0.2089, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 049, Train Loss: 0.0792, Test Loss 0.2089, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 000, Train Loss: 0.0868, Test Loss 0.2340, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 001, Train Loss: 0.0862, Test Loss 0.2165, Train Acc: 0.5682, Test Acc: 1.0000\n",
      "Epoch: 002, Train Loss: 0.0856, Test Loss 0.2187, Train Acc: 0.5341, Test Acc: 1.0000\n",
      "Epoch: 003, Train Loss: 0.0851, Test Loss 0.2212, Train Acc: 0.5909, Test Acc: 0.6667\n",
      "Epoch: 004, Train Loss: 0.0845, Test Loss 0.2189, Train Acc: 0.6364, Test Acc: 0.6667\n",
      "Epoch: 005, Train Loss: 0.0838, Test Loss 0.2228, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 006, Train Loss: 0.0830, Test Loss 0.2043, Train Acc: 0.5909, Test Acc: 1.0000\n",
      "Epoch: 007, Train Loss: 0.0824, Test Loss 0.2260, Train Acc: 0.6364, Test Acc: 0.6667\n",
      "Epoch: 008, Train Loss: 0.0802, Test Loss 0.2207, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 009, Train Loss: 0.0771, Test Loss 0.2065, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 010, Train Loss: 0.0742, Test Loss 0.2101, Train Acc: 0.6477, Test Acc: 0.6667\n",
      "Epoch: 011, Train Loss: 0.0768, Test Loss 0.2059, Train Acc: 0.8068, Test Acc: 0.6667\n",
      "Epoch: 012, Train Loss: 0.0765, Test Loss 0.2093, Train Acc: 0.8182, Test Acc: 0.6667\n",
      "Epoch: 013, Train Loss: 0.0759, Test Loss 0.2012, Train Acc: 0.7955, Test Acc: 0.6667\n",
      "Epoch: 014, Train Loss: 0.0751, Test Loss 0.2021, Train Acc: 0.7955, Test Acc: 0.6667\n",
      "Epoch: 015, Train Loss: 0.0746, Test Loss 0.2096, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 016, Train Loss: 0.0736, Test Loss 0.1971, Train Acc: 0.7955, Test Acc: 0.6667\n",
      "Epoch: 017, Train Loss: 0.0729, Test Loss 0.1985, Train Acc: 0.8068, Test Acc: 0.6667\n",
      "Epoch: 018, Train Loss: 0.0724, Test Loss 0.1930, Train Acc: 0.7955, Test Acc: 0.6667\n",
      "Epoch: 019, Train Loss: 0.0718, Test Loss 0.1978, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 020, Train Loss: 0.0714, Test Loss 0.1983, Train Acc: 0.8182, Test Acc: 0.6667\n",
      "Epoch: 021, Train Loss: 0.0708, Test Loss 0.1967, Train Acc: 0.8409, Test Acc: 0.6667\n",
      "Epoch: 022, Train Loss: 0.0704, Test Loss 0.1847, Train Acc: 0.8182, Test Acc: 1.0000\n",
      "Epoch: 023, Train Loss: 0.0702, Test Loss 0.2003, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 024, Train Loss: 0.0700, Test Loss 0.1981, Train Acc: 0.8068, Test Acc: 0.6667\n",
      "Epoch: 025, Train Loss: 0.0699, Test Loss 0.1957, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 026, Train Loss: 0.0698, Test Loss 0.1949, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 027, Train Loss: 0.0697, Test Loss 0.1929, Train Acc: 0.8409, Test Acc: 0.6667\n",
      "Epoch: 028, Train Loss: 0.0696, Test Loss 0.1910, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 029, Train Loss: 0.0695, Test Loss 0.1904, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 030, Train Loss: 0.0695, Test Loss 0.1901, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 031, Train Loss: 0.0694, Test Loss 0.1897, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 032, Train Loss: 0.0694, Test Loss 0.1892, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 033, Train Loss: 0.0693, Test Loss 0.1904, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 034, Train Loss: 0.0693, Test Loss 0.1898, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 035, Train Loss: 0.0692, Test Loss 0.1891, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 036, Train Loss: 0.0692, Test Loss 0.1890, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 037, Train Loss: 0.0692, Test Loss 0.1889, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 038, Train Loss: 0.0692, Test Loss 0.1888, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 039, Train Loss: 0.0692, Test Loss 0.1889, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 040, Train Loss: 0.0692, Test Loss 0.1888, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 041, Train Loss: 0.0692, Test Loss 0.1889, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 042, Train Loss: 0.0692, Test Loss 0.1888, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 043, Train Loss: 0.0692, Test Loss 0.1887, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 044, Train Loss: 0.0692, Test Loss 0.1888, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 045, Train Loss: 0.0692, Test Loss 0.1887, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 046, Train Loss: 0.0692, Test Loss 0.1887, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 047, Train Loss: 0.0692, Test Loss 0.1887, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 048, Train Loss: 0.0692, Test Loss 0.1887, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 049, Train Loss: 0.0692, Test Loss 0.1887, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 000, Train Loss: 0.0867, Test Loss 0.2148, Train Acc: 0.5114, Test Acc: 0.6667\n",
      "Epoch: 001, Train Loss: 0.0863, Test Loss 0.2289, Train Acc: 0.5227, Test Acc: 0.6667\n",
      "Epoch: 002, Train Loss: 0.0861, Test Loss 0.2261, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 003, Train Loss: 0.0860, Test Loss 0.2272, Train Acc: 0.5795, Test Acc: 0.6667\n",
      "Epoch: 004, Train Loss: 0.0855, Test Loss 0.2263, Train Acc: 0.5682, Test Acc: 0.6667\n",
      "Epoch: 005, Train Loss: 0.0851, Test Loss 0.2175, Train Acc: 0.6250, Test Acc: 1.0000\n",
      "Epoch: 006, Train Loss: 0.0851, Test Loss 0.2322, Train Acc: 0.5795, Test Acc: 0.3333\n",
      "Epoch: 007, Train Loss: 0.0844, Test Loss 0.2196, Train Acc: 0.6477, Test Acc: 0.6667\n",
      "Epoch: 008, Train Loss: 0.0841, Test Loss 0.2233, Train Acc: 0.7045, Test Acc: 0.6667\n",
      "Epoch: 009, Train Loss: 0.0836, Test Loss 0.2199, Train Acc: 0.6932, Test Acc: 0.6667\n",
      "Epoch: 010, Train Loss: 0.0837, Test Loss 0.2047, Train Acc: 0.5568, Test Acc: 0.6667\n",
      "Epoch: 011, Train Loss: 0.0824, Test Loss 0.2200, Train Acc: 0.6932, Test Acc: 0.6667\n",
      "Epoch: 012, Train Loss: 0.0821, Test Loss 0.2156, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 013, Train Loss: 0.0811, Test Loss 0.2128, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 014, Train Loss: 0.0793, Test Loss 0.2122, Train Acc: 0.6477, Test Acc: 0.6667\n",
      "Epoch: 015, Train Loss: 0.0786, Test Loss 0.2125, Train Acc: 0.6591, Test Acc: 0.6667\n",
      "Epoch: 016, Train Loss: 0.0779, Test Loss 0.2056, Train Acc: 0.7045, Test Acc: 0.6667\n",
      "Epoch: 017, Train Loss: 0.0773, Test Loss 0.2081, Train Acc: 0.6932, Test Acc: 0.6667\n",
      "Epoch: 018, Train Loss: 0.0769, Test Loss 0.1949, Train Acc: 0.7614, Test Acc: 1.0000\n",
      "Epoch: 019, Train Loss: 0.0758, Test Loss 0.2038, Train Acc: 0.7159, Test Acc: 0.6667\n",
      "Epoch: 020, Train Loss: 0.0752, Test Loss 0.1948, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 021, Train Loss: 0.0749, Test Loss 0.2101, Train Acc: 0.6477, Test Acc: 0.6667\n",
      "Epoch: 022, Train Loss: 0.0742, Test Loss 0.1874, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 023, Train Loss: 0.0733, Test Loss 0.1908, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 024, Train Loss: 0.0732, Test Loss 0.1912, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 025, Train Loss: 0.0731, Test Loss 0.1935, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 026, Train Loss: 0.0730, Test Loss 0.1933, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 027, Train Loss: 0.0730, Test Loss 0.1936, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 028, Train Loss: 0.0729, Test Loss 0.1935, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 029, Train Loss: 0.0729, Test Loss 0.1924, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 030, Train Loss: 0.0728, Test Loss 0.1920, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 031, Train Loss: 0.0727, Test Loss 0.1929, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 032, Train Loss: 0.0727, Test Loss 0.1932, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 033, Train Loss: 0.0726, Test Loss 0.1930, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 034, Train Loss: 0.0726, Test Loss 0.1920, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 035, Train Loss: 0.0725, Test Loss 0.1927, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 036, Train Loss: 0.0725, Test Loss 0.1927, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 037, Train Loss: 0.0725, Test Loss 0.1927, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 038, Train Loss: 0.0725, Test Loss 0.1928, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 039, Train Loss: 0.0725, Test Loss 0.1927, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 040, Train Loss: 0.0725, Test Loss 0.1927, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 041, Train Loss: 0.0725, Test Loss 0.1927, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 042, Train Loss: 0.0725, Test Loss 0.1927, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 043, Train Loss: 0.0725, Test Loss 0.1926, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 044, Train Loss: 0.0725, Test Loss 0.1926, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 045, Train Loss: 0.0725, Test Loss 0.1926, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 046, Train Loss: 0.0725, Test Loss 0.1926, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 047, Train Loss: 0.0725, Test Loss 0.1925, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 048, Train Loss: 0.0725, Test Loss 0.1925, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 049, Train Loss: 0.0725, Test Loss 0.1925, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 000, Train Loss: 0.0870, Test Loss 0.2216, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 001, Train Loss: 0.0865, Test Loss 0.2261, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 002, Train Loss: 0.0863, Test Loss 0.2395, Train Acc: 0.5455, Test Acc: 0.0000\n",
      "Epoch: 003, Train Loss: 0.0859, Test Loss 0.2246, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 004, Train Loss: 0.0857, Test Loss 0.2182, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 005, Train Loss: 0.0854, Test Loss 0.2493, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 006, Train Loss: 0.0859, Test Loss 0.2113, Train Acc: 0.5227, Test Acc: 0.6667\n",
      "Epoch: 007, Train Loss: 0.0844, Test Loss 0.2199, Train Acc: 0.5682, Test Acc: 0.6667\n",
      "Epoch: 008, Train Loss: 0.0834, Test Loss 0.2306, Train Acc: 0.6364, Test Acc: 0.3333\n",
      "Epoch: 009, Train Loss: 0.0823, Test Loss 0.2290, Train Acc: 0.6591, Test Acc: 0.6667\n",
      "Epoch: 010, Train Loss: 0.0814, Test Loss 0.2003, Train Acc: 0.6136, Test Acc: 0.6667\n",
      "Epoch: 011, Train Loss: 0.0804, Test Loss 0.2368, Train Acc: 0.5795, Test Acc: 0.3333\n",
      "Epoch: 012, Train Loss: 0.0793, Test Loss 0.2267, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 013, Train Loss: 0.0786, Test Loss 0.2153, Train Acc: 0.7045, Test Acc: 0.6667\n",
      "Epoch: 014, Train Loss: 0.0782, Test Loss 0.2060, Train Acc: 0.7614, Test Acc: 1.0000\n",
      "Epoch: 015, Train Loss: 0.0782, Test Loss 0.1991, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 016, Train Loss: 0.0776, Test Loss 0.2053, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 017, Train Loss: 0.0774, Test Loss 0.2108, Train Acc: 0.7045, Test Acc: 0.6667\n",
      "Epoch: 018, Train Loss: 0.0771, Test Loss 0.2098, Train Acc: 0.7045, Test Acc: 0.6667\n",
      "Epoch: 019, Train Loss: 0.0768, Test Loss 0.1988, Train Acc: 0.7614, Test Acc: 1.0000\n",
      "Epoch: 020, Train Loss: 0.0764, Test Loss 0.2039, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 021, Train Loss: 0.0761, Test Loss 0.2011, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 022, Train Loss: 0.0759, Test Loss 0.2064, Train Acc: 0.6818, Test Acc: 0.6667\n",
      "Epoch: 023, Train Loss: 0.0756, Test Loss 0.1975, Train Acc: 0.7727, Test Acc: 1.0000\n",
      "Epoch: 024, Train Loss: 0.0755, Test Loss 0.1982, Train Acc: 0.7614, Test Acc: 1.0000\n",
      "Epoch: 025, Train Loss: 0.0755, Test Loss 0.1984, Train Acc: 0.7614, Test Acc: 1.0000\n",
      "Epoch: 026, Train Loss: 0.0755, Test Loss 0.1984, Train Acc: 0.7614, Test Acc: 1.0000\n",
      "Epoch: 027, Train Loss: 0.0754, Test Loss 0.1990, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 028, Train Loss: 0.0754, Test Loss 0.1991, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 029, Train Loss: 0.0754, Test Loss 0.1994, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 030, Train Loss: 0.0753, Test Loss 0.2002, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 031, Train Loss: 0.0753, Test Loss 0.2002, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 032, Train Loss: 0.0753, Test Loss 0.2002, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 033, Train Loss: 0.0752, Test Loss 0.2000, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 034, Train Loss: 0.0752, Test Loss 0.1996, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 035, Train Loss: 0.0752, Test Loss 0.1993, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 036, Train Loss: 0.0752, Test Loss 0.1993, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 037, Train Loss: 0.0752, Test Loss 0.1994, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 038, Train Loss: 0.0752, Test Loss 0.1994, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 039, Train Loss: 0.0752, Test Loss 0.1994, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 040, Train Loss: 0.0752, Test Loss 0.1994, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 041, Train Loss: 0.0752, Test Loss 0.1994, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 042, Train Loss: 0.0752, Test Loss 0.1994, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 043, Train Loss: 0.0752, Test Loss 0.1994, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 044, Train Loss: 0.0752, Test Loss 0.1995, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 045, Train Loss: 0.0752, Test Loss 0.1994, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 046, Train Loss: 0.0752, Test Loss 0.1994, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 047, Train Loss: 0.0752, Test Loss 0.1994, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 048, Train Loss: 0.0752, Test Loss 0.1994, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 049, Train Loss: 0.0752, Test Loss 0.1994, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 000, Train Loss: 0.0910, Test Loss 0.2869, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 001, Train Loss: 0.0864, Test Loss 0.2362, Train Acc: 0.5909, Test Acc: 0.0000\n",
      "Epoch: 002, Train Loss: 0.0862, Test Loss 0.2256, Train Acc: 0.5114, Test Acc: 0.6667\n",
      "Epoch: 003, Train Loss: 0.0856, Test Loss 0.2410, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 004, Train Loss: 0.0851, Test Loss 0.2317, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 005, Train Loss: 0.0840, Test Loss 0.2217, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 006, Train Loss: 0.0823, Test Loss 0.2269, Train Acc: 0.5114, Test Acc: 0.3333\n",
      "Epoch: 007, Train Loss: 0.0818, Test Loss 0.2088, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "Epoch: 008, Train Loss: 0.0834, Test Loss 0.2110, Train Acc: 0.5682, Test Acc: 0.6667\n",
      "Epoch: 009, Train Loss: 0.0823, Test Loss 0.2382, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 010, Train Loss: 0.0828, Test Loss 0.1971, Train Acc: 0.5682, Test Acc: 0.6667\n",
      "Epoch: 011, Train Loss: 0.0820, Test Loss 0.2443, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 012, Train Loss: 0.0799, Test Loss 0.2295, Train Acc: 0.5341, Test Acc: 0.3333\n",
      "Epoch: 013, Train Loss: 0.0784, Test Loss 0.2128, Train Acc: 0.6932, Test Acc: 0.6667\n",
      "Epoch: 014, Train Loss: 0.0779, Test Loss 0.2018, Train Acc: 0.7727, Test Acc: 1.0000\n",
      "Epoch: 015, Train Loss: 0.0775, Test Loss 0.2041, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 016, Train Loss: 0.0773, Test Loss 0.2031, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 017, Train Loss: 0.0769, Test Loss 0.1984, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 018, Train Loss: 0.0766, Test Loss 0.1946, Train Acc: 0.7614, Test Acc: 1.0000\n",
      "Epoch: 019, Train Loss: 0.0763, Test Loss 0.1918, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 020, Train Loss: 0.0758, Test Loss 0.1945, Train Acc: 0.7614, Test Acc: 1.0000\n",
      "Epoch: 021, Train Loss: 0.0750, Test Loss 0.1909, Train Acc: 0.7614, Test Acc: 1.0000\n",
      "Epoch: 022, Train Loss: 0.0742, Test Loss 0.1907, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 023, Train Loss: 0.0735, Test Loss 0.1821, Train Acc: 0.7614, Test Acc: 1.0000\n",
      "Epoch: 024, Train Loss: 0.0735, Test Loss 0.1813, Train Acc: 0.7614, Test Acc: 1.0000\n",
      "Epoch: 025, Train Loss: 0.0734, Test Loss 0.1816, Train Acc: 0.7614, Test Acc: 1.0000\n",
      "Epoch: 026, Train Loss: 0.0733, Test Loss 0.1822, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 027, Train Loss: 0.0732, Test Loss 0.1822, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 028, Train Loss: 0.0732, Test Loss 0.1828, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 029, Train Loss: 0.0731, Test Loss 0.1832, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 030, Train Loss: 0.0731, Test Loss 0.1834, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 031, Train Loss: 0.0730, Test Loss 0.1835, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 032, Train Loss: 0.0730, Test Loss 0.1839, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 033, Train Loss: 0.0729, Test Loss 0.1837, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 034, Train Loss: 0.0729, Test Loss 0.1831, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 035, Train Loss: 0.0728, Test Loss 0.1827, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 036, Train Loss: 0.0728, Test Loss 0.1827, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 037, Train Loss: 0.0728, Test Loss 0.1827, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 038, Train Loss: 0.0728, Test Loss 0.1827, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 039, Train Loss: 0.0728, Test Loss 0.1827, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 040, Train Loss: 0.0728, Test Loss 0.1827, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 041, Train Loss: 0.0728, Test Loss 0.1827, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 042, Train Loss: 0.0728, Test Loss 0.1827, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 043, Train Loss: 0.0728, Test Loss 0.1826, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 044, Train Loss: 0.0728, Test Loss 0.1827, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 045, Train Loss: 0.0728, Test Loss 0.1827, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 046, Train Loss: 0.0728, Test Loss 0.1826, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 047, Train Loss: 0.0728, Test Loss 0.1826, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 048, Train Loss: 0.0728, Test Loss 0.1826, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 049, Train Loss: 0.0728, Test Loss 0.1826, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 000, Train Loss: 0.0865, Test Loss 0.2278, Train Acc: 0.5227, Test Acc: 0.6667\n",
      "Epoch: 001, Train Loss: 0.0865, Test Loss 0.2294, Train Acc: 0.5568, Test Acc: 1.0000\n",
      "Epoch: 002, Train Loss: 0.0864, Test Loss 0.2289, Train Acc: 0.5682, Test Acc: 1.0000\n",
      "Epoch: 003, Train Loss: 0.0862, Test Loss 0.2281, Train Acc: 0.5909, Test Acc: 1.0000\n",
      "Epoch: 004, Train Loss: 0.0860, Test Loss 0.2250, Train Acc: 0.5341, Test Acc: 0.6667\n",
      "Epoch: 005, Train Loss: 0.0857, Test Loss 0.2173, Train Acc: 0.5227, Test Acc: 0.6667\n",
      "Epoch: 006, Train Loss: 0.0862, Test Loss 0.2303, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 007, Train Loss: 0.0860, Test Loss 0.2197, Train Acc: 0.5227, Test Acc: 0.6667\n",
      "Epoch: 008, Train Loss: 0.0855, Test Loss 0.2192, Train Acc: 0.5227, Test Acc: 0.6667\n",
      "Epoch: 009, Train Loss: 0.0861, Test Loss 0.2300, Train Acc: 0.6477, Test Acc: 0.6667\n",
      "Epoch: 010, Train Loss: 0.0859, Test Loss 0.2240, Train Acc: 0.5227, Test Acc: 0.6667\n",
      "Epoch: 011, Train Loss: 0.0851, Test Loss 0.2225, Train Acc: 0.6023, Test Acc: 1.0000\n",
      "Epoch: 012, Train Loss: 0.0850, Test Loss 0.2242, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 013, Train Loss: 0.0848, Test Loss 0.2242, Train Acc: 0.7045, Test Acc: 0.6667\n",
      "Epoch: 014, Train Loss: 0.0847, Test Loss 0.2257, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 015, Train Loss: 0.0846, Test Loss 0.2261, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 016, Train Loss: 0.0845, Test Loss 0.2247, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 017, Train Loss: 0.0843, Test Loss 0.2260, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 018, Train Loss: 0.0842, Test Loss 0.2247, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 019, Train Loss: 0.0841, Test Loss 0.2262, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 020, Train Loss: 0.0839, Test Loss 0.2247, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 021, Train Loss: 0.0838, Test Loss 0.2252, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 022, Train Loss: 0.0837, Test Loss 0.2226, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 023, Train Loss: 0.0836, Test Loss 0.2235, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 024, Train Loss: 0.0836, Test Loss 0.2232, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 025, Train Loss: 0.0836, Test Loss 0.2231, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 026, Train Loss: 0.0836, Test Loss 0.2232, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 027, Train Loss: 0.0836, Test Loss 0.2229, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 028, Train Loss: 0.0836, Test Loss 0.2228, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 029, Train Loss: 0.0836, Test Loss 0.2229, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 030, Train Loss: 0.0835, Test Loss 0.2230, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 031, Train Loss: 0.0835, Test Loss 0.2228, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 032, Train Loss: 0.0835, Test Loss 0.2228, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 033, Train Loss: 0.0835, Test Loss 0.2230, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 034, Train Loss: 0.0835, Test Loss 0.2227, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 035, Train Loss: 0.0835, Test Loss 0.2227, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 036, Train Loss: 0.0835, Test Loss 0.2227, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 037, Train Loss: 0.0835, Test Loss 0.2227, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 038, Train Loss: 0.0835, Test Loss 0.2227, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 039, Train Loss: 0.0835, Test Loss 0.2227, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 040, Train Loss: 0.0835, Test Loss 0.2227, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 041, Train Loss: 0.0835, Test Loss 0.2227, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 042, Train Loss: 0.0835, Test Loss 0.2227, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 043, Train Loss: 0.0835, Test Loss 0.2227, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 044, Train Loss: 0.0835, Test Loss 0.2227, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 045, Train Loss: 0.0835, Test Loss 0.2227, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 046, Train Loss: 0.0835, Test Loss 0.2227, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 047, Train Loss: 0.0835, Test Loss 0.2227, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 048, Train Loss: 0.0835, Test Loss 0.2227, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 049, Train Loss: 0.0835, Test Loss 0.2227, Train Acc: 0.7273, Test Acc: 0.6667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "eval_metrics = np.zeros((skf.n_splits, 3))\n",
    "\n",
    "labels = [full_dataset[i].y for i in range(len(full_dataset))]\n",
    "\n",
    "\n",
    "for n_fold, (train_idx, test_idx) in tqdm(enumerate(skf.split(labels, labels))):\n",
    "    model = GCN(full_dataset.num_features, 2, channels=[256, 32, 8], dropout=0.3).to(device())\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=50//4, gamma=0.1, last_epoch=-1, verbose=False)\n",
    "\n",
    "    train_loader_ = DataLoader(full_dataset[list(train_idx)], batch_size=8, shuffle=True)\n",
    "    test_loader_ = DataLoader(full_dataset[list(test_idx)], batch_size=8, shuffle=True)\n",
    "    min_v_loss = np.inf\n",
    "    print(n_fold)\n",
    "    pr, rc, acc = [], [], []\n",
    "    for epoch in range(50):\n",
    "        train_epoch(train_loader, model, criterion, optimizer)\n",
    "        train_loss, train_acc, _, _ = eval_epoch(train_loader, model, criterion)\n",
    "        val_loss, test_acc, _, _ = eval_epoch(val_loader, model, criterion)\n",
    "        scheduler.step()\n",
    "        print(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.4f}, Test Loss {val_loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "        #print(f'Train Prec: {train_pr:.3f}, Train Rec: {train_rc:.3f}, Test Prec: {val_pr:.3f}, Test Rec: {val_rc:.3f}')\n",
    "        #rc.append(val_rc)\n",
    "        #pr.append(val_pr)\n",
    "        acc.append(test_acc)\n",
    "        if min_v_loss > val_loss:\n",
    "            min_v_loss = val_loss\n",
    "            best_test_acc = test_acc\n",
    "\n",
    "    eval_metrics[n_fold, 0] = best_test_acc\n",
    "    eval_metrics[n_fold, 1] = np.mean(acc)\n",
    "    eval_metrics[n_fold, 2] = np.std(acc)\n",
    "### eval_metrics[n_fold, 3] ="
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.66666667, 0.67333333, 0.08137704],\n       [1.        , 0.72      , 0.13920409],\n       [1.        , 0.59333333, 0.18      ],\n       [1.        , 0.86666667, 0.17638342],\n       [1.        , 0.66666667, 0.11547005],\n       [1.        , 0.68666667, 0.1034945 ],\n       [1.        , 0.68      , 0.09333333],\n       [1.        , 0.85333333, 0.23247461],\n       [1.        , 0.86666667, 0.25819889],\n       [0.66666667, 0.7       , 0.1       ]])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "(0.9333333333333332, 0.13333333333333336)"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(eval_metrics[:, 0]), np.std(eval_metrics[:, 0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "(0.7306666666666666, 0.0915641851380768)"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(eval_metrics[:, 1]), np.std(eval_metrics[:, 1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7b0c61b80a07460b875fa182f812bd74"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Epoch: 000, Train Loss: 0.0872, Test Loss 0.2138, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 001, Train Loss: 0.0851, Test Loss 0.2342, Train Acc: 0.5909, Test Acc: 0.3333\n",
      "Epoch: 002, Train Loss: 0.0846, Test Loss 0.2182, Train Acc: 0.5227, Test Acc: 0.6667\n",
      "Epoch: 003, Train Loss: 0.0813, Test Loss 0.2223, Train Acc: 0.6591, Test Acc: 0.6667\n",
      "Epoch: 004, Train Loss: 0.0795, Test Loss 0.2135, Train Acc: 0.6818, Test Acc: 0.6667\n",
      "Epoch: 005, Train Loss: 0.0826, Test Loss 0.2691, Train Acc: 0.5455, Test Acc: 0.3333\n",
      "Epoch: 006, Train Loss: 0.0795, Test Loss 0.1939, Train Acc: 0.6364, Test Acc: 1.0000\n",
      "Epoch: 007, Train Loss: 0.0734, Test Loss 0.2276, Train Acc: 0.7045, Test Acc: 0.6667\n",
      "Epoch: 008, Train Loss: 0.0764, Test Loss 0.2646, Train Acc: 0.6477, Test Acc: 0.3333\n",
      "Epoch: 009, Train Loss: 0.0801, Test Loss 0.2904, Train Acc: 0.5909, Test Acc: 0.3333\n",
      "Epoch: 010, Train Loss: 0.0687, Test Loss 0.1715, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 011, Train Loss: 0.0715, Test Loss 0.1550, Train Acc: 0.6591, Test Acc: 0.6667\n",
      "Epoch: 012, Train Loss: 0.0637, Test Loss 0.1778, Train Acc: 0.8523, Test Acc: 0.6667\n",
      "Epoch: 013, Train Loss: 0.0630, Test Loss 0.1929, Train Acc: 0.7955, Test Acc: 0.6667\n",
      "Epoch: 014, Train Loss: 0.0625, Test Loss 0.1859, Train Acc: 0.8409, Test Acc: 0.6667\n",
      "Epoch: 015, Train Loss: 0.0632, Test Loss 0.1701, Train Acc: 0.8409, Test Acc: 0.6667\n",
      "Epoch: 016, Train Loss: 0.0617, Test Loss 0.1828, Train Acc: 0.8523, Test Acc: 0.6667\n",
      "Epoch: 017, Train Loss: 0.0615, Test Loss 0.1756, Train Acc: 0.8523, Test Acc: 0.6667\n",
      "Epoch: 018, Train Loss: 0.0609, Test Loss 0.1907, Train Acc: 0.7955, Test Acc: 0.6667\n",
      "Epoch: 019, Train Loss: 0.0609, Test Loss 0.1707, Train Acc: 0.8409, Test Acc: 0.6667\n",
      "Epoch: 020, Train Loss: 0.0608, Test Loss 0.1671, Train Acc: 0.8409, Test Acc: 0.6667\n",
      "Epoch: 021, Train Loss: 0.0597, Test Loss 0.1706, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 022, Train Loss: 0.0591, Test Loss 0.1702, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 023, Train Loss: 0.0587, Test Loss 0.1673, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 024, Train Loss: 0.0587, Test Loss 0.1670, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 025, Train Loss: 0.0586, Test Loss 0.1674, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 026, Train Loss: 0.0586, Test Loss 0.1668, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 027, Train Loss: 0.0587, Test Loss 0.1652, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 028, Train Loss: 0.0587, Test Loss 0.1649, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 029, Train Loss: 0.0586, Test Loss 0.1654, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 030, Train Loss: 0.0589, Test Loss 0.1634, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 031, Train Loss: 0.0588, Test Loss 0.1634, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 032, Train Loss: 0.0587, Test Loss 0.1638, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 033, Train Loss: 0.0584, Test Loss 0.1653, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 034, Train Loss: 0.0586, Test Loss 0.1638, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 035, Train Loss: 0.0587, Test Loss 0.1624, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 036, Train Loss: 0.0587, Test Loss 0.1625, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 037, Train Loss: 0.0587, Test Loss 0.1624, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 038, Train Loss: 0.0587, Test Loss 0.1625, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 039, Train Loss: 0.0587, Test Loss 0.1626, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 040, Train Loss: 0.0587, Test Loss 0.1624, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 041, Train Loss: 0.0587, Test Loss 0.1625, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 042, Train Loss: 0.0587, Test Loss 0.1626, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 043, Train Loss: 0.0587, Test Loss 0.1625, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 044, Train Loss: 0.0587, Test Loss 0.1624, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 045, Train Loss: 0.0587, Test Loss 0.1625, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 046, Train Loss: 0.0587, Test Loss 0.1626, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 047, Train Loss: 0.0586, Test Loss 0.1626, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 048, Train Loss: 0.0586, Test Loss 0.1626, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 049, Train Loss: 0.0586, Test Loss 0.1626, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "1\n",
      "Epoch: 000, Train Loss: 0.0868, Test Loss 0.2403, Train Acc: 0.5341, Test Acc: 0.0000\n",
      "Epoch: 001, Train Loss: 0.0876, Test Loss 0.2195, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 002, Train Loss: 0.0865, Test Loss 0.2339, Train Acc: 0.5568, Test Acc: 0.6667\n",
      "Epoch: 003, Train Loss: 0.0863, Test Loss 0.2308, Train Acc: 0.5227, Test Acc: 0.6667\n",
      "Epoch: 004, Train Loss: 0.0863, Test Loss 0.2238, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 005, Train Loss: 0.0858, Test Loss 0.2343, Train Acc: 0.6136, Test Acc: 0.0000\n",
      "Epoch: 006, Train Loss: 0.0855, Test Loss 0.2252, Train Acc: 0.5341, Test Acc: 0.6667\n",
      "Epoch: 007, Train Loss: 0.0856, Test Loss 0.2205, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 008, Train Loss: 0.0851, Test Loss 0.2417, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 009, Train Loss: 0.0859, Test Loss 0.2139, Train Acc: 0.5114, Test Acc: 0.6667\n",
      "Epoch: 010, Train Loss: 0.0838, Test Loss 0.2309, Train Acc: 0.5682, Test Acc: 0.3333\n",
      "Epoch: 011, Train Loss: 0.0844, Test Loss 0.2097, Train Acc: 0.5568, Test Acc: 0.6667\n",
      "Epoch: 012, Train Loss: 0.0835, Test Loss 0.2101, Train Acc: 0.5682, Test Acc: 0.6667\n",
      "Epoch: 013, Train Loss: 0.0826, Test Loss 0.2116, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "Epoch: 014, Train Loss: 0.0821, Test Loss 0.2136, Train Acc: 0.7614, Test Acc: 1.0000\n",
      "Epoch: 015, Train Loss: 0.0819, Test Loss 0.2099, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 016, Train Loss: 0.0814, Test Loss 0.2078, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 017, Train Loss: 0.0810, Test Loss 0.2065, Train Acc: 0.7159, Test Acc: 0.6667\n",
      "Epoch: 018, Train Loss: 0.0807, Test Loss 0.2033, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 019, Train Loss: 0.0808, Test Loss 0.1998, Train Acc: 0.6932, Test Acc: 1.0000\n",
      "Epoch: 020, Train Loss: 0.0807, Test Loss 0.1979, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "Epoch: 021, Train Loss: 0.0798, Test Loss 0.1967, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 022, Train Loss: 0.0791, Test Loss 0.1962, Train Acc: 0.7159, Test Acc: 0.6667\n",
      "Epoch: 023, Train Loss: 0.0789, Test Loss 0.1928, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 024, Train Loss: 0.0788, Test Loss 0.1926, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 025, Train Loss: 0.0788, Test Loss 0.1925, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 026, Train Loss: 0.0786, Test Loss 0.1926, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 027, Train Loss: 0.0786, Test Loss 0.1924, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 028, Train Loss: 0.0786, Test Loss 0.1921, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 029, Train Loss: 0.0785, Test Loss 0.1917, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 030, Train Loss: 0.0785, Test Loss 0.1913, Train Acc: 0.6818, Test Acc: 1.0000\n",
      "Epoch: 031, Train Loss: 0.0785, Test Loss 0.1911, Train Acc: 0.6818, Test Acc: 1.0000\n",
      "Epoch: 032, Train Loss: 0.0784, Test Loss 0.1912, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 033, Train Loss: 0.0784, Test Loss 0.1909, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 034, Train Loss: 0.0783, Test Loss 0.1909, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 035, Train Loss: 0.0782, Test Loss 0.1906, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 036, Train Loss: 0.0782, Test Loss 0.1906, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 037, Train Loss: 0.0782, Test Loss 0.1906, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 038, Train Loss: 0.0782, Test Loss 0.1906, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 039, Train Loss: 0.0782, Test Loss 0.1906, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 040, Train Loss: 0.0782, Test Loss 0.1905, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 041, Train Loss: 0.0782, Test Loss 0.1906, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 042, Train Loss: 0.0782, Test Loss 0.1905, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 043, Train Loss: 0.0782, Test Loss 0.1905, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 044, Train Loss: 0.0782, Test Loss 0.1905, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 045, Train Loss: 0.0782, Test Loss 0.1905, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 046, Train Loss: 0.0782, Test Loss 0.1905, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 047, Train Loss: 0.0782, Test Loss 0.1905, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 048, Train Loss: 0.0782, Test Loss 0.1905, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 049, Train Loss: 0.0782, Test Loss 0.1905, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "2\n",
      "Epoch: 000, Train Loss: 0.0895, Test Loss 0.2098, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 001, Train Loss: 0.0868, Test Loss 0.2271, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 002, Train Loss: 0.0867, Test Loss 0.2283, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 003, Train Loss: 0.0866, Test Loss 0.2278, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 004, Train Loss: 0.0864, Test Loss 0.2344, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 005, Train Loss: 0.0866, Test Loss 0.2429, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 006, Train Loss: 0.0861, Test Loss 0.2278, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "Epoch: 007, Train Loss: 0.0858, Test Loss 0.2281, Train Acc: 0.6818, Test Acc: 0.6667\n",
      "Epoch: 008, Train Loss: 0.0857, Test Loss 0.2410, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 009, Train Loss: 0.0848, Test Loss 0.2272, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 010, Train Loss: 0.0842, Test Loss 0.2338, Train Acc: 0.5227, Test Acc: 0.3333\n",
      "Epoch: 011, Train Loss: 0.0836, Test Loss 0.2384, Train Acc: 0.5227, Test Acc: 0.3333\n",
      "Epoch: 012, Train Loss: 0.0833, Test Loss 0.2360, Train Acc: 0.5455, Test Acc: 0.3333\n",
      "Epoch: 013, Train Loss: 0.0830, Test Loss 0.2357, Train Acc: 0.5568, Test Acc: 0.3333\n",
      "Epoch: 014, Train Loss: 0.0821, Test Loss 0.2210, Train Acc: 0.7614, Test Acc: 1.0000\n",
      "Epoch: 015, Train Loss: 0.0820, Test Loss 0.2262, Train Acc: 0.6932, Test Acc: 0.6667\n",
      "Epoch: 016, Train Loss: 0.0824, Test Loss 0.2362, Train Acc: 0.5795, Test Acc: 0.3333\n",
      "Epoch: 017, Train Loss: 0.0817, Test Loss 0.2279, Train Acc: 0.6818, Test Acc: 0.3333\n",
      "Epoch: 018, Train Loss: 0.0816, Test Loss 0.2284, Train Acc: 0.6705, Test Acc: 0.3333\n",
      "Epoch: 019, Train Loss: 0.0811, Test Loss 0.2203, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 020, Train Loss: 0.0809, Test Loss 0.2211, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 021, Train Loss: 0.0809, Test Loss 0.2248, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 022, Train Loss: 0.0810, Test Loss 0.2293, Train Acc: 0.6705, Test Acc: 0.3333\n",
      "Epoch: 023, Train Loss: 0.0804, Test Loss 0.2214, Train Acc: 0.7159, Test Acc: 0.6667\n",
      "Epoch: 024, Train Loss: 0.0804, Test Loss 0.2214, Train Acc: 0.7159, Test Acc: 0.6667\n",
      "Epoch: 025, Train Loss: 0.0804, Test Loss 0.2212, Train Acc: 0.7159, Test Acc: 0.6667\n",
      "Epoch: 026, Train Loss: 0.0803, Test Loss 0.2207, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 027, Train Loss: 0.0803, Test Loss 0.2214, Train Acc: 0.7159, Test Acc: 0.6667\n",
      "Epoch: 028, Train Loss: 0.0803, Test Loss 0.2224, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 029, Train Loss: 0.0803, Test Loss 0.2219, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 030, Train Loss: 0.0803, Test Loss 0.2225, Train Acc: 0.7159, Test Acc: 0.6667\n",
      "Epoch: 031, Train Loss: 0.0803, Test Loss 0.2223, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 032, Train Loss: 0.0803, Test Loss 0.2219, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 033, Train Loss: 0.0802, Test Loss 0.2215, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 034, Train Loss: 0.0802, Test Loss 0.2218, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 035, Train Loss: 0.0802, Test Loss 0.2220, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 036, Train Loss: 0.0802, Test Loss 0.2221, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 037, Train Loss: 0.0802, Test Loss 0.2221, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 038, Train Loss: 0.0802, Test Loss 0.2221, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 039, Train Loss: 0.0802, Test Loss 0.2221, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 040, Train Loss: 0.0802, Test Loss 0.2221, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 041, Train Loss: 0.0802, Test Loss 0.2221, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 042, Train Loss: 0.0802, Test Loss 0.2220, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 043, Train Loss: 0.0802, Test Loss 0.2220, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 044, Train Loss: 0.0802, Test Loss 0.2220, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 045, Train Loss: 0.0802, Test Loss 0.2220, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 046, Train Loss: 0.0802, Test Loss 0.2220, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 047, Train Loss: 0.0802, Test Loss 0.2220, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 048, Train Loss: 0.0802, Test Loss 0.2220, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 049, Train Loss: 0.0802, Test Loss 0.2220, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "3\n",
      "Epoch: 000, Train Loss: 0.0949, Test Loss 0.2967, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 001, Train Loss: 0.0873, Test Loss 0.2368, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 002, Train Loss: 0.0888, Test Loss 0.1947, Train Acc: 0.5114, Test Acc: 0.6667\n",
      "Epoch: 003, Train Loss: 0.0861, Test Loss 0.2290, Train Acc: 0.5227, Test Acc: 0.3333\n",
      "Epoch: 004, Train Loss: 0.0857, Test Loss 0.2129, Train Acc: 0.5795, Test Acc: 1.0000\n",
      "Epoch: 005, Train Loss: 0.0854, Test Loss 0.2105, Train Acc: 0.5795, Test Acc: 1.0000\n",
      "Epoch: 006, Train Loss: 0.0848, Test Loss 0.2142, Train Acc: 0.6250, Test Acc: 1.0000\n",
      "Epoch: 007, Train Loss: 0.0847, Test Loss 0.2037, Train Acc: 0.5568, Test Acc: 1.0000\n",
      "Epoch: 008, Train Loss: 0.0831, Test Loss 0.2161, Train Acc: 0.5909, Test Acc: 0.6667\n",
      "Epoch: 009, Train Loss: 0.0841, Test Loss 0.1977, Train Acc: 0.5455, Test Acc: 1.0000\n",
      "Epoch: 010, Train Loss: 0.0834, Test Loss 0.2259, Train Acc: 0.6136, Test Acc: 0.3333\n",
      "Epoch: 011, Train Loss: 0.0834, Test Loss 0.1921, Train Acc: 0.5341, Test Acc: 1.0000\n",
      "Epoch: 012, Train Loss: 0.0826, Test Loss 0.1971, Train Acc: 0.5795, Test Acc: 1.0000\n",
      "Epoch: 013, Train Loss: 0.0824, Test Loss 0.1978, Train Acc: 0.5909, Test Acc: 1.0000\n",
      "Epoch: 014, Train Loss: 0.0820, Test Loss 0.2008, Train Acc: 0.6250, Test Acc: 1.0000\n",
      "Epoch: 015, Train Loss: 0.0817, Test Loss 0.2024, Train Acc: 0.6818, Test Acc: 1.0000\n",
      "Epoch: 016, Train Loss: 0.0816, Test Loss 0.2001, Train Acc: 0.6477, Test Acc: 1.0000\n",
      "Epoch: 017, Train Loss: 0.0816, Test Loss 0.1972, Train Acc: 0.6136, Test Acc: 1.0000\n",
      "Epoch: 018, Train Loss: 0.0813, Test Loss 0.1981, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "Epoch: 019, Train Loss: 0.0810, Test Loss 0.1984, Train Acc: 0.6818, Test Acc: 1.0000\n",
      "Epoch: 020, Train Loss: 0.0811, Test Loss 0.1931, Train Acc: 0.6136, Test Acc: 1.0000\n",
      "Epoch: 021, Train Loss: 0.0806, Test Loss 0.1952, Train Acc: 0.6818, Test Acc: 1.0000\n",
      "Epoch: 022, Train Loss: 0.0803, Test Loss 0.1926, Train Acc: 0.6818, Test Acc: 1.0000\n",
      "Epoch: 023, Train Loss: 0.0804, Test Loss 0.1844, Train Acc: 0.6364, Test Acc: 1.0000\n",
      "Epoch: 024, Train Loss: 0.0802, Test Loss 0.1853, Train Acc: 0.6364, Test Acc: 1.0000\n",
      "Epoch: 025, Train Loss: 0.0801, Test Loss 0.1855, Train Acc: 0.6477, Test Acc: 1.0000\n",
      "Epoch: 026, Train Loss: 0.0799, Test Loss 0.1856, Train Acc: 0.6705, Test Acc: 1.0000\n",
      "Epoch: 027, Train Loss: 0.0798, Test Loss 0.1855, Train Acc: 0.6705, Test Acc: 1.0000\n",
      "Epoch: 028, Train Loss: 0.0797, Test Loss 0.1854, Train Acc: 0.6705, Test Acc: 1.0000\n",
      "Epoch: 029, Train Loss: 0.0796, Test Loss 0.1855, Train Acc: 0.6705, Test Acc: 1.0000\n",
      "Epoch: 030, Train Loss: 0.0795, Test Loss 0.1850, Train Acc: 0.6705, Test Acc: 1.0000\n",
      "Epoch: 031, Train Loss: 0.0793, Test Loss 0.1855, Train Acc: 0.6705, Test Acc: 1.0000\n",
      "Epoch: 032, Train Loss: 0.0792, Test Loss 0.1849, Train Acc: 0.6705, Test Acc: 1.0000\n",
      "Epoch: 033, Train Loss: 0.0791, Test Loss 0.1847, Train Acc: 0.6705, Test Acc: 1.0000\n",
      "Epoch: 034, Train Loss: 0.0790, Test Loss 0.1842, Train Acc: 0.6705, Test Acc: 1.0000\n",
      "Epoch: 035, Train Loss: 0.0789, Test Loss 0.1842, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "Epoch: 036, Train Loss: 0.0789, Test Loss 0.1841, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "Epoch: 037, Train Loss: 0.0789, Test Loss 0.1841, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "Epoch: 038, Train Loss: 0.0789, Test Loss 0.1840, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "Epoch: 039, Train Loss: 0.0789, Test Loss 0.1839, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "Epoch: 040, Train Loss: 0.0788, Test Loss 0.1839, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "Epoch: 041, Train Loss: 0.0788, Test Loss 0.1838, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "Epoch: 042, Train Loss: 0.0788, Test Loss 0.1838, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "Epoch: 043, Train Loss: 0.0788, Test Loss 0.1838, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "Epoch: 044, Train Loss: 0.0788, Test Loss 0.1837, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "Epoch: 045, Train Loss: 0.0788, Test Loss 0.1837, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "Epoch: 046, Train Loss: 0.0788, Test Loss 0.1836, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "Epoch: 047, Train Loss: 0.0788, Test Loss 0.1836, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "Epoch: 048, Train Loss: 0.0788, Test Loss 0.1836, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "Epoch: 049, Train Loss: 0.0788, Test Loss 0.1836, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "4\n",
      "Epoch: 000, Train Loss: 0.0866, Test Loss 0.2417, Train Acc: 0.5227, Test Acc: 0.3333\n",
      "Epoch: 001, Train Loss: 0.0850, Test Loss 0.2202, Train Acc: 0.5341, Test Acc: 0.6667\n",
      "Epoch: 002, Train Loss: 0.0835, Test Loss 0.2207, Train Acc: 0.6477, Test Acc: 0.6667\n",
      "Epoch: 003, Train Loss: 0.0826, Test Loss 0.2195, Train Acc: 0.6591, Test Acc: 0.6667\n",
      "Epoch: 004, Train Loss: 0.0855, Test Loss 0.2622, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 005, Train Loss: 0.0832, Test Loss 0.2266, Train Acc: 0.6023, Test Acc: 0.6667\n",
      "Epoch: 006, Train Loss: 0.0817, Test Loss 0.2126, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 007, Train Loss: 0.0871, Test Loss 0.1793, Train Acc: 0.5341, Test Acc: 0.6667\n",
      "Epoch: 008, Train Loss: 0.0908, Test Loss 0.3034, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 009, Train Loss: 0.0823, Test Loss 0.2126, Train Acc: 0.6136, Test Acc: 0.6667\n",
      "Epoch: 010, Train Loss: 0.0864, Test Loss 0.1866, Train Acc: 0.5341, Test Acc: 0.6667\n",
      "Epoch: 011, Train Loss: 0.0822, Test Loss 0.2368, Train Acc: 0.6250, Test Acc: 0.6667\n",
      "Epoch: 012, Train Loss: 0.0815, Test Loss 0.2314, Train Acc: 0.6591, Test Acc: 0.6667\n",
      "Epoch: 013, Train Loss: 0.0808, Test Loss 0.2246, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 014, Train Loss: 0.0802, Test Loss 0.2159, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 015, Train Loss: 0.0799, Test Loss 0.2151, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 016, Train Loss: 0.0795, Test Loss 0.2132, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 017, Train Loss: 0.0793, Test Loss 0.2135, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 018, Train Loss: 0.0790, Test Loss 0.2099, Train Acc: 0.7045, Test Acc: 0.6667\n",
      "Epoch: 019, Train Loss: 0.0787, Test Loss 0.2139, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 020, Train Loss: 0.0785, Test Loss 0.2180, Train Acc: 0.7159, Test Acc: 0.6667\n",
      "Epoch: 021, Train Loss: 0.0781, Test Loss 0.2117, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 022, Train Loss: 0.0778, Test Loss 0.2127, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 023, Train Loss: 0.0775, Test Loss 0.2121, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 024, Train Loss: 0.0775, Test Loss 0.2116, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 025, Train Loss: 0.0775, Test Loss 0.2121, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 026, Train Loss: 0.0775, Test Loss 0.2130, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 027, Train Loss: 0.0774, Test Loss 0.2127, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 028, Train Loss: 0.0774, Test Loss 0.2122, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 029, Train Loss: 0.0774, Test Loss 0.2119, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 030, Train Loss: 0.0773, Test Loss 0.2126, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 031, Train Loss: 0.0773, Test Loss 0.2129, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 032, Train Loss: 0.0773, Test Loss 0.2124, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 033, Train Loss: 0.0772, Test Loss 0.2123, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 034, Train Loss: 0.0772, Test Loss 0.2125, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 035, Train Loss: 0.0772, Test Loss 0.2126, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 036, Train Loss: 0.0772, Test Loss 0.2126, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 037, Train Loss: 0.0772, Test Loss 0.2126, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 038, Train Loss: 0.0772, Test Loss 0.2126, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 039, Train Loss: 0.0772, Test Loss 0.2127, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 040, Train Loss: 0.0772, Test Loss 0.2126, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 041, Train Loss: 0.0772, Test Loss 0.2126, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 042, Train Loss: 0.0772, Test Loss 0.2127, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 043, Train Loss: 0.0772, Test Loss 0.2127, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 044, Train Loss: 0.0771, Test Loss 0.2126, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 045, Train Loss: 0.0771, Test Loss 0.2127, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 046, Train Loss: 0.0771, Test Loss 0.2126, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 047, Train Loss: 0.0771, Test Loss 0.2127, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 048, Train Loss: 0.0771, Test Loss 0.2127, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 049, Train Loss: 0.0771, Test Loss 0.2127, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "5\n",
      "Epoch: 000, Train Loss: 0.0864, Test Loss 0.2392, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 001, Train Loss: 0.0864, Test Loss 0.2169, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 002, Train Loss: 0.0857, Test Loss 0.2215, Train Acc: 0.5568, Test Acc: 0.6667\n",
      "Epoch: 003, Train Loss: 0.0863, Test Loss 0.2115, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 004, Train Loss: 0.0847, Test Loss 0.2298, Train Acc: 0.5568, Test Acc: 0.3333\n",
      "Epoch: 005, Train Loss: 0.0840, Test Loss 0.2143, Train Acc: 0.6250, Test Acc: 1.0000\n",
      "Epoch: 006, Train Loss: 0.0832, Test Loss 0.2042, Train Acc: 0.6364, Test Acc: 1.0000\n",
      "Epoch: 007, Train Loss: 0.0852, Test Loss 0.1925, Train Acc: 0.5227, Test Acc: 0.6667\n",
      "Epoch: 008, Train Loss: 0.0849, Test Loss 0.1939, Train Acc: 0.5341, Test Acc: 0.6667\n",
      "Epoch: 009, Train Loss: 0.0914, Test Loss 0.3045, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 010, Train Loss: 0.0824, Test Loss 0.1972, Train Acc: 0.6023, Test Acc: 1.0000\n",
      "Epoch: 011, Train Loss: 0.0855, Test Loss 0.2658, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 012, Train Loss: 0.0806, Test Loss 0.2330, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 013, Train Loss: 0.0781, Test Loss 0.1984, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 014, Train Loss: 0.0777, Test Loss 0.1969, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 015, Train Loss: 0.0771, Test Loss 0.1929, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 016, Train Loss: 0.0765, Test Loss 0.1886, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 017, Train Loss: 0.0760, Test Loss 0.1825, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 018, Train Loss: 0.0755, Test Loss 0.1765, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 019, Train Loss: 0.0751, Test Loss 0.1722, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 020, Train Loss: 0.0740, Test Loss 0.1727, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 021, Train Loss: 0.0751, Test Loss 0.1647, Train Acc: 0.6705, Test Acc: 1.0000\n",
      "Epoch: 022, Train Loss: 0.0737, Test Loss 0.1652, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 023, Train Loss: 0.0733, Test Loss 0.1634, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 024, Train Loss: 0.0733, Test Loss 0.1632, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 025, Train Loss: 0.0733, Test Loss 0.1629, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 026, Train Loss: 0.0733, Test Loss 0.1627, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 027, Train Loss: 0.0732, Test Loss 0.1627, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 028, Train Loss: 0.0733, Test Loss 0.1623, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 029, Train Loss: 0.0732, Test Loss 0.1622, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 030, Train Loss: 0.0734, Test Loss 0.1618, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 031, Train Loss: 0.0734, Test Loss 0.1615, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 032, Train Loss: 0.0732, Test Loss 0.1616, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 033, Train Loss: 0.0731, Test Loss 0.1615, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 034, Train Loss: 0.0730, Test Loss 0.1615, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 035, Train Loss: 0.0731, Test Loss 0.1612, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 036, Train Loss: 0.0731, Test Loss 0.1611, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 037, Train Loss: 0.0731, Test Loss 0.1611, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 038, Train Loss: 0.0731, Test Loss 0.1611, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 039, Train Loss: 0.0731, Test Loss 0.1611, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 040, Train Loss: 0.0731, Test Loss 0.1612, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 041, Train Loss: 0.0731, Test Loss 0.1611, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 042, Train Loss: 0.0731, Test Loss 0.1611, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 043, Train Loss: 0.0731, Test Loss 0.1611, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 044, Train Loss: 0.0731, Test Loss 0.1611, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 045, Train Loss: 0.0730, Test Loss 0.1611, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 046, Train Loss: 0.0730, Test Loss 0.1611, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 047, Train Loss: 0.0730, Test Loss 0.1611, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 048, Train Loss: 0.0730, Test Loss 0.1611, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 049, Train Loss: 0.0730, Test Loss 0.1611, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "6\n",
      "Epoch: 000, Train Loss: 0.0870, Test Loss 0.2201, Train Acc: 0.4886, Test Acc: 0.6667\n",
      "Epoch: 001, Train Loss: 0.0871, Test Loss 0.2289, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 002, Train Loss: 0.0862, Test Loss 0.2299, Train Acc: 0.5455, Test Acc: 0.6667\n",
      "Epoch: 003, Train Loss: 0.0867, Test Loss 0.2167, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 004, Train Loss: 0.0847, Test Loss 0.2400, Train Acc: 0.5455, Test Acc: 0.3333\n",
      "Epoch: 005, Train Loss: 0.0834, Test Loss 0.2310, Train Acc: 0.6136, Test Acc: 0.6667\n",
      "Epoch: 006, Train Loss: 0.0823, Test Loss 0.2301, Train Acc: 0.6477, Test Acc: 0.3333\n",
      "Epoch: 007, Train Loss: 0.0840, Test Loss 0.1881, Train Acc: 0.5114, Test Acc: 0.6667\n",
      "Epoch: 008, Train Loss: 0.0818, Test Loss 0.2449, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 009, Train Loss: 0.0889, Test Loss 0.1850, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 010, Train Loss: 0.0828, Test Loss 0.2392, Train Acc: 0.5795, Test Acc: 0.3333\n",
      "Epoch: 011, Train Loss: 0.0821, Test Loss 0.1974, Train Acc: 0.5227, Test Acc: 0.6667\n",
      "Epoch: 012, Train Loss: 0.0820, Test Loss 0.1966, Train Acc: 0.5227, Test Acc: 0.6667\n",
      "Epoch: 013, Train Loss: 0.0808, Test Loss 0.2012, Train Acc: 0.5795, Test Acc: 0.6667\n",
      "Epoch: 014, Train Loss: 0.0800, Test Loss 0.2063, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 015, Train Loss: 0.0797, Test Loss 0.2071, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 016, Train Loss: 0.0794, Test Loss 0.2048, Train Acc: 0.7727, Test Acc: 1.0000\n",
      "Epoch: 017, Train Loss: 0.0792, Test Loss 0.2037, Train Acc: 0.7727, Test Acc: 1.0000\n",
      "Epoch: 018, Train Loss: 0.0789, Test Loss 0.2008, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 019, Train Loss: 0.0787, Test Loss 0.1982, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 020, Train Loss: 0.0781, Test Loss 0.2007, Train Acc: 0.8068, Test Acc: 1.0000\n",
      "Epoch: 021, Train Loss: 0.0779, Test Loss 0.1967, Train Acc: 0.8068, Test Acc: 1.0000\n",
      "Epoch: 022, Train Loss: 0.0776, Test Loss 0.1934, Train Acc: 0.8068, Test Acc: 1.0000\n",
      "Epoch: 023, Train Loss: 0.0775, Test Loss 0.1889, Train Acc: 0.7727, Test Acc: 1.0000\n",
      "Epoch: 024, Train Loss: 0.0774, Test Loss 0.1891, Train Acc: 0.7727, Test Acc: 1.0000\n",
      "Epoch: 025, Train Loss: 0.0774, Test Loss 0.1888, Train Acc: 0.7727, Test Acc: 1.0000\n",
      "Epoch: 026, Train Loss: 0.0773, Test Loss 0.1890, Train Acc: 0.7727, Test Acc: 1.0000\n",
      "Epoch: 027, Train Loss: 0.0773, Test Loss 0.1887, Train Acc: 0.7727, Test Acc: 1.0000\n",
      "Epoch: 028, Train Loss: 0.0773, Test Loss 0.1883, Train Acc: 0.7727, Test Acc: 1.0000\n",
      "Epoch: 029, Train Loss: 0.0772, Test Loss 0.1883, Train Acc: 0.7727, Test Acc: 1.0000\n",
      "Epoch: 030, Train Loss: 0.0772, Test Loss 0.1882, Train Acc: 0.7727, Test Acc: 1.0000\n",
      "Epoch: 031, Train Loss: 0.0771, Test Loss 0.1882, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 032, Train Loss: 0.0771, Test Loss 0.1880, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 033, Train Loss: 0.0771, Test Loss 0.1878, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 034, Train Loss: 0.0770, Test Loss 0.1876, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 035, Train Loss: 0.0770, Test Loss 0.1876, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 036, Train Loss: 0.0770, Test Loss 0.1876, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 037, Train Loss: 0.0769, Test Loss 0.1875, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 038, Train Loss: 0.0769, Test Loss 0.1875, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 039, Train Loss: 0.0769, Test Loss 0.1875, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 040, Train Loss: 0.0769, Test Loss 0.1875, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 041, Train Loss: 0.0769, Test Loss 0.1874, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 042, Train Loss: 0.0769, Test Loss 0.1874, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 043, Train Loss: 0.0769, Test Loss 0.1874, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 044, Train Loss: 0.0769, Test Loss 0.1874, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 045, Train Loss: 0.0769, Test Loss 0.1873, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 046, Train Loss: 0.0769, Test Loss 0.1873, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 047, Train Loss: 0.0769, Test Loss 0.1873, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 048, Train Loss: 0.0769, Test Loss 0.1873, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 049, Train Loss: 0.0769, Test Loss 0.1873, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "7\n",
      "Epoch: 000, Train Loss: 0.0949, Test Loss 0.1830, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 001, Train Loss: 0.0863, Test Loss 0.2440, Train Acc: 0.5114, Test Acc: 0.3333\n",
      "Epoch: 002, Train Loss: 0.0854, Test Loss 0.2221, Train Acc: 0.5568, Test Acc: 0.6667\n",
      "Epoch: 003, Train Loss: 0.0867, Test Loss 0.2579, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 004, Train Loss: 0.0847, Test Loss 0.2116, Train Acc: 0.5114, Test Acc: 0.6667\n",
      "Epoch: 005, Train Loss: 0.0837, Test Loss 0.2280, Train Acc: 0.6932, Test Acc: 0.6667\n",
      "Epoch: 006, Train Loss: 0.0849, Test Loss 0.2436, Train Acc: 0.5227, Test Acc: 0.3333\n",
      "Epoch: 007, Train Loss: 0.0845, Test Loss 0.2076, Train Acc: 0.5341, Test Acc: 0.6667\n",
      "Epoch: 008, Train Loss: 0.0844, Test Loss 0.2468, Train Acc: 0.5227, Test Acc: 0.3333\n",
      "Epoch: 009, Train Loss: 0.0822, Test Loss 0.2183, Train Acc: 0.6932, Test Acc: 0.6667\n",
      "Epoch: 010, Train Loss: 0.0817, Test Loss 0.2298, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 011, Train Loss: 0.0800, Test Loss 0.2046, Train Acc: 0.6364, Test Acc: 0.6667\n",
      "Epoch: 012, Train Loss: 0.0797, Test Loss 0.2059, Train Acc: 0.6477, Test Acc: 1.0000\n",
      "Epoch: 013, Train Loss: 0.0793, Test Loss 0.2103, Train Acc: 0.7045, Test Acc: 0.6667\n",
      "Epoch: 014, Train Loss: 0.0791, Test Loss 0.2129, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 015, Train Loss: 0.0788, Test Loss 0.2119, Train Acc: 0.6932, Test Acc: 0.6667\n",
      "Epoch: 016, Train Loss: 0.0789, Test Loss 0.2193, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 017, Train Loss: 0.0784, Test Loss 0.2079, Train Acc: 0.7159, Test Acc: 0.6667\n",
      "Epoch: 018, Train Loss: 0.0782, Test Loss 0.2069, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 019, Train Loss: 0.0780, Test Loss 0.2035, Train Acc: 0.7159, Test Acc: 0.6667\n",
      "Epoch: 020, Train Loss: 0.0778, Test Loss 0.2094, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 021, Train Loss: 0.0776, Test Loss 0.2027, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 022, Train Loss: 0.0773, Test Loss 0.2045, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 023, Train Loss: 0.0771, Test Loss 0.2088, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 024, Train Loss: 0.0771, Test Loss 0.2082, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 025, Train Loss: 0.0771, Test Loss 0.2073, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 026, Train Loss: 0.0770, Test Loss 0.2062, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 027, Train Loss: 0.0770, Test Loss 0.2053, Train Acc: 0.7159, Test Acc: 0.6667\n",
      "Epoch: 028, Train Loss: 0.0770, Test Loss 0.2047, Train Acc: 0.7159, Test Acc: 0.6667\n",
      "Epoch: 029, Train Loss: 0.0770, Test Loss 0.2044, Train Acc: 0.7159, Test Acc: 0.6667\n",
      "Epoch: 030, Train Loss: 0.0769, Test Loss 0.2039, Train Acc: 0.7159, Test Acc: 0.6667\n",
      "Epoch: 031, Train Loss: 0.0769, Test Loss 0.2034, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 032, Train Loss: 0.0769, Test Loss 0.2036, Train Acc: 0.7159, Test Acc: 0.6667\n",
      "Epoch: 033, Train Loss: 0.0769, Test Loss 0.2019, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 034, Train Loss: 0.0769, Test Loss 0.2019, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 035, Train Loss: 0.0769, Test Loss 0.2014, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 036, Train Loss: 0.0768, Test Loss 0.2014, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 037, Train Loss: 0.0768, Test Loss 0.2015, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 038, Train Loss: 0.0768, Test Loss 0.2014, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 039, Train Loss: 0.0768, Test Loss 0.2014, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 040, Train Loss: 0.0768, Test Loss 0.2014, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 041, Train Loss: 0.0768, Test Loss 0.2015, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 042, Train Loss: 0.0768, Test Loss 0.2014, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 043, Train Loss: 0.0768, Test Loss 0.2014, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 044, Train Loss: 0.0768, Test Loss 0.2014, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 045, Train Loss: 0.0768, Test Loss 0.2013, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 046, Train Loss: 0.0768, Test Loss 0.2013, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 047, Train Loss: 0.0768, Test Loss 0.2014, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 048, Train Loss: 0.0768, Test Loss 0.2014, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 049, Train Loss: 0.0768, Test Loss 0.2014, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "8\n",
      "Epoch: 000, Train Loss: 0.0903, Test Loss 0.2060, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 001, Train Loss: 0.0880, Test Loss 0.2748, Train Acc: 0.4773, Test Acc: 0.3333\n",
      "Epoch: 002, Train Loss: 0.0828, Test Loss 0.2338, Train Acc: 0.6591, Test Acc: 0.3333\n",
      "Epoch: 003, Train Loss: 0.0861, Test Loss 0.1975, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 004, Train Loss: 0.0807, Test Loss 0.2373, Train Acc: 0.6136, Test Acc: 0.6667\n",
      "Epoch: 005, Train Loss: 0.0824, Test Loss 0.2593, Train Acc: 0.5227, Test Acc: 0.3333\n",
      "Epoch: 006, Train Loss: 0.0762, Test Loss 0.2062, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 007, Train Loss: 0.0748, Test Loss 0.1800, Train Acc: 0.7614, Test Acc: 1.0000\n",
      "Epoch: 008, Train Loss: 0.0778, Test Loss 0.1640, Train Acc: 0.5909, Test Acc: 0.6667\n",
      "Epoch: 009, Train Loss: 0.0976, Test Loss 0.1683, Train Acc: 0.5227, Test Acc: 0.6667\n",
      "Epoch: 010, Train Loss: 0.0864, Test Loss 0.2791, Train Acc: 0.5114, Test Acc: 0.3333\n",
      "Epoch: 011, Train Loss: 0.0842, Test Loss 0.1581, Train Acc: 0.5682, Test Acc: 0.6667\n",
      "Epoch: 012, Train Loss: 0.0731, Test Loss 0.1593, Train Acc: 0.6818, Test Acc: 1.0000\n",
      "Epoch: 013, Train Loss: 0.0681, Test Loss 0.1666, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 014, Train Loss: 0.0675, Test Loss 0.1658, Train Acc: 0.8068, Test Acc: 1.0000\n",
      "Epoch: 015, Train Loss: 0.0671, Test Loss 0.1644, Train Acc: 0.8068, Test Acc: 1.0000\n",
      "Epoch: 016, Train Loss: 0.0683, Test Loss 0.1590, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 017, Train Loss: 0.0667, Test Loss 0.1617, Train Acc: 0.8182, Test Acc: 1.0000\n",
      "Epoch: 018, Train Loss: 0.0669, Test Loss 0.1593, Train Acc: 0.8182, Test Acc: 1.0000\n",
      "Epoch: 019, Train Loss: 0.0664, Test Loss 0.1595, Train Acc: 0.8068, Test Acc: 1.0000\n",
      "Epoch: 020, Train Loss: 0.0666, Test Loss 0.1566, Train Acc: 0.8068, Test Acc: 1.0000\n",
      "Epoch: 021, Train Loss: 0.0666, Test Loss 0.1554, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 022, Train Loss: 0.0663, Test Loss 0.1544, Train Acc: 0.8068, Test Acc: 1.0000\n",
      "Epoch: 023, Train Loss: 0.0666, Test Loss 0.1526, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 024, Train Loss: 0.0664, Test Loss 0.1527, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 025, Train Loss: 0.0664, Test Loss 0.1527, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 026, Train Loss: 0.0662, Test Loss 0.1530, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 027, Train Loss: 0.0661, Test Loss 0.1530, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 028, Train Loss: 0.0662, Test Loss 0.1528, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 029, Train Loss: 0.0659, Test Loss 0.1531, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 030, Train Loss: 0.0660, Test Loss 0.1528, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 031, Train Loss: 0.0659, Test Loss 0.1528, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 032, Train Loss: 0.0659, Test Loss 0.1527, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 033, Train Loss: 0.0660, Test Loss 0.1526, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 034, Train Loss: 0.0658, Test Loss 0.1528, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 035, Train Loss: 0.0658, Test Loss 0.1526, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 036, Train Loss: 0.0658, Test Loss 0.1526, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 037, Train Loss: 0.0658, Test Loss 0.1526, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 038, Train Loss: 0.0658, Test Loss 0.1526, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 039, Train Loss: 0.0658, Test Loss 0.1526, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 040, Train Loss: 0.0658, Test Loss 0.1526, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 041, Train Loss: 0.0658, Test Loss 0.1526, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 042, Train Loss: 0.0658, Test Loss 0.1525, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 043, Train Loss: 0.0658, Test Loss 0.1525, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 044, Train Loss: 0.0658, Test Loss 0.1526, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 045, Train Loss: 0.0658, Test Loss 0.1525, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 046, Train Loss: 0.0658, Test Loss 0.1525, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 047, Train Loss: 0.0658, Test Loss 0.1525, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 048, Train Loss: 0.0658, Test Loss 0.1525, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 049, Train Loss: 0.0658, Test Loss 0.1525, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "9\n",
      "Epoch: 000, Train Loss: 0.0907, Test Loss 0.2022, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 001, Train Loss: 0.0862, Test Loss 0.2312, Train Acc: 0.5568, Test Acc: 0.3333\n",
      "Epoch: 002, Train Loss: 0.0864, Test Loss 0.2203, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 003, Train Loss: 0.0861, Test Loss 0.2203, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 004, Train Loss: 0.0855, Test Loss 0.2289, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 005, Train Loss: 0.0850, Test Loss 0.2238, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 006, Train Loss: 0.0845, Test Loss 0.2184, Train Acc: 0.6250, Test Acc: 1.0000\n",
      "Epoch: 007, Train Loss: 0.0852, Test Loss 0.2086, Train Acc: 0.5341, Test Acc: 0.6667\n",
      "Epoch: 008, Train Loss: 0.0832, Test Loss 0.2183, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 009, Train Loss: 0.0828, Test Loss 0.2092, Train Acc: 0.6477, Test Acc: 1.0000\n",
      "Epoch: 010, Train Loss: 0.0823, Test Loss 0.2006, Train Acc: 0.6250, Test Acc: 1.0000\n",
      "Epoch: 011, Train Loss: 0.0802, Test Loss 0.2000, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 012, Train Loss: 0.0803, Test Loss 0.1964, Train Acc: 0.6705, Test Acc: 1.0000\n",
      "Epoch: 013, Train Loss: 0.0800, Test Loss 0.1953, Train Acc: 0.6932, Test Acc: 1.0000\n",
      "Epoch: 014, Train Loss: 0.0794, Test Loss 0.1946, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 015, Train Loss: 0.0789, Test Loss 0.1929, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 016, Train Loss: 0.0785, Test Loss 0.1906, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 017, Train Loss: 0.0781, Test Loss 0.1857, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 018, Train Loss: 0.0774, Test Loss 0.1847, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 019, Train Loss: 0.0775, Test Loss 0.1783, Train Acc: 0.6932, Test Acc: 1.0000\n",
      "Epoch: 020, Train Loss: 0.0765, Test Loss 0.1780, Train Acc: 0.6932, Test Acc: 1.0000\n",
      "Epoch: 021, Train Loss: 0.0759, Test Loss 0.1755, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 022, Train Loss: 0.0749, Test Loss 0.1751, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 023, Train Loss: 0.0755, Test Loss 0.1685, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 024, Train Loss: 0.0753, Test Loss 0.1688, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 025, Train Loss: 0.0753, Test Loss 0.1683, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 026, Train Loss: 0.0751, Test Loss 0.1686, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 027, Train Loss: 0.0749, Test Loss 0.1690, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 028, Train Loss: 0.0747, Test Loss 0.1692, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 029, Train Loss: 0.0748, Test Loss 0.1687, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 030, Train Loss: 0.0746, Test Loss 0.1691, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 031, Train Loss: 0.0745, Test Loss 0.1691, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 032, Train Loss: 0.0746, Test Loss 0.1681, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 033, Train Loss: 0.0745, Test Loss 0.1681, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 034, Train Loss: 0.0745, Test Loss 0.1679, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 035, Train Loss: 0.0744, Test Loss 0.1679, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 036, Train Loss: 0.0744, Test Loss 0.1679, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 037, Train Loss: 0.0744, Test Loss 0.1679, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 038, Train Loss: 0.0744, Test Loss 0.1679, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 039, Train Loss: 0.0744, Test Loss 0.1679, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 040, Train Loss: 0.0744, Test Loss 0.1679, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 041, Train Loss: 0.0744, Test Loss 0.1678, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 042, Train Loss: 0.0744, Test Loss 0.1678, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 043, Train Loss: 0.0744, Test Loss 0.1678, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 044, Train Loss: 0.0744, Test Loss 0.1678, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 045, Train Loss: 0.0744, Test Loss 0.1678, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 046, Train Loss: 0.0744, Test Loss 0.1677, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 047, Train Loss: 0.0744, Test Loss 0.1677, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 048, Train Loss: 0.0744, Test Loss 0.1677, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 049, Train Loss: 0.0744, Test Loss 0.1677, Train Acc: 0.7386, Test Acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "eval_metrics = np.zeros((skf.n_splits, 3))\n",
    "\n",
    "labels = [full_dataset[i].y for i in range(len(full_dataset))]\n",
    "\n",
    "\n",
    "for n_fold, (train_idx, test_idx) in tqdm(enumerate(skf.split(labels, labels))):\n",
    "    model = GATv2(full_dataset.num_features, 128, 8).to(device())\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=50//4, gamma=0.1, last_epoch=-1, verbose=False)\n",
    "\n",
    "    train_loader_ = DataLoader(full_dataset[list(train_idx)], batch_size=8, shuffle=True)\n",
    "    test_loader_ = DataLoader(full_dataset[list(test_idx)], batch_size=8, shuffle=True)\n",
    "    min_v_loss = np.inf\n",
    "    print(n_fold)\n",
    "    pr, rc, acc = [], [], []\n",
    "    for epoch in range(50):\n",
    "        train_epoch(train_loader, model, criterion, optimizer)\n",
    "        train_loss, train_acc, _, _ = eval_epoch(train_loader, model, criterion)\n",
    "        val_loss, test_acc, _, _ = eval_epoch(val_loader, model, criterion)\n",
    "        scheduler.step()\n",
    "        print(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.4f}, Test Loss {val_loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "        #print(f'Train Prec: {train_pr:.3f}, Train Rec: {train_rc:.3f}, Test Prec: {val_pr:.3f}, Test Rec: {val_rc:.3f}')\n",
    "        #rc.append(val_rc)\n",
    "        #pr.append(val_pr)\n",
    "        acc.append(test_acc)\n",
    "        if min_v_loss > val_loss:\n",
    "            min_v_loss = val_loss\n",
    "            best_test_acc = test_acc\n",
    "\n",
    "    eval_metrics[n_fold, 0] = best_test_acc\n",
    "    eval_metrics[n_fold, 1] = np.mean(acc)\n",
    "    eval_metrics[n_fold, 2] = np.std(acc)\n",
    "### eval_metrics[n_fold, 3] ="
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.66666667, 0.65333333, 0.11469767],\n       [1.        , 0.85333333, 0.25086517],\n       [0.66666667, 0.61333333, 0.16812694],\n       [1.        , 0.93333333, 0.18856181],\n       [0.66666667, 0.64666667, 0.07916228],\n       [1.        , 0.89333333, 0.21540659],\n       [0.66666667, 0.87333333, 0.2096558 ],\n       [0.66666667, 0.64666667, 0.1034945 ],\n       [1.        , 0.9       , 0.20275875],\n       [1.        , 0.96      , 0.12719189]])"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "(0.8333333333333333, 0.16666666666666669)"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(eval_metrics[:, 0]), np.std(eval_metrics[:, 0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "(0.7973333333333333, 0.13176409897152477)"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(eval_metrics[:, 1]), np.std(eval_metrics[:, 1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from captum.attr import Saliency, IntegratedGradients\n",
    "\n",
    "def model_forward(edge_mask, data):\n",
    "    batch = torch.zeros(data.x.shape[0], dtype=int).to(device())\n",
    "    out = model(data) # .x, data.edge_index, batch, edge_mask\n",
    "    return out\n",
    "\n",
    "\n",
    "def explain(method, data, target=0):\n",
    "    input_mask = torch.ones(data.edge_index.shape[1]).requires_grad_(True).to(device)\n",
    "    if method == 'ig':\n",
    "        ig = IntegratedGradients(model_forward)\n",
    "        mask = ig.attribute(input_mask, target=target,\n",
    "                            additional_forward_args=(data,),\n",
    "                            internal_batch_size=data.edge_index.shape[1])\n",
    "    elif method == 'saliency':\n",
    "        saliency = Saliency(model_forward)\n",
    "        mask = saliency.attribute(input_mask, target=target,\n",
    "                                  additional_forward_args=(data,))\n",
    "    else:\n",
    "        raise Exception('Unknown explanation method')\n",
    "\n",
    "    edge_mask = np.abs(mask.cpu().detach().numpy())\n",
    "    if edge_mask.max() > 0:  # avoid division by zero\n",
    "        edge_mask = edge_mask / edge_mask.max()\n",
    "    return edge_mask"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
