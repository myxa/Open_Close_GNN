{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.16 (default, Jan 17 2023, 16:06:28) [MSC v.1916 64 bit (AMD64)] on win32\n"
     ]
    }
   ],
   "source": [
    "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
    "sys.path.extend(['C:\\\\Users\\\\user\\\\PycharmProjects\\\\Open_Close_GNN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from model.dataset import OpenCloseDataset\n",
    "from model.gnn_model import GCN, GATv2\n",
    "import os\n",
    "from model.utils import train, device, train_epoch, eval_epoch, cross_val\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import lr_scheduler\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from seaborn import heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import dense_to_sparse\n",
    "from scipy.io import loadmat\n",
    "from torch_geometric.data import Dataset, Data\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "class newOpenCloseDataset(Dataset):\n",
    "    def __init__(self, datafolder, reload=False, test=False, transform=None, pre_transform=None, k_degree=10):\n",
    "        self.reload = reload\n",
    "        self.test = test\n",
    "        self.datafolder = datafolder\n",
    "        self.close = loadmat(f'{datafolder}/raw/resultsROI_Condition001.mat')['Z']\n",
    "        self.open  = loadmat(f'{datafolder}/raw/resultsROI_Condition002.mat')['Z']\n",
    "        self.edge_attr = None\n",
    "        self.k_degree = k_degree\n",
    "\n",
    "        super().__init__(root=datafolder, transform=transform, pre_transform=pre_transform)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['resultsROI_Condition001.mat', 'resultsROI_Condition002.mat']\n",
    "    \n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        if self.reload:\n",
    "            return [f'data_{i}.pt' for i in range(84*2)]\n",
    "        else:\n",
    "            return [f'data_{i}.pt' for i in range(84*2)]\n",
    "        \n",
    "    def len(self):\n",
    "        return 84 + 84\n",
    "    \n",
    "    def download(self):\n",
    "        print('yo')\n",
    "    \n",
    "    def process(self):\n",
    "\n",
    "        for index in range(84):\n",
    "            _ = self._load_and_save(index, 'open')\n",
    "\n",
    "        for index in range(84):\n",
    "            _ = self._load_and_save(index, 'close')\n",
    "\n",
    "    def _load_and_save(self, index, state):\n",
    "\n",
    "        if state == 'open':\n",
    "            matr = self.open[:, :, index]\n",
    "        elif state == 'close':\n",
    "            matr = self.close[:, :, index]\n",
    "\n",
    "        np.fill_diagonal(matr, 0)\n",
    "\n",
    "        x = torch.from_numpy(matr).float()\n",
    "\n",
    "        if self.k_degree is not None:\n",
    "            adj = self.compute_KNN_graph(matr, k_degree=self.k_degree)\n",
    "            adj = torch.from_numpy(adj).float()\n",
    "            edge_index, edge_attr = dense_to_sparse(adj)\n",
    "            self.edge_attr = edge_attr\n",
    "        else:\n",
    "            edge_index = self._adjacency_threshold(x)\n",
    "\n",
    "        label = torch.tensor(0 if state == 'close' else 1).long()\n",
    "\n",
    "        data = Data(x=x, edge_index=edge_index, edge_attr=self.edge_attr, y=label)\n",
    "\n",
    "        index = index + 84 if state == 'close' else index\n",
    "        if self.test:\n",
    "            torch.save(data,\n",
    "                       os.path.join(self.processed_dir, 'test',\n",
    "                                    f'data_{index}.pt'))\n",
    "        else:\n",
    "            torch.save(data,\n",
    "                       os.path.join(self.processed_dir,\n",
    "                                    f'data_{index}.pt'))\n",
    "        return data\n",
    "    \n",
    "    def compute_KNN_graph(self, matrix, k_degree):\n",
    "        \"\"\" Calculate the adjacency matrix from the connectivity matrix.\"\"\"\n",
    "\n",
    "        matrix = np.abs(matrix)\n",
    "        idx = np.argsort(-matrix)[:, 0:k_degree]\n",
    "        matrix.sort()\n",
    "        matrix = matrix[:, ::-1]\n",
    "        matrix = matrix[:, 0:k_degree]\n",
    "\n",
    "        A = self._adjacency(matrix, idx).astype(np.float32)\n",
    "\n",
    "        return A\n",
    "\n",
    "    def _adjacency(self, dist, idx):\n",
    "\n",
    "        m, k = dist.shape\n",
    "        assert m, k == idx.shape\n",
    "        assert dist.min() >= 0\n",
    "\n",
    "        # Weight matrix.\n",
    "        I = np.arange(0, m).repeat(k)\n",
    "        J = idx.reshape(m * k)\n",
    "        V = dist.reshape(m * k)\n",
    "        W = coo_matrix((V, (I, J)), shape=(m, m))\n",
    "\n",
    "        # No self-connections.\n",
    "        W.setdiag(0)\n",
    "\n",
    "        # Non-directed graph.\n",
    "        bigger = W.T > W\n",
    "        W = W - W.multiply(bigger) + W.T.multiply(bigger)\n",
    "\n",
    "        return W.todense()\n",
    "\n",
    "    def _adjacency_threshold(self, matr, threshold=0.5):\n",
    "        # todo optimize ???\n",
    "        idx = []\n",
    "        for i in range(len(matr)):\n",
    "            for j in range(len(matr)):\n",
    "                if abs(matr[i, j]) > threshold:\n",
    "                    idx.append((i, j))\n",
    "\n",
    "        return torch.tensor(idx).long().t().contiguous()\n",
    "\n",
    "    def get(self, idx):\n",
    "        \"\"\" - Equivalent to __getitem__ in pytorch\n",
    "            - Is not needed for PyG's InMemoryDataset\n",
    "        \"\"\"\n",
    "        if self.test:\n",
    "            data = torch.load(os.path.join(self.processed_dir,\n",
    "                                           f'data_test_{idx}.pt'))\n",
    "        else:\n",
    "            data = torch.load(os.path.join(self.processed_dir,\n",
    "                                           f'data_{idx}.pt'))\n",
    "        return data\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "full_dataset = newOpenCloseDataset(datafolder='../data/new', reload=True, k_degree=10).shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "426"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset.num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#full_dataset = OpenCloseDataset(datafolder='../data/old', reload=False, k_degree=10).shuffle()\n",
    "#len(os.listdir('../data/old/processed'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_size = int(0.94 * len(full_dataset))\n",
    "train_dataset, val_dataset = full_dataset[:train_size], full_dataset[train_size:]\n",
    "#val_dataset, test_dataset = val_dataset[:-3], val_dataset[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-02.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "118394"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GCN(full_dataset.num_features, channels=[256, 32, 8], dropout=0.1).to(device())\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "epochs = 40\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=epochs//4, gamma=0.1, last_epoch=-1, verbose=True)\n",
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b95a52acff7b4f5bbf4bb9af7b916fc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Epoch: 001, Train Loss: 0.0191, Test Loss 0.0587, Train Acc: 0.8024, Test Acc: 0.9091\n",
      "Test precision: 1.0000, Test recall: 0.8333\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Epoch: 002, Train Loss: 0.0142, Test Loss 0.0468, Train Acc: 0.9043, Test Acc: 0.9091\n",
      "Test precision: 0.8571, Test recall: 1.0000\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Epoch: 003, Train Loss: 0.0086, Test Loss 0.0415, Train Acc: 1.0000, Test Acc: 0.6364\n",
      "Test precision: 0.6000, Test recall: 1.0000\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Epoch: 004, Train Loss: 0.0032, Test Loss 0.0550, Train Acc: 1.0000, Test Acc: 0.6364\n",
      "Test precision: 0.6000, Test recall: 1.0000\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Epoch: 005, Train Loss: 0.0003, Test Loss 0.0351, Train Acc: 1.0000, Test Acc: 0.8182\n",
      "Test precision: 0.7500, Test recall: 1.0000\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Epoch: 006, Train Loss: 0.0000, Test Loss 0.0260, Train Acc: 1.0000, Test Acc: 0.9091\n",
      "Test precision: 0.8571, Test recall: 1.0000\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Epoch: 007, Train Loss: 0.0000, Test Loss 0.0430, Train Acc: 1.0000, Test Acc: 0.9091\n",
      "Test precision: 0.8571, Test recall: 1.0000\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Epoch: 008, Train Loss: 0.0000, Test Loss 0.0532, Train Acc: 1.0000, Test Acc: 0.7273\n",
      "Test precision: 0.6667, Test recall: 1.0000\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Epoch: 009, Train Loss: 0.0000, Test Loss 0.0574, Train Acc: 1.0000, Test Acc: 0.7273\n",
      "Test precision: 0.6667, Test recall: 1.0000\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch: 010, Train Loss: 0.0000, Test Loss 0.0615, Train Acc: 1.0000, Test Acc: 0.8182\n",
      "Test precision: 0.7500, Test recall: 1.0000\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch: 011, Train Loss: 0.0000, Test Loss 0.0667, Train Acc: 1.0000, Test Acc: 0.7273\n",
      "Test precision: 0.6667, Test recall: 1.0000\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch: 012, Train Loss: 0.0000, Test Loss 0.0697, Train Acc: 1.0000, Test Acc: 0.7273\n",
      "Test precision: 0.6667, Test recall: 1.0000\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch: 013, Train Loss: 0.0000, Test Loss 0.0703, Train Acc: 1.0000, Test Acc: 0.7273\n",
      "Test precision: 0.6667, Test recall: 1.0000\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch: 014, Train Loss: 0.0000, Test Loss 0.0676, Train Acc: 1.0000, Test Acc: 0.7273\n",
      "Test precision: 0.6667, Test recall: 1.0000\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch: 015, Train Loss: 0.0000, Test Loss 0.0690, Train Acc: 1.0000, Test Acc: 0.7273\n",
      "Test precision: 0.6667, Test recall: 1.0000\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch: 016, Train Loss: 0.0000, Test Loss 0.0661, Train Acc: 1.0000, Test Acc: 0.7273\n",
      "Test precision: 0.6667, Test recall: 1.0000\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch: 017, Train Loss: 0.0000, Test Loss 0.0648, Train Acc: 1.0000, Test Acc: 0.7273\n",
      "Test precision: 0.6667, Test recall: 1.0000\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch: 018, Train Loss: 0.0000, Test Loss 0.0643, Train Acc: 1.0000, Test Acc: 0.7273\n",
      "Test precision: 0.6667, Test recall: 1.0000\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch: 019, Train Loss: 0.0000, Test Loss 0.0673, Train Acc: 1.0000, Test Acc: 0.7273\n",
      "Test precision: 0.6667, Test recall: 1.0000\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 020, Train Loss: 0.0000, Test Loss 0.0667, Train Acc: 1.0000, Test Acc: 0.7273\n",
      "Test precision: 0.6667, Test recall: 1.0000\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 021, Train Loss: 0.0000, Test Loss 0.0652, Train Acc: 1.0000, Test Acc: 0.7273\n",
      "Test precision: 0.6667, Test recall: 1.0000\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 022, Train Loss: 0.0000, Test Loss 0.0646, Train Acc: 1.0000, Test Acc: 0.8182\n",
      "Test precision: 0.7500, Test recall: 1.0000\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 023, Train Loss: 0.0000, Test Loss 0.0634, Train Acc: 1.0000, Test Acc: 0.8182\n",
      "Test precision: 0.7500, Test recall: 1.0000\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 024, Train Loss: 0.0000, Test Loss 0.0642, Train Acc: 1.0000, Test Acc: 0.8182\n",
      "Test precision: 0.7500, Test recall: 1.0000\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 025, Train Loss: 0.0000, Test Loss 0.0660, Train Acc: 1.0000, Test Acc: 0.7273\n",
      "Test precision: 0.6667, Test recall: 1.0000\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 026, Train Loss: 0.0000, Test Loss 0.0645, Train Acc: 1.0000, Test Acc: 0.8182\n",
      "Test precision: 0.7500, Test recall: 1.0000\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 027, Train Loss: 0.0000, Test Loss 0.0660, Train Acc: 1.0000, Test Acc: 0.7273\n",
      "Test precision: 0.6667, Test recall: 1.0000\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 028, Train Loss: 0.0000, Test Loss 0.0639, Train Acc: 1.0000, Test Acc: 0.8182\n",
      "Test precision: 0.7500, Test recall: 1.0000\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 029, Train Loss: 0.0000, Test Loss 0.0635, Train Acc: 1.0000, Test Acc: 0.8182\n",
      "Test precision: 0.7500, Test recall: 1.0000\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 030, Train Loss: 0.0000, Test Loss 0.0673, Train Acc: 1.0000, Test Acc: 0.7273\n",
      "Test precision: 0.6667, Test recall: 1.0000\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 031, Train Loss: 0.0000, Test Loss 0.0657, Train Acc: 1.0000, Test Acc: 0.7273\n",
      "Test precision: 0.6667, Test recall: 1.0000\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 032, Train Loss: 0.0000, Test Loss 0.0623, Train Acc: 1.0000, Test Acc: 0.8182\n",
      "Test precision: 0.7500, Test recall: 1.0000\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 033, Train Loss: 0.0000, Test Loss 0.0630, Train Acc: 1.0000, Test Acc: 0.8182\n",
      "Test precision: 0.7500, Test recall: 1.0000\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 034, Train Loss: 0.0000, Test Loss 0.0684, Train Acc: 1.0000, Test Acc: 0.7273\n",
      "Test precision: 0.6667, Test recall: 1.0000\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 035, Train Loss: 0.0000, Test Loss 0.0673, Train Acc: 1.0000, Test Acc: 0.7273\n",
      "Test precision: 0.6667, Test recall: 1.0000\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 036, Train Loss: 0.0000, Test Loss 0.0654, Train Acc: 1.0000, Test Acc: 0.7273\n",
      "Test precision: 0.6667, Test recall: 1.0000\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 037, Train Loss: 0.0000, Test Loss 0.0647, Train Acc: 1.0000, Test Acc: 0.8182\n",
      "Test precision: 0.7500, Test recall: 1.0000\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 038, Train Loss: 0.0000, Test Loss 0.0639, Train Acc: 1.0000, Test Acc: 0.8182\n",
      "Test precision: 0.7500, Test recall: 1.0000\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 039, Train Loss: 0.0000, Test Loss 0.0650, Train Acc: 1.0000, Test Acc: 0.8182\n",
      "Test precision: 0.7500, Test recall: 1.0000\n",
      "Adjusting learning rate of group 0 to 1.0000e-06.\n",
      "Epoch: 040, Train Loss: 0.0000, Test Loss 0.0606, Train Acc: 1.0000, Test Acc: 0.8182\n",
      "Test precision: 0.7500, Test recall: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history = train(model, epochs, train_loader, val_loader, loss, optimizer, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loss, val_loss, train_acc, test_acc = zip(*history)\n",
    "# k = None no dropuot\n",
    "plt.plot(train_loss)\n",
    "plt.plot(val_loss);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_acc)\n",
    "plt.plot(test_acc);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220722"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GATv2(full_dataset.num_features, 128, 8).to(device())\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "epochs = 40\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=epochs//4, gamma=0.1, last_epoch=-1, verbose=False)\n",
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd684b0d9b454d3daaa118d4d64b114b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Loss: 0.0235, Test Loss 0.1103, Train Acc: 0.4886, Test Acc: 0.6667\n",
      "Epoch: 002, Train Loss: 0.0226, Test Loss 0.1197, Train Acc: 0.6591, Test Acc: 0.3333\n",
      "Epoch: 003, Train Loss: 0.0217, Test Loss 0.1306, Train Acc: 0.6477, Test Acc: 0.3333\n",
      "Epoch: 004, Train Loss: 0.0205, Test Loss 0.1211, Train Acc: 0.7159, Test Acc: 0.5000\n",
      "Epoch: 005, Train Loss: 0.0209, Test Loss 0.1129, Train Acc: 0.5795, Test Acc: 0.6667\n",
      "Epoch: 006, Train Loss: 0.0171, Test Loss 0.1398, Train Acc: 0.8750, Test Acc: 0.3333\n",
      "Epoch: 007, Train Loss: 0.0167, Test Loss 0.1238, Train Acc: 0.7386, Test Acc: 0.5000\n",
      "Epoch: 008, Train Loss: 0.0152, Test Loss 0.1208, Train Acc: 0.7955, Test Acc: 0.5000\n",
      "Epoch: 009, Train Loss: 0.0124, Test Loss 0.1614, Train Acc: 0.9205, Test Acc: 0.3333\n",
      "Epoch: 010, Train Loss: 0.0105, Test Loss 0.1592, Train Acc: 0.9659, Test Acc: 0.5000\n",
      "Epoch: 011, Train Loss: 0.0084, Test Loss 0.1703, Train Acc: 0.9886, Test Acc: 0.5000\n",
      "Epoch: 012, Train Loss: 0.0071, Test Loss 0.1517, Train Acc: 0.9659, Test Acc: 0.6667\n",
      "Epoch: 013, Train Loss: 0.0045, Test Loss 0.1826, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 014, Train Loss: 0.0032, Test Loss 0.2240, Train Acc: 1.0000, Test Acc: 0.5000\n",
      "Epoch: 015, Train Loss: 0.0028, Test Loss 0.1990, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 016, Train Loss: 0.0019, Test Loss 0.2210, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 017, Train Loss: 0.0016, Test Loss 0.2919, Train Acc: 1.0000, Test Acc: 0.5000\n",
      "Epoch: 018, Train Loss: 0.0019, Test Loss 0.3517, Train Acc: 1.0000, Test Acc: 0.5000\n",
      "Epoch: 019, Train Loss: 0.0008, Test Loss 0.3261, Train Acc: 1.0000, Test Acc: 0.5000\n",
      "Epoch: 020, Train Loss: 0.0007, Test Loss 0.2985, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 021, Train Loss: 0.0005, Test Loss 0.3158, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 022, Train Loss: 0.0005, Test Loss 0.3789, Train Acc: 1.0000, Test Acc: 0.5000\n",
      "Epoch: 023, Train Loss: 0.0013, Test Loss 0.4599, Train Acc: 1.0000, Test Acc: 0.5000\n",
      "Epoch: 024, Train Loss: 0.0003, Test Loss 0.4063, Train Acc: 1.0000, Test Acc: 0.5000\n",
      "Epoch: 025, Train Loss: 0.0004, Test Loss 0.3520, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 026, Train Loss: 0.0003, Test Loss 0.3682, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 027, Train Loss: 0.0002, Test Loss 0.4074, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 028, Train Loss: 0.0002, Test Loss 0.4326, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 029, Train Loss: 0.0001, Test Loss 0.4364, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 030, Train Loss: 0.0001, Test Loss 0.4230, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 031, Train Loss: 0.0002, Test Loss 0.4087, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 032, Train Loss: 0.0002, Test Loss 0.4036, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 033, Train Loss: 0.0001, Test Loss 0.4097, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 034, Train Loss: 0.0001, Test Loss 0.4259, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 035, Train Loss: 0.0001, Test Loss 0.4392, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 036, Train Loss: 0.0001, Test Loss 0.4370, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 037, Train Loss: 0.0001, Test Loss 0.4389, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 038, Train Loss: 0.0001, Test Loss 0.4398, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 039, Train Loss: 0.0001, Test Loss 0.4334, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 040, Train Loss: 0.0001, Test Loss 0.4350, Train Acc: 1.0000, Test Acc: 0.6667\n"
     ]
    }
   ],
   "source": [
    "history = train(model, epochs, train_loader, val_loader, loss, optimizer, scheduler=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loss, val_loss = zip(*history)\n",
    "\n",
    "plt.plot(train_loss)\n",
    "plt.plot(val_loss);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '../weights/gat_2heads_128_8_knn30.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b301190b54244c4b410d148fbadcd6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'lr'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17000\\2619165759.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m cross_val(full_dataset, GCN, lr=0.001, step_size=50//4, gamma=0.1, last_epoch=-1, verbose=False,\n\u001b[1;32m----> 2\u001b[1;33m           num_features=full_dataset.num_features, channels=[256, 32, 8], dropout=0.3)\n\u001b[0m",
      "\u001b[1;32m~\\PycharmProjects\\Open_Close_GNN\\model\\utils.py\u001b[0m in \u001b[0;36mcross_val\u001b[1;34m(data, model_name, n_splits, epochs, batch_size, **kwargs)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mn_fold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_idx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mskf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m         \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[0mcriterion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'lr'"
     ]
    }
   ],
   "source": [
    "cross_val(full_dataset, GCN, lr=0.001, step_size=50//4, gamma=0.1, last_epoch=-1, verbose=False,\n",
    "          num_features=full_dataset.num_features, channels=[256, 32, 8], dropout=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e79c08e398d24141950bddd238dbc5f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Train Loss: 0.0865, Test Loss 0.2112, Train Acc: 0.4545, Test Acc: 0.6667\n",
      "Epoch: 001, Train Loss: 0.0863, Test Loss 0.2283, Train Acc: 0.4886, Test Acc: 0.3333\n",
      "Epoch: 002, Train Loss: 0.0859, Test Loss 0.2202, Train Acc: 0.5114, Test Acc: 0.6667\n",
      "Epoch: 003, Train Loss: 0.0856, Test Loss 0.2265, Train Acc: 0.5227, Test Acc: 0.6667\n",
      "Epoch: 004, Train Loss: 0.0854, Test Loss 0.2065, Train Acc: 0.5568, Test Acc: 1.0000\n",
      "Epoch: 005, Train Loss: 0.0848, Test Loss 0.2246, Train Acc: 0.5455, Test Acc: 0.6667\n",
      "Epoch: 006, Train Loss: 0.0840, Test Loss 0.2156, Train Acc: 0.6136, Test Acc: 0.6667\n",
      "Epoch: 007, Train Loss: 0.0831, Test Loss 0.2139, Train Acc: 0.6364, Test Acc: 0.6667\n",
      "Epoch: 008, Train Loss: 0.0820, Test Loss 0.2101, Train Acc: 0.6477, Test Acc: 0.6667\n",
      "Epoch: 009, Train Loss: 0.0819, Test Loss 0.2072, Train Acc: 0.6364, Test Acc: 0.6667\n",
      "Epoch: 010, Train Loss: 0.0820, Test Loss 0.1909, Train Acc: 0.6136, Test Acc: 1.0000\n",
      "Epoch: 011, Train Loss: 0.0778, Test Loss 0.2059, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 012, Train Loss: 0.0778, Test Loss 0.1972, Train Acc: 0.7045, Test Acc: 0.6667\n",
      "Epoch: 013, Train Loss: 0.0768, Test Loss 0.2073, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 014, Train Loss: 0.0763, Test Loss 0.2080, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 015, Train Loss: 0.0753, Test Loss 0.2019, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 016, Train Loss: 0.0746, Test Loss 0.1985, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 017, Train Loss: 0.0740, Test Loss 0.2023, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 018, Train Loss: 0.0733, Test Loss 0.1936, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 019, Train Loss: 0.0727, Test Loss 0.1914, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 020, Train Loss: 0.0720, Test Loss 0.1976, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 021, Train Loss: 0.0717, Test Loss 0.2040, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 022, Train Loss: 0.0710, Test Loss 0.1873, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 023, Train Loss: 0.0704, Test Loss 0.1975, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 024, Train Loss: 0.0703, Test Loss 0.1954, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 025, Train Loss: 0.0702, Test Loss 0.1941, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 026, Train Loss: 0.0702, Test Loss 0.1934, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 027, Train Loss: 0.0701, Test Loss 0.1940, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 028, Train Loss: 0.0701, Test Loss 0.1928, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 029, Train Loss: 0.0700, Test Loss 0.1917, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 030, Train Loss: 0.0700, Test Loss 0.1913, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 031, Train Loss: 0.0699, Test Loss 0.1918, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 032, Train Loss: 0.0699, Test Loss 0.1906, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 033, Train Loss: 0.0698, Test Loss 0.1903, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 034, Train Loss: 0.0698, Test Loss 0.1894, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 035, Train Loss: 0.0697, Test Loss 0.1898, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 036, Train Loss: 0.0697, Test Loss 0.1898, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 037, Train Loss: 0.0697, Test Loss 0.1898, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 038, Train Loss: 0.0697, Test Loss 0.1899, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 039, Train Loss: 0.0697, Test Loss 0.1899, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 040, Train Loss: 0.0697, Test Loss 0.1899, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 041, Train Loss: 0.0697, Test Loss 0.1900, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 042, Train Loss: 0.0697, Test Loss 0.1900, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 043, Train Loss: 0.0697, Test Loss 0.1900, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 044, Train Loss: 0.0697, Test Loss 0.1899, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 045, Train Loss: 0.0697, Test Loss 0.1900, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 046, Train Loss: 0.0697, Test Loss 0.1899, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 047, Train Loss: 0.0697, Test Loss 0.1899, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 048, Train Loss: 0.0697, Test Loss 0.1899, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 049, Train Loss: 0.0697, Test Loss 0.1900, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 000, Train Loss: 0.0863, Test Loss 0.2153, Train Acc: 0.5000, Test Acc: 1.0000\n",
      "Epoch: 001, Train Loss: 0.0860, Test Loss 0.2284, Train Acc: 0.5455, Test Acc: 0.3333\n",
      "Epoch: 002, Train Loss: 0.0859, Test Loss 0.2244, Train Acc: 0.5682, Test Acc: 0.6667\n",
      "Epoch: 003, Train Loss: 0.0857, Test Loss 0.2200, Train Acc: 0.5455, Test Acc: 0.6667\n",
      "Epoch: 004, Train Loss: 0.0855, Test Loss 0.2143, Train Acc: 0.5455, Test Acc: 1.0000\n",
      "Epoch: 005, Train Loss: 0.0853, Test Loss 0.2111, Train Acc: 0.6023, Test Acc: 1.0000\n",
      "Epoch: 006, Train Loss: 0.0847, Test Loss 0.2223, Train Acc: 0.6023, Test Acc: 0.6667\n",
      "Epoch: 007, Train Loss: 0.0842, Test Loss 0.2197, Train Acc: 0.6591, Test Acc: 0.6667\n",
      "Epoch: 008, Train Loss: 0.0838, Test Loss 0.2053, Train Acc: 0.6364, Test Acc: 1.0000\n",
      "Epoch: 009, Train Loss: 0.0824, Test Loss 0.2195, Train Acc: 0.6364, Test Acc: 0.6667\n",
      "Epoch: 010, Train Loss: 0.0825, Test Loss 0.1957, Train Acc: 0.6136, Test Acc: 1.0000\n",
      "Epoch: 011, Train Loss: 0.0801, Test Loss 0.2043, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 012, Train Loss: 0.0800, Test Loss 0.2017, Train Acc: 0.6818, Test Acc: 1.0000\n",
      "Epoch: 013, Train Loss: 0.0796, Test Loss 0.2037, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 014, Train Loss: 0.0793, Test Loss 0.2061, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 015, Train Loss: 0.0790, Test Loss 0.2098, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 016, Train Loss: 0.0784, Test Loss 0.2043, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 017, Train Loss: 0.0779, Test Loss 0.1996, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 018, Train Loss: 0.0775, Test Loss 0.2045, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 019, Train Loss: 0.0771, Test Loss 0.2047, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 020, Train Loss: 0.0767, Test Loss 0.2005, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 021, Train Loss: 0.0763, Test Loss 0.2012, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 022, Train Loss: 0.0765, Test Loss 0.1869, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 023, Train Loss: 0.0756, Test Loss 0.2018, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 024, Train Loss: 0.0755, Test Loss 0.2016, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 025, Train Loss: 0.0755, Test Loss 0.2011, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 026, Train Loss: 0.0754, Test Loss 0.1998, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 027, Train Loss: 0.0754, Test Loss 0.1999, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 028, Train Loss: 0.0753, Test Loss 0.1993, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 029, Train Loss: 0.0753, Test Loss 0.1992, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 030, Train Loss: 0.0752, Test Loss 0.1978, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 031, Train Loss: 0.0752, Test Loss 0.1982, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 032, Train Loss: 0.0752, Test Loss 0.1984, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 033, Train Loss: 0.0751, Test Loss 0.1975, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 034, Train Loss: 0.0751, Test Loss 0.1986, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 035, Train Loss: 0.0750, Test Loss 0.1981, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 036, Train Loss: 0.0750, Test Loss 0.1983, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 037, Train Loss: 0.0750, Test Loss 0.1982, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 038, Train Loss: 0.0750, Test Loss 0.1982, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 039, Train Loss: 0.0750, Test Loss 0.1982, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 040, Train Loss: 0.0750, Test Loss 0.1982, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 041, Train Loss: 0.0750, Test Loss 0.1981, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 042, Train Loss: 0.0750, Test Loss 0.1981, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 043, Train Loss: 0.0750, Test Loss 0.1981, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 044, Train Loss: 0.0750, Test Loss 0.1981, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 045, Train Loss: 0.0750, Test Loss 0.1981, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 046, Train Loss: 0.0750, Test Loss 0.1981, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 047, Train Loss: 0.0750, Test Loss 0.1980, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 048, Train Loss: 0.0750, Test Loss 0.1981, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 049, Train Loss: 0.0750, Test Loss 0.1980, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 000, Train Loss: 0.0869, Test Loss 0.2471, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 001, Train Loss: 0.0866, Test Loss 0.2323, Train Acc: 0.4432, Test Acc: 0.3333\n",
      "Epoch: 002, Train Loss: 0.0865, Test Loss 0.2306, Train Acc: 0.5568, Test Acc: 0.6667\n",
      "Epoch: 003, Train Loss: 0.0862, Test Loss 0.2260, Train Acc: 0.5114, Test Acc: 0.6667\n",
      "Epoch: 004, Train Loss: 0.0861, Test Loss 0.2290, Train Acc: 0.6364, Test Acc: 0.6667\n",
      "Epoch: 005, Train Loss: 0.0863, Test Loss 0.2177, Train Acc: 0.5114, Test Acc: 0.6667\n",
      "Epoch: 006, Train Loss: 0.0857, Test Loss 0.2294, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 007, Train Loss: 0.0851, Test Loss 0.2198, Train Acc: 0.5341, Test Acc: 0.6667\n",
      "Epoch: 008, Train Loss: 0.0844, Test Loss 0.2124, Train Acc: 0.5795, Test Acc: 1.0000\n",
      "Epoch: 009, Train Loss: 0.0841, Test Loss 0.2324, Train Acc: 0.6023, Test Acc: 0.3333\n",
      "Epoch: 010, Train Loss: 0.0852, Test Loss 0.2402, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 011, Train Loss: 0.0844, Test Loss 0.2322, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 012, Train Loss: 0.0843, Test Loss 0.2316, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 013, Train Loss: 0.0840, Test Loss 0.2297, Train Acc: 0.5114, Test Acc: 0.3333\n",
      "Epoch: 014, Train Loss: 0.0836, Test Loss 0.2273, Train Acc: 0.5227, Test Acc: 0.3333\n",
      "Epoch: 015, Train Loss: 0.0828, Test Loss 0.2249, Train Acc: 0.5227, Test Acc: 0.3333\n",
      "Epoch: 016, Train Loss: 0.0820, Test Loss 0.2288, Train Acc: 0.5455, Test Acc: 0.3333\n",
      "Epoch: 017, Train Loss: 0.0817, Test Loss 0.2318, Train Acc: 0.5455, Test Acc: 0.3333\n",
      "Epoch: 018, Train Loss: 0.0816, Test Loss 0.2173, Train Acc: 0.6818, Test Acc: 1.0000\n",
      "Epoch: 019, Train Loss: 0.0811, Test Loss 0.2178, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 020, Train Loss: 0.0805, Test Loss 0.2243, Train Acc: 0.6023, Test Acc: 0.3333\n",
      "Epoch: 021, Train Loss: 0.0802, Test Loss 0.2208, Train Acc: 0.6477, Test Acc: 0.3333\n",
      "Epoch: 022, Train Loss: 0.0799, Test Loss 0.2211, Train Acc: 0.6364, Test Acc: 0.3333\n",
      "Epoch: 023, Train Loss: 0.0798, Test Loss 0.2146, Train Acc: 0.6818, Test Acc: 0.6667\n",
      "Epoch: 024, Train Loss: 0.0797, Test Loss 0.2152, Train Acc: 0.6932, Test Acc: 0.6667\n",
      "Epoch: 025, Train Loss: 0.0796, Test Loss 0.2161, Train Acc: 0.6932, Test Acc: 0.6667\n",
      "Epoch: 026, Train Loss: 0.0796, Test Loss 0.2161, Train Acc: 0.6932, Test Acc: 0.6667\n",
      "Epoch: 027, Train Loss: 0.0795, Test Loss 0.2165, Train Acc: 0.6932, Test Acc: 0.6667\n",
      "Epoch: 028, Train Loss: 0.0795, Test Loss 0.2168, Train Acc: 0.6818, Test Acc: 0.6667\n",
      "Epoch: 029, Train Loss: 0.0794, Test Loss 0.2174, Train Acc: 0.6818, Test Acc: 0.6667\n",
      "Epoch: 030, Train Loss: 0.0794, Test Loss 0.2176, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 031, Train Loss: 0.0794, Test Loss 0.2173, Train Acc: 0.6818, Test Acc: 0.6667\n",
      "Epoch: 032, Train Loss: 0.0793, Test Loss 0.2172, Train Acc: 0.6818, Test Acc: 0.6667\n",
      "Epoch: 033, Train Loss: 0.0793, Test Loss 0.2175, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 034, Train Loss: 0.0793, Test Loss 0.2173, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 035, Train Loss: 0.0793, Test Loss 0.2175, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 036, Train Loss: 0.0793, Test Loss 0.2175, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 037, Train Loss: 0.0793, Test Loss 0.2175, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 038, Train Loss: 0.0793, Test Loss 0.2174, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 039, Train Loss: 0.0792, Test Loss 0.2174, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 040, Train Loss: 0.0792, Test Loss 0.2174, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 041, Train Loss: 0.0792, Test Loss 0.2174, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 042, Train Loss: 0.0792, Test Loss 0.2174, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 043, Train Loss: 0.0792, Test Loss 0.2174, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 044, Train Loss: 0.0792, Test Loss 0.2174, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 045, Train Loss: 0.0792, Test Loss 0.2174, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 046, Train Loss: 0.0792, Test Loss 0.2174, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 047, Train Loss: 0.0792, Test Loss 0.2173, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 048, Train Loss: 0.0792, Test Loss 0.2173, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 049, Train Loss: 0.0792, Test Loss 0.2173, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 000, Train Loss: 0.0867, Test Loss 0.2206, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 001, Train Loss: 0.0865, Test Loss 0.2273, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 002, Train Loss: 0.0863, Test Loss 0.2260, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 003, Train Loss: 0.0862, Test Loss 0.2211, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 004, Train Loss: 0.0860, Test Loss 0.2263, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 005, Train Loss: 0.0859, Test Loss 0.2191, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 006, Train Loss: 0.0863, Test Loss 0.2157, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 007, Train Loss: 0.0860, Test Loss 0.2184, Train Acc: 0.5114, Test Acc: 0.6667\n",
      "Epoch: 008, Train Loss: 0.0862, Test Loss 0.2302, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 009, Train Loss: 0.0857, Test Loss 0.2373, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 010, Train Loss: 0.0854, Test Loss 0.2252, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 011, Train Loss: 0.0853, Test Loss 0.2308, Train Acc: 0.6591, Test Acc: 0.6667\n",
      "Epoch: 012, Train Loss: 0.0851, Test Loss 0.2289, Train Acc: 0.6932, Test Acc: 0.6667\n",
      "Epoch: 013, Train Loss: 0.0847, Test Loss 0.2260, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 014, Train Loss: 0.0843, Test Loss 0.2246, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 015, Train Loss: 0.0839, Test Loss 0.2243, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 016, Train Loss: 0.0836, Test Loss 0.2198, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 017, Train Loss: 0.0833, Test Loss 0.2245, Train Acc: 0.6818, Test Acc: 0.6667\n",
      "Epoch: 018, Train Loss: 0.0830, Test Loss 0.2167, Train Acc: 0.7614, Test Acc: 1.0000\n",
      "Epoch: 019, Train Loss: 0.0828, Test Loss 0.2215, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 020, Train Loss: 0.0825, Test Loss 0.2209, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 021, Train Loss: 0.0823, Test Loss 0.2176, Train Acc: 0.7614, Test Acc: 1.0000\n",
      "Epoch: 022, Train Loss: 0.0821, Test Loss 0.2176, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 023, Train Loss: 0.0821, Test Loss 0.2079, Train Acc: 0.6818, Test Acc: 1.0000\n",
      "Epoch: 024, Train Loss: 0.0820, Test Loss 0.2085, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 025, Train Loss: 0.0819, Test Loss 0.2094, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 026, Train Loss: 0.0819, Test Loss 0.2102, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 027, Train Loss: 0.0818, Test Loss 0.2106, Train Acc: 0.7614, Test Acc: 1.0000\n",
      "Epoch: 028, Train Loss: 0.0818, Test Loss 0.2108, Train Acc: 0.7614, Test Acc: 1.0000\n",
      "Epoch: 029, Train Loss: 0.0818, Test Loss 0.2112, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 030, Train Loss: 0.0817, Test Loss 0.2117, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 031, Train Loss: 0.0817, Test Loss 0.2124, Train Acc: 0.8068, Test Acc: 1.0000\n",
      "Epoch: 032, Train Loss: 0.0817, Test Loss 0.2123, Train Acc: 0.8068, Test Acc: 1.0000\n",
      "Epoch: 033, Train Loss: 0.0817, Test Loss 0.2122, Train Acc: 0.8068, Test Acc: 1.0000\n",
      "Epoch: 034, Train Loss: 0.0816, Test Loss 0.2123, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 035, Train Loss: 0.0816, Test Loss 0.2130, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 036, Train Loss: 0.0816, Test Loss 0.2130, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 037, Train Loss: 0.0816, Test Loss 0.2130, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 038, Train Loss: 0.0816, Test Loss 0.2130, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 039, Train Loss: 0.0816, Test Loss 0.2130, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 040, Train Loss: 0.0816, Test Loss 0.2130, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 041, Train Loss: 0.0816, Test Loss 0.2130, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 042, Train Loss: 0.0816, Test Loss 0.2130, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 043, Train Loss: 0.0816, Test Loss 0.2130, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 044, Train Loss: 0.0816, Test Loss 0.2130, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 045, Train Loss: 0.0816, Test Loss 0.2130, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 046, Train Loss: 0.0816, Test Loss 0.2130, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 047, Train Loss: 0.0816, Test Loss 0.2130, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 048, Train Loss: 0.0816, Test Loss 0.2130, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 049, Train Loss: 0.0816, Test Loss 0.2130, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 000, Train Loss: 0.0864, Test Loss 0.2116, Train Acc: 0.5568, Test Acc: 1.0000\n",
      "Epoch: 001, Train Loss: 0.0860, Test Loss 0.2201, Train Acc: 0.5114, Test Acc: 0.6667\n",
      "Epoch: 002, Train Loss: 0.0861, Test Loss 0.2337, Train Acc: 0.5114, Test Acc: 0.3333\n",
      "Epoch: 003, Train Loss: 0.0854, Test Loss 0.2180, Train Acc: 0.5114, Test Acc: 0.6667\n",
      "Epoch: 004, Train Loss: 0.0852, Test Loss 0.2149, Train Acc: 0.5682, Test Acc: 0.6667\n",
      "Epoch: 005, Train Loss: 0.0847, Test Loss 0.2122, Train Acc: 0.5909, Test Acc: 1.0000\n",
      "Epoch: 006, Train Loss: 0.0845, Test Loss 0.2303, Train Acc: 0.5568, Test Acc: 0.3333\n",
      "Epoch: 007, Train Loss: 0.0836, Test Loss 0.2120, Train Acc: 0.6023, Test Acc: 0.6667\n",
      "Epoch: 008, Train Loss: 0.0832, Test Loss 0.2296, Train Acc: 0.5909, Test Acc: 0.3333\n",
      "Epoch: 009, Train Loss: 0.0820, Test Loss 0.2148, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 010, Train Loss: 0.0829, Test Loss 0.2136, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 011, Train Loss: 0.0822, Test Loss 0.2070, Train Acc: 0.6364, Test Acc: 1.0000\n",
      "Epoch: 012, Train Loss: 0.0820, Test Loss 0.2078, Train Acc: 0.6364, Test Acc: 0.6667\n",
      "Epoch: 013, Train Loss: 0.0818, Test Loss 0.2091, Train Acc: 0.6364, Test Acc: 0.6667\n",
      "Epoch: 014, Train Loss: 0.0815, Test Loss 0.2140, Train Acc: 0.7159, Test Acc: 0.6667\n",
      "Epoch: 015, Train Loss: 0.0813, Test Loss 0.2164, Train Acc: 0.7045, Test Acc: 0.6667\n",
      "Epoch: 016, Train Loss: 0.0811, Test Loss 0.2134, Train Acc: 0.7159, Test Acc: 0.6667\n",
      "Epoch: 017, Train Loss: 0.0809, Test Loss 0.2134, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 018, Train Loss: 0.0808, Test Loss 0.2163, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 019, Train Loss: 0.0805, Test Loss 0.2096, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 020, Train Loss: 0.0801, Test Loss 0.2099, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 021, Train Loss: 0.0799, Test Loss 0.2125, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 022, Train Loss: 0.0797, Test Loss 0.2099, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 023, Train Loss: 0.0795, Test Loss 0.2097, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 024, Train Loss: 0.0794, Test Loss 0.2091, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 025, Train Loss: 0.0794, Test Loss 0.2087, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 026, Train Loss: 0.0794, Test Loss 0.2089, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 027, Train Loss: 0.0794, Test Loss 0.2090, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 028, Train Loss: 0.0793, Test Loss 0.2092, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 029, Train Loss: 0.0793, Test Loss 0.2092, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 030, Train Loss: 0.0793, Test Loss 0.2088, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 031, Train Loss: 0.0793, Test Loss 0.2093, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 032, Train Loss: 0.0793, Test Loss 0.2094, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 033, Train Loss: 0.0792, Test Loss 0.2091, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 034, Train Loss: 0.0792, Test Loss 0.2088, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 035, Train Loss: 0.0792, Test Loss 0.2089, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 036, Train Loss: 0.0792, Test Loss 0.2088, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 037, Train Loss: 0.0792, Test Loss 0.2089, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 038, Train Loss: 0.0792, Test Loss 0.2088, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 039, Train Loss: 0.0792, Test Loss 0.2089, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 040, Train Loss: 0.0792, Test Loss 0.2089, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 041, Train Loss: 0.0792, Test Loss 0.2089, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 042, Train Loss: 0.0792, Test Loss 0.2089, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 043, Train Loss: 0.0792, Test Loss 0.2089, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 044, Train Loss: 0.0792, Test Loss 0.2089, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 045, Train Loss: 0.0792, Test Loss 0.2089, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 046, Train Loss: 0.0792, Test Loss 0.2089, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 047, Train Loss: 0.0792, Test Loss 0.2089, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 048, Train Loss: 0.0792, Test Loss 0.2089, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 049, Train Loss: 0.0792, Test Loss 0.2089, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 000, Train Loss: 0.0868, Test Loss 0.2340, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 001, Train Loss: 0.0862, Test Loss 0.2165, Train Acc: 0.5682, Test Acc: 1.0000\n",
      "Epoch: 002, Train Loss: 0.0856, Test Loss 0.2187, Train Acc: 0.5341, Test Acc: 1.0000\n",
      "Epoch: 003, Train Loss: 0.0851, Test Loss 0.2212, Train Acc: 0.5909, Test Acc: 0.6667\n",
      "Epoch: 004, Train Loss: 0.0845, Test Loss 0.2189, Train Acc: 0.6364, Test Acc: 0.6667\n",
      "Epoch: 005, Train Loss: 0.0838, Test Loss 0.2228, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 006, Train Loss: 0.0830, Test Loss 0.2043, Train Acc: 0.5909, Test Acc: 1.0000\n",
      "Epoch: 007, Train Loss: 0.0824, Test Loss 0.2260, Train Acc: 0.6364, Test Acc: 0.6667\n",
      "Epoch: 008, Train Loss: 0.0802, Test Loss 0.2207, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 009, Train Loss: 0.0771, Test Loss 0.2065, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 010, Train Loss: 0.0742, Test Loss 0.2101, Train Acc: 0.6477, Test Acc: 0.6667\n",
      "Epoch: 011, Train Loss: 0.0768, Test Loss 0.2059, Train Acc: 0.8068, Test Acc: 0.6667\n",
      "Epoch: 012, Train Loss: 0.0765, Test Loss 0.2093, Train Acc: 0.8182, Test Acc: 0.6667\n",
      "Epoch: 013, Train Loss: 0.0759, Test Loss 0.2012, Train Acc: 0.7955, Test Acc: 0.6667\n",
      "Epoch: 014, Train Loss: 0.0751, Test Loss 0.2021, Train Acc: 0.7955, Test Acc: 0.6667\n",
      "Epoch: 015, Train Loss: 0.0746, Test Loss 0.2096, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 016, Train Loss: 0.0736, Test Loss 0.1971, Train Acc: 0.7955, Test Acc: 0.6667\n",
      "Epoch: 017, Train Loss: 0.0729, Test Loss 0.1985, Train Acc: 0.8068, Test Acc: 0.6667\n",
      "Epoch: 018, Train Loss: 0.0724, Test Loss 0.1930, Train Acc: 0.7955, Test Acc: 0.6667\n",
      "Epoch: 019, Train Loss: 0.0718, Test Loss 0.1978, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 020, Train Loss: 0.0714, Test Loss 0.1983, Train Acc: 0.8182, Test Acc: 0.6667\n",
      "Epoch: 021, Train Loss: 0.0708, Test Loss 0.1967, Train Acc: 0.8409, Test Acc: 0.6667\n",
      "Epoch: 022, Train Loss: 0.0704, Test Loss 0.1847, Train Acc: 0.8182, Test Acc: 1.0000\n",
      "Epoch: 023, Train Loss: 0.0702, Test Loss 0.2003, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 024, Train Loss: 0.0700, Test Loss 0.1981, Train Acc: 0.8068, Test Acc: 0.6667\n",
      "Epoch: 025, Train Loss: 0.0699, Test Loss 0.1957, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 026, Train Loss: 0.0698, Test Loss 0.1949, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 027, Train Loss: 0.0697, Test Loss 0.1929, Train Acc: 0.8409, Test Acc: 0.6667\n",
      "Epoch: 028, Train Loss: 0.0696, Test Loss 0.1910, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 029, Train Loss: 0.0695, Test Loss 0.1904, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 030, Train Loss: 0.0695, Test Loss 0.1901, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 031, Train Loss: 0.0694, Test Loss 0.1897, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 032, Train Loss: 0.0694, Test Loss 0.1892, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 033, Train Loss: 0.0693, Test Loss 0.1904, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 034, Train Loss: 0.0693, Test Loss 0.1898, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 035, Train Loss: 0.0692, Test Loss 0.1891, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 036, Train Loss: 0.0692, Test Loss 0.1890, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 037, Train Loss: 0.0692, Test Loss 0.1889, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 038, Train Loss: 0.0692, Test Loss 0.1888, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 039, Train Loss: 0.0692, Test Loss 0.1889, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 040, Train Loss: 0.0692, Test Loss 0.1888, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 041, Train Loss: 0.0692, Test Loss 0.1889, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 042, Train Loss: 0.0692, Test Loss 0.1888, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 043, Train Loss: 0.0692, Test Loss 0.1887, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 044, Train Loss: 0.0692, Test Loss 0.1888, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 045, Train Loss: 0.0692, Test Loss 0.1887, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 046, Train Loss: 0.0692, Test Loss 0.1887, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 047, Train Loss: 0.0692, Test Loss 0.1887, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 048, Train Loss: 0.0692, Test Loss 0.1887, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 049, Train Loss: 0.0692, Test Loss 0.1887, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 000, Train Loss: 0.0867, Test Loss 0.2148, Train Acc: 0.5114, Test Acc: 0.6667\n",
      "Epoch: 001, Train Loss: 0.0863, Test Loss 0.2289, Train Acc: 0.5227, Test Acc: 0.6667\n",
      "Epoch: 002, Train Loss: 0.0861, Test Loss 0.2261, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 003, Train Loss: 0.0860, Test Loss 0.2272, Train Acc: 0.5795, Test Acc: 0.6667\n",
      "Epoch: 004, Train Loss: 0.0855, Test Loss 0.2263, Train Acc: 0.5682, Test Acc: 0.6667\n",
      "Epoch: 005, Train Loss: 0.0851, Test Loss 0.2175, Train Acc: 0.6250, Test Acc: 1.0000\n",
      "Epoch: 006, Train Loss: 0.0851, Test Loss 0.2322, Train Acc: 0.5795, Test Acc: 0.3333\n",
      "Epoch: 007, Train Loss: 0.0844, Test Loss 0.2196, Train Acc: 0.6477, Test Acc: 0.6667\n",
      "Epoch: 008, Train Loss: 0.0841, Test Loss 0.2233, Train Acc: 0.7045, Test Acc: 0.6667\n",
      "Epoch: 009, Train Loss: 0.0836, Test Loss 0.2199, Train Acc: 0.6932, Test Acc: 0.6667\n",
      "Epoch: 010, Train Loss: 0.0837, Test Loss 0.2047, Train Acc: 0.5568, Test Acc: 0.6667\n",
      "Epoch: 011, Train Loss: 0.0824, Test Loss 0.2200, Train Acc: 0.6932, Test Acc: 0.6667\n",
      "Epoch: 012, Train Loss: 0.0821, Test Loss 0.2156, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 013, Train Loss: 0.0811, Test Loss 0.2128, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 014, Train Loss: 0.0793, Test Loss 0.2122, Train Acc: 0.6477, Test Acc: 0.6667\n",
      "Epoch: 015, Train Loss: 0.0786, Test Loss 0.2125, Train Acc: 0.6591, Test Acc: 0.6667\n",
      "Epoch: 016, Train Loss: 0.0779, Test Loss 0.2056, Train Acc: 0.7045, Test Acc: 0.6667\n",
      "Epoch: 017, Train Loss: 0.0773, Test Loss 0.2081, Train Acc: 0.6932, Test Acc: 0.6667\n",
      "Epoch: 018, Train Loss: 0.0769, Test Loss 0.1949, Train Acc: 0.7614, Test Acc: 1.0000\n",
      "Epoch: 019, Train Loss: 0.0758, Test Loss 0.2038, Train Acc: 0.7159, Test Acc: 0.6667\n",
      "Epoch: 020, Train Loss: 0.0752, Test Loss 0.1948, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 021, Train Loss: 0.0749, Test Loss 0.2101, Train Acc: 0.6477, Test Acc: 0.6667\n",
      "Epoch: 022, Train Loss: 0.0742, Test Loss 0.1874, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 023, Train Loss: 0.0733, Test Loss 0.1908, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 024, Train Loss: 0.0732, Test Loss 0.1912, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 025, Train Loss: 0.0731, Test Loss 0.1935, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 026, Train Loss: 0.0730, Test Loss 0.1933, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 027, Train Loss: 0.0730, Test Loss 0.1936, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 028, Train Loss: 0.0729, Test Loss 0.1935, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 029, Train Loss: 0.0729, Test Loss 0.1924, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 030, Train Loss: 0.0728, Test Loss 0.1920, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 031, Train Loss: 0.0727, Test Loss 0.1929, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 032, Train Loss: 0.0727, Test Loss 0.1932, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 033, Train Loss: 0.0726, Test Loss 0.1930, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 034, Train Loss: 0.0726, Test Loss 0.1920, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 035, Train Loss: 0.0725, Test Loss 0.1927, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 036, Train Loss: 0.0725, Test Loss 0.1927, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 037, Train Loss: 0.0725, Test Loss 0.1927, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 038, Train Loss: 0.0725, Test Loss 0.1928, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 039, Train Loss: 0.0725, Test Loss 0.1927, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 040, Train Loss: 0.0725, Test Loss 0.1927, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 041, Train Loss: 0.0725, Test Loss 0.1927, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 042, Train Loss: 0.0725, Test Loss 0.1927, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 043, Train Loss: 0.0725, Test Loss 0.1926, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 044, Train Loss: 0.0725, Test Loss 0.1926, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 045, Train Loss: 0.0725, Test Loss 0.1926, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 046, Train Loss: 0.0725, Test Loss 0.1926, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 047, Train Loss: 0.0725, Test Loss 0.1925, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 048, Train Loss: 0.0725, Test Loss 0.1925, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 049, Train Loss: 0.0725, Test Loss 0.1925, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 000, Train Loss: 0.0870, Test Loss 0.2216, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 001, Train Loss: 0.0865, Test Loss 0.2261, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 002, Train Loss: 0.0863, Test Loss 0.2395, Train Acc: 0.5455, Test Acc: 0.0000\n",
      "Epoch: 003, Train Loss: 0.0859, Test Loss 0.2246, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 004, Train Loss: 0.0857, Test Loss 0.2182, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 005, Train Loss: 0.0854, Test Loss 0.2493, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 006, Train Loss: 0.0859, Test Loss 0.2113, Train Acc: 0.5227, Test Acc: 0.6667\n",
      "Epoch: 007, Train Loss: 0.0844, Test Loss 0.2199, Train Acc: 0.5682, Test Acc: 0.6667\n",
      "Epoch: 008, Train Loss: 0.0834, Test Loss 0.2306, Train Acc: 0.6364, Test Acc: 0.3333\n",
      "Epoch: 009, Train Loss: 0.0823, Test Loss 0.2290, Train Acc: 0.6591, Test Acc: 0.6667\n",
      "Epoch: 010, Train Loss: 0.0814, Test Loss 0.2003, Train Acc: 0.6136, Test Acc: 0.6667\n",
      "Epoch: 011, Train Loss: 0.0804, Test Loss 0.2368, Train Acc: 0.5795, Test Acc: 0.3333\n",
      "Epoch: 012, Train Loss: 0.0793, Test Loss 0.2267, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 013, Train Loss: 0.0786, Test Loss 0.2153, Train Acc: 0.7045, Test Acc: 0.6667\n",
      "Epoch: 014, Train Loss: 0.0782, Test Loss 0.2060, Train Acc: 0.7614, Test Acc: 1.0000\n",
      "Epoch: 015, Train Loss: 0.0782, Test Loss 0.1991, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 016, Train Loss: 0.0776, Test Loss 0.2053, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 017, Train Loss: 0.0774, Test Loss 0.2108, Train Acc: 0.7045, Test Acc: 0.6667\n",
      "Epoch: 018, Train Loss: 0.0771, Test Loss 0.2098, Train Acc: 0.7045, Test Acc: 0.6667\n",
      "Epoch: 019, Train Loss: 0.0768, Test Loss 0.1988, Train Acc: 0.7614, Test Acc: 1.0000\n",
      "Epoch: 020, Train Loss: 0.0764, Test Loss 0.2039, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 021, Train Loss: 0.0761, Test Loss 0.2011, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 022, Train Loss: 0.0759, Test Loss 0.2064, Train Acc: 0.6818, Test Acc: 0.6667\n",
      "Epoch: 023, Train Loss: 0.0756, Test Loss 0.1975, Train Acc: 0.7727, Test Acc: 1.0000\n",
      "Epoch: 024, Train Loss: 0.0755, Test Loss 0.1982, Train Acc: 0.7614, Test Acc: 1.0000\n",
      "Epoch: 025, Train Loss: 0.0755, Test Loss 0.1984, Train Acc: 0.7614, Test Acc: 1.0000\n",
      "Epoch: 026, Train Loss: 0.0755, Test Loss 0.1984, Train Acc: 0.7614, Test Acc: 1.0000\n",
      "Epoch: 027, Train Loss: 0.0754, Test Loss 0.1990, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 028, Train Loss: 0.0754, Test Loss 0.1991, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 029, Train Loss: 0.0754, Test Loss 0.1994, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 030, Train Loss: 0.0753, Test Loss 0.2002, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 031, Train Loss: 0.0753, Test Loss 0.2002, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 032, Train Loss: 0.0753, Test Loss 0.2002, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 033, Train Loss: 0.0752, Test Loss 0.2000, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 034, Train Loss: 0.0752, Test Loss 0.1996, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 035, Train Loss: 0.0752, Test Loss 0.1993, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 036, Train Loss: 0.0752, Test Loss 0.1993, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 037, Train Loss: 0.0752, Test Loss 0.1994, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 038, Train Loss: 0.0752, Test Loss 0.1994, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 039, Train Loss: 0.0752, Test Loss 0.1994, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 040, Train Loss: 0.0752, Test Loss 0.1994, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 041, Train Loss: 0.0752, Test Loss 0.1994, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 042, Train Loss: 0.0752, Test Loss 0.1994, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 043, Train Loss: 0.0752, Test Loss 0.1994, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 044, Train Loss: 0.0752, Test Loss 0.1995, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 045, Train Loss: 0.0752, Test Loss 0.1994, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 046, Train Loss: 0.0752, Test Loss 0.1994, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 047, Train Loss: 0.0752, Test Loss 0.1994, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 048, Train Loss: 0.0752, Test Loss 0.1994, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 049, Train Loss: 0.0752, Test Loss 0.1994, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 000, Train Loss: 0.0910, Test Loss 0.2869, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 001, Train Loss: 0.0864, Test Loss 0.2362, Train Acc: 0.5909, Test Acc: 0.0000\n",
      "Epoch: 002, Train Loss: 0.0862, Test Loss 0.2256, Train Acc: 0.5114, Test Acc: 0.6667\n",
      "Epoch: 003, Train Loss: 0.0856, Test Loss 0.2410, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 004, Train Loss: 0.0851, Test Loss 0.2317, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 005, Train Loss: 0.0840, Test Loss 0.2217, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 006, Train Loss: 0.0823, Test Loss 0.2269, Train Acc: 0.5114, Test Acc: 0.3333\n",
      "Epoch: 007, Train Loss: 0.0818, Test Loss 0.2088, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "Epoch: 008, Train Loss: 0.0834, Test Loss 0.2110, Train Acc: 0.5682, Test Acc: 0.6667\n",
      "Epoch: 009, Train Loss: 0.0823, Test Loss 0.2382, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 010, Train Loss: 0.0828, Test Loss 0.1971, Train Acc: 0.5682, Test Acc: 0.6667\n",
      "Epoch: 011, Train Loss: 0.0820, Test Loss 0.2443, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 012, Train Loss: 0.0799, Test Loss 0.2295, Train Acc: 0.5341, Test Acc: 0.3333\n",
      "Epoch: 013, Train Loss: 0.0784, Test Loss 0.2128, Train Acc: 0.6932, Test Acc: 0.6667\n",
      "Epoch: 014, Train Loss: 0.0779, Test Loss 0.2018, Train Acc: 0.7727, Test Acc: 1.0000\n",
      "Epoch: 015, Train Loss: 0.0775, Test Loss 0.2041, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 016, Train Loss: 0.0773, Test Loss 0.2031, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 017, Train Loss: 0.0769, Test Loss 0.1984, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 018, Train Loss: 0.0766, Test Loss 0.1946, Train Acc: 0.7614, Test Acc: 1.0000\n",
      "Epoch: 019, Train Loss: 0.0763, Test Loss 0.1918, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 020, Train Loss: 0.0758, Test Loss 0.1945, Train Acc: 0.7614, Test Acc: 1.0000\n",
      "Epoch: 021, Train Loss: 0.0750, Test Loss 0.1909, Train Acc: 0.7614, Test Acc: 1.0000\n",
      "Epoch: 022, Train Loss: 0.0742, Test Loss 0.1907, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 023, Train Loss: 0.0735, Test Loss 0.1821, Train Acc: 0.7614, Test Acc: 1.0000\n",
      "Epoch: 024, Train Loss: 0.0735, Test Loss 0.1813, Train Acc: 0.7614, Test Acc: 1.0000\n",
      "Epoch: 025, Train Loss: 0.0734, Test Loss 0.1816, Train Acc: 0.7614, Test Acc: 1.0000\n",
      "Epoch: 026, Train Loss: 0.0733, Test Loss 0.1822, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 027, Train Loss: 0.0732, Test Loss 0.1822, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 028, Train Loss: 0.0732, Test Loss 0.1828, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 029, Train Loss: 0.0731, Test Loss 0.1832, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 030, Train Loss: 0.0731, Test Loss 0.1834, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 031, Train Loss: 0.0730, Test Loss 0.1835, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 032, Train Loss: 0.0730, Test Loss 0.1839, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 033, Train Loss: 0.0729, Test Loss 0.1837, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 034, Train Loss: 0.0729, Test Loss 0.1831, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 035, Train Loss: 0.0728, Test Loss 0.1827, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 036, Train Loss: 0.0728, Test Loss 0.1827, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 037, Train Loss: 0.0728, Test Loss 0.1827, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 038, Train Loss: 0.0728, Test Loss 0.1827, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 039, Train Loss: 0.0728, Test Loss 0.1827, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 040, Train Loss: 0.0728, Test Loss 0.1827, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 041, Train Loss: 0.0728, Test Loss 0.1827, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 042, Train Loss: 0.0728, Test Loss 0.1827, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 043, Train Loss: 0.0728, Test Loss 0.1826, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 044, Train Loss: 0.0728, Test Loss 0.1827, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 045, Train Loss: 0.0728, Test Loss 0.1827, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 046, Train Loss: 0.0728, Test Loss 0.1826, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 047, Train Loss: 0.0728, Test Loss 0.1826, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 048, Train Loss: 0.0728, Test Loss 0.1826, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 049, Train Loss: 0.0728, Test Loss 0.1826, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 000, Train Loss: 0.0865, Test Loss 0.2278, Train Acc: 0.5227, Test Acc: 0.6667\n",
      "Epoch: 001, Train Loss: 0.0865, Test Loss 0.2294, Train Acc: 0.5568, Test Acc: 1.0000\n",
      "Epoch: 002, Train Loss: 0.0864, Test Loss 0.2289, Train Acc: 0.5682, Test Acc: 1.0000\n",
      "Epoch: 003, Train Loss: 0.0862, Test Loss 0.2281, Train Acc: 0.5909, Test Acc: 1.0000\n",
      "Epoch: 004, Train Loss: 0.0860, Test Loss 0.2250, Train Acc: 0.5341, Test Acc: 0.6667\n",
      "Epoch: 005, Train Loss: 0.0857, Test Loss 0.2173, Train Acc: 0.5227, Test Acc: 0.6667\n",
      "Epoch: 006, Train Loss: 0.0862, Test Loss 0.2303, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 007, Train Loss: 0.0860, Test Loss 0.2197, Train Acc: 0.5227, Test Acc: 0.6667\n",
      "Epoch: 008, Train Loss: 0.0855, Test Loss 0.2192, Train Acc: 0.5227, Test Acc: 0.6667\n",
      "Epoch: 009, Train Loss: 0.0861, Test Loss 0.2300, Train Acc: 0.6477, Test Acc: 0.6667\n",
      "Epoch: 010, Train Loss: 0.0859, Test Loss 0.2240, Train Acc: 0.5227, Test Acc: 0.6667\n",
      "Epoch: 011, Train Loss: 0.0851, Test Loss 0.2225, Train Acc: 0.6023, Test Acc: 1.0000\n",
      "Epoch: 012, Train Loss: 0.0850, Test Loss 0.2242, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 013, Train Loss: 0.0848, Test Loss 0.2242, Train Acc: 0.7045, Test Acc: 0.6667\n",
      "Epoch: 014, Train Loss: 0.0847, Test Loss 0.2257, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 015, Train Loss: 0.0846, Test Loss 0.2261, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 016, Train Loss: 0.0845, Test Loss 0.2247, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 017, Train Loss: 0.0843, Test Loss 0.2260, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 018, Train Loss: 0.0842, Test Loss 0.2247, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 019, Train Loss: 0.0841, Test Loss 0.2262, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 020, Train Loss: 0.0839, Test Loss 0.2247, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 021, Train Loss: 0.0838, Test Loss 0.2252, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 022, Train Loss: 0.0837, Test Loss 0.2226, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 023, Train Loss: 0.0836, Test Loss 0.2235, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 024, Train Loss: 0.0836, Test Loss 0.2232, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 025, Train Loss: 0.0836, Test Loss 0.2231, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 026, Train Loss: 0.0836, Test Loss 0.2232, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 027, Train Loss: 0.0836, Test Loss 0.2229, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 028, Train Loss: 0.0836, Test Loss 0.2228, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 029, Train Loss: 0.0836, Test Loss 0.2229, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 030, Train Loss: 0.0835, Test Loss 0.2230, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 031, Train Loss: 0.0835, Test Loss 0.2228, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 032, Train Loss: 0.0835, Test Loss 0.2228, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 033, Train Loss: 0.0835, Test Loss 0.2230, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 034, Train Loss: 0.0835, Test Loss 0.2227, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 035, Train Loss: 0.0835, Test Loss 0.2227, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 036, Train Loss: 0.0835, Test Loss 0.2227, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 037, Train Loss: 0.0835, Test Loss 0.2227, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 038, Train Loss: 0.0835, Test Loss 0.2227, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 039, Train Loss: 0.0835, Test Loss 0.2227, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 040, Train Loss: 0.0835, Test Loss 0.2227, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 041, Train Loss: 0.0835, Test Loss 0.2227, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 042, Train Loss: 0.0835, Test Loss 0.2227, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 043, Train Loss: 0.0835, Test Loss 0.2227, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 044, Train Loss: 0.0835, Test Loss 0.2227, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 045, Train Loss: 0.0835, Test Loss 0.2227, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 046, Train Loss: 0.0835, Test Loss 0.2227, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 047, Train Loss: 0.0835, Test Loss 0.2227, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 048, Train Loss: 0.0835, Test Loss 0.2227, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 049, Train Loss: 0.0835, Test Loss 0.2227, Train Acc: 0.7273, Test Acc: 0.6667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "eval_metrics = np.zeros((skf.n_splits, 3))\n",
    "\n",
    "labels = [full_dataset[i].y for i in range(len(full_dataset))]\n",
    "\n",
    "\n",
    "for n_fold, (train_idx, test_idx) in tqdm(enumerate(skf.split(labels, labels))):\n",
    "    model = GCN(full_dataset.num_features, 2, channels=[256, 32, 8], dropout=0.3).to(device())\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=50//4, gamma=0.1, last_epoch=-1, verbose=False)\n",
    "\n",
    "    train_loader_ = DataLoader(full_dataset[list(train_idx)], batch_size=8, shuffle=True)\n",
    "    test_loader_ = DataLoader(full_dataset[list(test_idx)], batch_size=8, shuffle=True)\n",
    "    min_v_loss = np.inf\n",
    "    print(n_fold)\n",
    "    pr, rc, acc = [], [], []\n",
    "    for epoch in range(50):\n",
    "        train_epoch(train_loader, model, criterion, optimizer)\n",
    "        train_loss, train_acc, _, _ = eval_epoch(train_loader, model, criterion)\n",
    "        val_loss, test_acc, _, _ = eval_epoch(val_loader, model, criterion)\n",
    "        scheduler.step()\n",
    "        print(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.4f}, Test Loss {val_loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "        #print(f'Train Prec: {train_pr:.3f}, Train Rec: {train_rc:.3f}, Test Prec: {val_pr:.3f}, Test Rec: {val_rc:.3f}')\n",
    "        #rc.append(val_rc)\n",
    "        #pr.append(val_pr)\n",
    "        acc.append(test_acc)\n",
    "        if min_v_loss > val_loss:\n",
    "            min_v_loss = val_loss\n",
    "            best_test_acc = test_acc\n",
    "\n",
    "    eval_metrics[n_fold, 0] = best_test_acc\n",
    "    eval_metrics[n_fold, 1] = np.mean(acc)\n",
    "    eval_metrics[n_fold, 2] = np.std(acc)\n",
    "### eval_metrics[n_fold, 3] ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.66666667, 0.67333333, 0.08137704],\n",
       "       [1.        , 0.72      , 0.13920409],\n",
       "       [1.        , 0.59333333, 0.18      ],\n",
       "       [1.        , 0.86666667, 0.17638342],\n",
       "       [1.        , 0.66666667, 0.11547005],\n",
       "       [1.        , 0.68666667, 0.1034945 ],\n",
       "       [1.        , 0.68      , 0.09333333],\n",
       "       [1.        , 0.85333333, 0.23247461],\n",
       "       [1.        , 0.86666667, 0.25819889],\n",
       "       [0.66666667, 0.7       , 0.1       ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9333333333333332, 0.13333333333333336)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(eval_metrics[:, 0]), np.std(eval_metrics[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7306666666666666, 0.0915641851380768)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(eval_metrics[:, 1]), np.std(eval_metrics[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b0c61b80a07460b875fa182f812bd74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Epoch: 000, Train Loss: 0.0872, Test Loss 0.2138, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 001, Train Loss: 0.0851, Test Loss 0.2342, Train Acc: 0.5909, Test Acc: 0.3333\n",
      "Epoch: 002, Train Loss: 0.0846, Test Loss 0.2182, Train Acc: 0.5227, Test Acc: 0.6667\n",
      "Epoch: 003, Train Loss: 0.0813, Test Loss 0.2223, Train Acc: 0.6591, Test Acc: 0.6667\n",
      "Epoch: 004, Train Loss: 0.0795, Test Loss 0.2135, Train Acc: 0.6818, Test Acc: 0.6667\n",
      "Epoch: 005, Train Loss: 0.0826, Test Loss 0.2691, Train Acc: 0.5455, Test Acc: 0.3333\n",
      "Epoch: 006, Train Loss: 0.0795, Test Loss 0.1939, Train Acc: 0.6364, Test Acc: 1.0000\n",
      "Epoch: 007, Train Loss: 0.0734, Test Loss 0.2276, Train Acc: 0.7045, Test Acc: 0.6667\n",
      "Epoch: 008, Train Loss: 0.0764, Test Loss 0.2646, Train Acc: 0.6477, Test Acc: 0.3333\n",
      "Epoch: 009, Train Loss: 0.0801, Test Loss 0.2904, Train Acc: 0.5909, Test Acc: 0.3333\n",
      "Epoch: 010, Train Loss: 0.0687, Test Loss 0.1715, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 011, Train Loss: 0.0715, Test Loss 0.1550, Train Acc: 0.6591, Test Acc: 0.6667\n",
      "Epoch: 012, Train Loss: 0.0637, Test Loss 0.1778, Train Acc: 0.8523, Test Acc: 0.6667\n",
      "Epoch: 013, Train Loss: 0.0630, Test Loss 0.1929, Train Acc: 0.7955, Test Acc: 0.6667\n",
      "Epoch: 014, Train Loss: 0.0625, Test Loss 0.1859, Train Acc: 0.8409, Test Acc: 0.6667\n",
      "Epoch: 015, Train Loss: 0.0632, Test Loss 0.1701, Train Acc: 0.8409, Test Acc: 0.6667\n",
      "Epoch: 016, Train Loss: 0.0617, Test Loss 0.1828, Train Acc: 0.8523, Test Acc: 0.6667\n",
      "Epoch: 017, Train Loss: 0.0615, Test Loss 0.1756, Train Acc: 0.8523, Test Acc: 0.6667\n",
      "Epoch: 018, Train Loss: 0.0609, Test Loss 0.1907, Train Acc: 0.7955, Test Acc: 0.6667\n",
      "Epoch: 019, Train Loss: 0.0609, Test Loss 0.1707, Train Acc: 0.8409, Test Acc: 0.6667\n",
      "Epoch: 020, Train Loss: 0.0608, Test Loss 0.1671, Train Acc: 0.8409, Test Acc: 0.6667\n",
      "Epoch: 021, Train Loss: 0.0597, Test Loss 0.1706, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 022, Train Loss: 0.0591, Test Loss 0.1702, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 023, Train Loss: 0.0587, Test Loss 0.1673, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 024, Train Loss: 0.0587, Test Loss 0.1670, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 025, Train Loss: 0.0586, Test Loss 0.1674, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 026, Train Loss: 0.0586, Test Loss 0.1668, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 027, Train Loss: 0.0587, Test Loss 0.1652, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 028, Train Loss: 0.0587, Test Loss 0.1649, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 029, Train Loss: 0.0586, Test Loss 0.1654, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 030, Train Loss: 0.0589, Test Loss 0.1634, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 031, Train Loss: 0.0588, Test Loss 0.1634, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 032, Train Loss: 0.0587, Test Loss 0.1638, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 033, Train Loss: 0.0584, Test Loss 0.1653, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 034, Train Loss: 0.0586, Test Loss 0.1638, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 035, Train Loss: 0.0587, Test Loss 0.1624, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 036, Train Loss: 0.0587, Test Loss 0.1625, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 037, Train Loss: 0.0587, Test Loss 0.1624, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 038, Train Loss: 0.0587, Test Loss 0.1625, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 039, Train Loss: 0.0587, Test Loss 0.1626, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 040, Train Loss: 0.0587, Test Loss 0.1624, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 041, Train Loss: 0.0587, Test Loss 0.1625, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 042, Train Loss: 0.0587, Test Loss 0.1626, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 043, Train Loss: 0.0587, Test Loss 0.1625, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 044, Train Loss: 0.0587, Test Loss 0.1624, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 045, Train Loss: 0.0587, Test Loss 0.1625, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 046, Train Loss: 0.0587, Test Loss 0.1626, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 047, Train Loss: 0.0586, Test Loss 0.1626, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 048, Train Loss: 0.0586, Test Loss 0.1626, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 049, Train Loss: 0.0586, Test Loss 0.1626, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "1\n",
      "Epoch: 000, Train Loss: 0.0868, Test Loss 0.2403, Train Acc: 0.5341, Test Acc: 0.0000\n",
      "Epoch: 001, Train Loss: 0.0876, Test Loss 0.2195, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 002, Train Loss: 0.0865, Test Loss 0.2339, Train Acc: 0.5568, Test Acc: 0.6667\n",
      "Epoch: 003, Train Loss: 0.0863, Test Loss 0.2308, Train Acc: 0.5227, Test Acc: 0.6667\n",
      "Epoch: 004, Train Loss: 0.0863, Test Loss 0.2238, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 005, Train Loss: 0.0858, Test Loss 0.2343, Train Acc: 0.6136, Test Acc: 0.0000\n",
      "Epoch: 006, Train Loss: 0.0855, Test Loss 0.2252, Train Acc: 0.5341, Test Acc: 0.6667\n",
      "Epoch: 007, Train Loss: 0.0856, Test Loss 0.2205, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 008, Train Loss: 0.0851, Test Loss 0.2417, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 009, Train Loss: 0.0859, Test Loss 0.2139, Train Acc: 0.5114, Test Acc: 0.6667\n",
      "Epoch: 010, Train Loss: 0.0838, Test Loss 0.2309, Train Acc: 0.5682, Test Acc: 0.3333\n",
      "Epoch: 011, Train Loss: 0.0844, Test Loss 0.2097, Train Acc: 0.5568, Test Acc: 0.6667\n",
      "Epoch: 012, Train Loss: 0.0835, Test Loss 0.2101, Train Acc: 0.5682, Test Acc: 0.6667\n",
      "Epoch: 013, Train Loss: 0.0826, Test Loss 0.2116, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "Epoch: 014, Train Loss: 0.0821, Test Loss 0.2136, Train Acc: 0.7614, Test Acc: 1.0000\n",
      "Epoch: 015, Train Loss: 0.0819, Test Loss 0.2099, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 016, Train Loss: 0.0814, Test Loss 0.2078, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 017, Train Loss: 0.0810, Test Loss 0.2065, Train Acc: 0.7159, Test Acc: 0.6667\n",
      "Epoch: 018, Train Loss: 0.0807, Test Loss 0.2033, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 019, Train Loss: 0.0808, Test Loss 0.1998, Train Acc: 0.6932, Test Acc: 1.0000\n",
      "Epoch: 020, Train Loss: 0.0807, Test Loss 0.1979, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "Epoch: 021, Train Loss: 0.0798, Test Loss 0.1967, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 022, Train Loss: 0.0791, Test Loss 0.1962, Train Acc: 0.7159, Test Acc: 0.6667\n",
      "Epoch: 023, Train Loss: 0.0789, Test Loss 0.1928, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 024, Train Loss: 0.0788, Test Loss 0.1926, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 025, Train Loss: 0.0788, Test Loss 0.1925, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 026, Train Loss: 0.0786, Test Loss 0.1926, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 027, Train Loss: 0.0786, Test Loss 0.1924, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 028, Train Loss: 0.0786, Test Loss 0.1921, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 029, Train Loss: 0.0785, Test Loss 0.1917, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 030, Train Loss: 0.0785, Test Loss 0.1913, Train Acc: 0.6818, Test Acc: 1.0000\n",
      "Epoch: 031, Train Loss: 0.0785, Test Loss 0.1911, Train Acc: 0.6818, Test Acc: 1.0000\n",
      "Epoch: 032, Train Loss: 0.0784, Test Loss 0.1912, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 033, Train Loss: 0.0784, Test Loss 0.1909, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 034, Train Loss: 0.0783, Test Loss 0.1909, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 035, Train Loss: 0.0782, Test Loss 0.1906, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 036, Train Loss: 0.0782, Test Loss 0.1906, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 037, Train Loss: 0.0782, Test Loss 0.1906, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 038, Train Loss: 0.0782, Test Loss 0.1906, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 039, Train Loss: 0.0782, Test Loss 0.1906, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 040, Train Loss: 0.0782, Test Loss 0.1905, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 041, Train Loss: 0.0782, Test Loss 0.1906, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 042, Train Loss: 0.0782, Test Loss 0.1905, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 043, Train Loss: 0.0782, Test Loss 0.1905, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 044, Train Loss: 0.0782, Test Loss 0.1905, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 045, Train Loss: 0.0782, Test Loss 0.1905, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 046, Train Loss: 0.0782, Test Loss 0.1905, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 047, Train Loss: 0.0782, Test Loss 0.1905, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 048, Train Loss: 0.0782, Test Loss 0.1905, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 049, Train Loss: 0.0782, Test Loss 0.1905, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "2\n",
      "Epoch: 000, Train Loss: 0.0895, Test Loss 0.2098, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 001, Train Loss: 0.0868, Test Loss 0.2271, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 002, Train Loss: 0.0867, Test Loss 0.2283, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 003, Train Loss: 0.0866, Test Loss 0.2278, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 004, Train Loss: 0.0864, Test Loss 0.2344, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 005, Train Loss: 0.0866, Test Loss 0.2429, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 006, Train Loss: 0.0861, Test Loss 0.2278, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "Epoch: 007, Train Loss: 0.0858, Test Loss 0.2281, Train Acc: 0.6818, Test Acc: 0.6667\n",
      "Epoch: 008, Train Loss: 0.0857, Test Loss 0.2410, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 009, Train Loss: 0.0848, Test Loss 0.2272, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 010, Train Loss: 0.0842, Test Loss 0.2338, Train Acc: 0.5227, Test Acc: 0.3333\n",
      "Epoch: 011, Train Loss: 0.0836, Test Loss 0.2384, Train Acc: 0.5227, Test Acc: 0.3333\n",
      "Epoch: 012, Train Loss: 0.0833, Test Loss 0.2360, Train Acc: 0.5455, Test Acc: 0.3333\n",
      "Epoch: 013, Train Loss: 0.0830, Test Loss 0.2357, Train Acc: 0.5568, Test Acc: 0.3333\n",
      "Epoch: 014, Train Loss: 0.0821, Test Loss 0.2210, Train Acc: 0.7614, Test Acc: 1.0000\n",
      "Epoch: 015, Train Loss: 0.0820, Test Loss 0.2262, Train Acc: 0.6932, Test Acc: 0.6667\n",
      "Epoch: 016, Train Loss: 0.0824, Test Loss 0.2362, Train Acc: 0.5795, Test Acc: 0.3333\n",
      "Epoch: 017, Train Loss: 0.0817, Test Loss 0.2279, Train Acc: 0.6818, Test Acc: 0.3333\n",
      "Epoch: 018, Train Loss: 0.0816, Test Loss 0.2284, Train Acc: 0.6705, Test Acc: 0.3333\n",
      "Epoch: 019, Train Loss: 0.0811, Test Loss 0.2203, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 020, Train Loss: 0.0809, Test Loss 0.2211, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 021, Train Loss: 0.0809, Test Loss 0.2248, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 022, Train Loss: 0.0810, Test Loss 0.2293, Train Acc: 0.6705, Test Acc: 0.3333\n",
      "Epoch: 023, Train Loss: 0.0804, Test Loss 0.2214, Train Acc: 0.7159, Test Acc: 0.6667\n",
      "Epoch: 024, Train Loss: 0.0804, Test Loss 0.2214, Train Acc: 0.7159, Test Acc: 0.6667\n",
      "Epoch: 025, Train Loss: 0.0804, Test Loss 0.2212, Train Acc: 0.7159, Test Acc: 0.6667\n",
      "Epoch: 026, Train Loss: 0.0803, Test Loss 0.2207, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 027, Train Loss: 0.0803, Test Loss 0.2214, Train Acc: 0.7159, Test Acc: 0.6667\n",
      "Epoch: 028, Train Loss: 0.0803, Test Loss 0.2224, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 029, Train Loss: 0.0803, Test Loss 0.2219, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 030, Train Loss: 0.0803, Test Loss 0.2225, Train Acc: 0.7159, Test Acc: 0.6667\n",
      "Epoch: 031, Train Loss: 0.0803, Test Loss 0.2223, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 032, Train Loss: 0.0803, Test Loss 0.2219, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 033, Train Loss: 0.0802, Test Loss 0.2215, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 034, Train Loss: 0.0802, Test Loss 0.2218, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 035, Train Loss: 0.0802, Test Loss 0.2220, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 036, Train Loss: 0.0802, Test Loss 0.2221, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 037, Train Loss: 0.0802, Test Loss 0.2221, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 038, Train Loss: 0.0802, Test Loss 0.2221, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 039, Train Loss: 0.0802, Test Loss 0.2221, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 040, Train Loss: 0.0802, Test Loss 0.2221, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 041, Train Loss: 0.0802, Test Loss 0.2221, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 042, Train Loss: 0.0802, Test Loss 0.2220, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 043, Train Loss: 0.0802, Test Loss 0.2220, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 044, Train Loss: 0.0802, Test Loss 0.2220, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 045, Train Loss: 0.0802, Test Loss 0.2220, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 046, Train Loss: 0.0802, Test Loss 0.2220, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 047, Train Loss: 0.0802, Test Loss 0.2220, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 048, Train Loss: 0.0802, Test Loss 0.2220, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 049, Train Loss: 0.0802, Test Loss 0.2220, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "3\n",
      "Epoch: 000, Train Loss: 0.0949, Test Loss 0.2967, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 001, Train Loss: 0.0873, Test Loss 0.2368, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 002, Train Loss: 0.0888, Test Loss 0.1947, Train Acc: 0.5114, Test Acc: 0.6667\n",
      "Epoch: 003, Train Loss: 0.0861, Test Loss 0.2290, Train Acc: 0.5227, Test Acc: 0.3333\n",
      "Epoch: 004, Train Loss: 0.0857, Test Loss 0.2129, Train Acc: 0.5795, Test Acc: 1.0000\n",
      "Epoch: 005, Train Loss: 0.0854, Test Loss 0.2105, Train Acc: 0.5795, Test Acc: 1.0000\n",
      "Epoch: 006, Train Loss: 0.0848, Test Loss 0.2142, Train Acc: 0.6250, Test Acc: 1.0000\n",
      "Epoch: 007, Train Loss: 0.0847, Test Loss 0.2037, Train Acc: 0.5568, Test Acc: 1.0000\n",
      "Epoch: 008, Train Loss: 0.0831, Test Loss 0.2161, Train Acc: 0.5909, Test Acc: 0.6667\n",
      "Epoch: 009, Train Loss: 0.0841, Test Loss 0.1977, Train Acc: 0.5455, Test Acc: 1.0000\n",
      "Epoch: 010, Train Loss: 0.0834, Test Loss 0.2259, Train Acc: 0.6136, Test Acc: 0.3333\n",
      "Epoch: 011, Train Loss: 0.0834, Test Loss 0.1921, Train Acc: 0.5341, Test Acc: 1.0000\n",
      "Epoch: 012, Train Loss: 0.0826, Test Loss 0.1971, Train Acc: 0.5795, Test Acc: 1.0000\n",
      "Epoch: 013, Train Loss: 0.0824, Test Loss 0.1978, Train Acc: 0.5909, Test Acc: 1.0000\n",
      "Epoch: 014, Train Loss: 0.0820, Test Loss 0.2008, Train Acc: 0.6250, Test Acc: 1.0000\n",
      "Epoch: 015, Train Loss: 0.0817, Test Loss 0.2024, Train Acc: 0.6818, Test Acc: 1.0000\n",
      "Epoch: 016, Train Loss: 0.0816, Test Loss 0.2001, Train Acc: 0.6477, Test Acc: 1.0000\n",
      "Epoch: 017, Train Loss: 0.0816, Test Loss 0.1972, Train Acc: 0.6136, Test Acc: 1.0000\n",
      "Epoch: 018, Train Loss: 0.0813, Test Loss 0.1981, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "Epoch: 019, Train Loss: 0.0810, Test Loss 0.1984, Train Acc: 0.6818, Test Acc: 1.0000\n",
      "Epoch: 020, Train Loss: 0.0811, Test Loss 0.1931, Train Acc: 0.6136, Test Acc: 1.0000\n",
      "Epoch: 021, Train Loss: 0.0806, Test Loss 0.1952, Train Acc: 0.6818, Test Acc: 1.0000\n",
      "Epoch: 022, Train Loss: 0.0803, Test Loss 0.1926, Train Acc: 0.6818, Test Acc: 1.0000\n",
      "Epoch: 023, Train Loss: 0.0804, Test Loss 0.1844, Train Acc: 0.6364, Test Acc: 1.0000\n",
      "Epoch: 024, Train Loss: 0.0802, Test Loss 0.1853, Train Acc: 0.6364, Test Acc: 1.0000\n",
      "Epoch: 025, Train Loss: 0.0801, Test Loss 0.1855, Train Acc: 0.6477, Test Acc: 1.0000\n",
      "Epoch: 026, Train Loss: 0.0799, Test Loss 0.1856, Train Acc: 0.6705, Test Acc: 1.0000\n",
      "Epoch: 027, Train Loss: 0.0798, Test Loss 0.1855, Train Acc: 0.6705, Test Acc: 1.0000\n",
      "Epoch: 028, Train Loss: 0.0797, Test Loss 0.1854, Train Acc: 0.6705, Test Acc: 1.0000\n",
      "Epoch: 029, Train Loss: 0.0796, Test Loss 0.1855, Train Acc: 0.6705, Test Acc: 1.0000\n",
      "Epoch: 030, Train Loss: 0.0795, Test Loss 0.1850, Train Acc: 0.6705, Test Acc: 1.0000\n",
      "Epoch: 031, Train Loss: 0.0793, Test Loss 0.1855, Train Acc: 0.6705, Test Acc: 1.0000\n",
      "Epoch: 032, Train Loss: 0.0792, Test Loss 0.1849, Train Acc: 0.6705, Test Acc: 1.0000\n",
      "Epoch: 033, Train Loss: 0.0791, Test Loss 0.1847, Train Acc: 0.6705, Test Acc: 1.0000\n",
      "Epoch: 034, Train Loss: 0.0790, Test Loss 0.1842, Train Acc: 0.6705, Test Acc: 1.0000\n",
      "Epoch: 035, Train Loss: 0.0789, Test Loss 0.1842, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "Epoch: 036, Train Loss: 0.0789, Test Loss 0.1841, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "Epoch: 037, Train Loss: 0.0789, Test Loss 0.1841, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "Epoch: 038, Train Loss: 0.0789, Test Loss 0.1840, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "Epoch: 039, Train Loss: 0.0789, Test Loss 0.1839, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "Epoch: 040, Train Loss: 0.0788, Test Loss 0.1839, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "Epoch: 041, Train Loss: 0.0788, Test Loss 0.1838, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "Epoch: 042, Train Loss: 0.0788, Test Loss 0.1838, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "Epoch: 043, Train Loss: 0.0788, Test Loss 0.1838, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "Epoch: 044, Train Loss: 0.0788, Test Loss 0.1837, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "Epoch: 045, Train Loss: 0.0788, Test Loss 0.1837, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "Epoch: 046, Train Loss: 0.0788, Test Loss 0.1836, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "Epoch: 047, Train Loss: 0.0788, Test Loss 0.1836, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "Epoch: 048, Train Loss: 0.0788, Test Loss 0.1836, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "Epoch: 049, Train Loss: 0.0788, Test Loss 0.1836, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "4\n",
      "Epoch: 000, Train Loss: 0.0866, Test Loss 0.2417, Train Acc: 0.5227, Test Acc: 0.3333\n",
      "Epoch: 001, Train Loss: 0.0850, Test Loss 0.2202, Train Acc: 0.5341, Test Acc: 0.6667\n",
      "Epoch: 002, Train Loss: 0.0835, Test Loss 0.2207, Train Acc: 0.6477, Test Acc: 0.6667\n",
      "Epoch: 003, Train Loss: 0.0826, Test Loss 0.2195, Train Acc: 0.6591, Test Acc: 0.6667\n",
      "Epoch: 004, Train Loss: 0.0855, Test Loss 0.2622, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 005, Train Loss: 0.0832, Test Loss 0.2266, Train Acc: 0.6023, Test Acc: 0.6667\n",
      "Epoch: 006, Train Loss: 0.0817, Test Loss 0.2126, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 007, Train Loss: 0.0871, Test Loss 0.1793, Train Acc: 0.5341, Test Acc: 0.6667\n",
      "Epoch: 008, Train Loss: 0.0908, Test Loss 0.3034, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 009, Train Loss: 0.0823, Test Loss 0.2126, Train Acc: 0.6136, Test Acc: 0.6667\n",
      "Epoch: 010, Train Loss: 0.0864, Test Loss 0.1866, Train Acc: 0.5341, Test Acc: 0.6667\n",
      "Epoch: 011, Train Loss: 0.0822, Test Loss 0.2368, Train Acc: 0.6250, Test Acc: 0.6667\n",
      "Epoch: 012, Train Loss: 0.0815, Test Loss 0.2314, Train Acc: 0.6591, Test Acc: 0.6667\n",
      "Epoch: 013, Train Loss: 0.0808, Test Loss 0.2246, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 014, Train Loss: 0.0802, Test Loss 0.2159, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 015, Train Loss: 0.0799, Test Loss 0.2151, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 016, Train Loss: 0.0795, Test Loss 0.2132, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 017, Train Loss: 0.0793, Test Loss 0.2135, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 018, Train Loss: 0.0790, Test Loss 0.2099, Train Acc: 0.7045, Test Acc: 0.6667\n",
      "Epoch: 019, Train Loss: 0.0787, Test Loss 0.2139, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 020, Train Loss: 0.0785, Test Loss 0.2180, Train Acc: 0.7159, Test Acc: 0.6667\n",
      "Epoch: 021, Train Loss: 0.0781, Test Loss 0.2117, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 022, Train Loss: 0.0778, Test Loss 0.2127, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 023, Train Loss: 0.0775, Test Loss 0.2121, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 024, Train Loss: 0.0775, Test Loss 0.2116, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 025, Train Loss: 0.0775, Test Loss 0.2121, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 026, Train Loss: 0.0775, Test Loss 0.2130, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 027, Train Loss: 0.0774, Test Loss 0.2127, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 028, Train Loss: 0.0774, Test Loss 0.2122, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 029, Train Loss: 0.0774, Test Loss 0.2119, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 030, Train Loss: 0.0773, Test Loss 0.2126, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 031, Train Loss: 0.0773, Test Loss 0.2129, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 032, Train Loss: 0.0773, Test Loss 0.2124, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 033, Train Loss: 0.0772, Test Loss 0.2123, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 034, Train Loss: 0.0772, Test Loss 0.2125, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 035, Train Loss: 0.0772, Test Loss 0.2126, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 036, Train Loss: 0.0772, Test Loss 0.2126, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 037, Train Loss: 0.0772, Test Loss 0.2126, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 038, Train Loss: 0.0772, Test Loss 0.2126, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 039, Train Loss: 0.0772, Test Loss 0.2127, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 040, Train Loss: 0.0772, Test Loss 0.2126, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 041, Train Loss: 0.0772, Test Loss 0.2126, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 042, Train Loss: 0.0772, Test Loss 0.2127, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 043, Train Loss: 0.0772, Test Loss 0.2127, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 044, Train Loss: 0.0771, Test Loss 0.2126, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 045, Train Loss: 0.0771, Test Loss 0.2127, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 046, Train Loss: 0.0771, Test Loss 0.2126, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 047, Train Loss: 0.0771, Test Loss 0.2127, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 048, Train Loss: 0.0771, Test Loss 0.2127, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 049, Train Loss: 0.0771, Test Loss 0.2127, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "5\n",
      "Epoch: 000, Train Loss: 0.0864, Test Loss 0.2392, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 001, Train Loss: 0.0864, Test Loss 0.2169, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 002, Train Loss: 0.0857, Test Loss 0.2215, Train Acc: 0.5568, Test Acc: 0.6667\n",
      "Epoch: 003, Train Loss: 0.0863, Test Loss 0.2115, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 004, Train Loss: 0.0847, Test Loss 0.2298, Train Acc: 0.5568, Test Acc: 0.3333\n",
      "Epoch: 005, Train Loss: 0.0840, Test Loss 0.2143, Train Acc: 0.6250, Test Acc: 1.0000\n",
      "Epoch: 006, Train Loss: 0.0832, Test Loss 0.2042, Train Acc: 0.6364, Test Acc: 1.0000\n",
      "Epoch: 007, Train Loss: 0.0852, Test Loss 0.1925, Train Acc: 0.5227, Test Acc: 0.6667\n",
      "Epoch: 008, Train Loss: 0.0849, Test Loss 0.1939, Train Acc: 0.5341, Test Acc: 0.6667\n",
      "Epoch: 009, Train Loss: 0.0914, Test Loss 0.3045, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 010, Train Loss: 0.0824, Test Loss 0.1972, Train Acc: 0.6023, Test Acc: 1.0000\n",
      "Epoch: 011, Train Loss: 0.0855, Test Loss 0.2658, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 012, Train Loss: 0.0806, Test Loss 0.2330, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 013, Train Loss: 0.0781, Test Loss 0.1984, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 014, Train Loss: 0.0777, Test Loss 0.1969, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 015, Train Loss: 0.0771, Test Loss 0.1929, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 016, Train Loss: 0.0765, Test Loss 0.1886, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 017, Train Loss: 0.0760, Test Loss 0.1825, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 018, Train Loss: 0.0755, Test Loss 0.1765, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 019, Train Loss: 0.0751, Test Loss 0.1722, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 020, Train Loss: 0.0740, Test Loss 0.1727, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 021, Train Loss: 0.0751, Test Loss 0.1647, Train Acc: 0.6705, Test Acc: 1.0000\n",
      "Epoch: 022, Train Loss: 0.0737, Test Loss 0.1652, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 023, Train Loss: 0.0733, Test Loss 0.1634, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 024, Train Loss: 0.0733, Test Loss 0.1632, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 025, Train Loss: 0.0733, Test Loss 0.1629, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 026, Train Loss: 0.0733, Test Loss 0.1627, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 027, Train Loss: 0.0732, Test Loss 0.1627, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 028, Train Loss: 0.0733, Test Loss 0.1623, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 029, Train Loss: 0.0732, Test Loss 0.1622, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 030, Train Loss: 0.0734, Test Loss 0.1618, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 031, Train Loss: 0.0734, Test Loss 0.1615, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 032, Train Loss: 0.0732, Test Loss 0.1616, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 033, Train Loss: 0.0731, Test Loss 0.1615, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 034, Train Loss: 0.0730, Test Loss 0.1615, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 035, Train Loss: 0.0731, Test Loss 0.1612, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 036, Train Loss: 0.0731, Test Loss 0.1611, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 037, Train Loss: 0.0731, Test Loss 0.1611, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 038, Train Loss: 0.0731, Test Loss 0.1611, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 039, Train Loss: 0.0731, Test Loss 0.1611, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 040, Train Loss: 0.0731, Test Loss 0.1612, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 041, Train Loss: 0.0731, Test Loss 0.1611, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 042, Train Loss: 0.0731, Test Loss 0.1611, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 043, Train Loss: 0.0731, Test Loss 0.1611, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 044, Train Loss: 0.0731, Test Loss 0.1611, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 045, Train Loss: 0.0730, Test Loss 0.1611, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 046, Train Loss: 0.0730, Test Loss 0.1611, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 047, Train Loss: 0.0730, Test Loss 0.1611, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 048, Train Loss: 0.0730, Test Loss 0.1611, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 049, Train Loss: 0.0730, Test Loss 0.1611, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "6\n",
      "Epoch: 000, Train Loss: 0.0870, Test Loss 0.2201, Train Acc: 0.4886, Test Acc: 0.6667\n",
      "Epoch: 001, Train Loss: 0.0871, Test Loss 0.2289, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 002, Train Loss: 0.0862, Test Loss 0.2299, Train Acc: 0.5455, Test Acc: 0.6667\n",
      "Epoch: 003, Train Loss: 0.0867, Test Loss 0.2167, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 004, Train Loss: 0.0847, Test Loss 0.2400, Train Acc: 0.5455, Test Acc: 0.3333\n",
      "Epoch: 005, Train Loss: 0.0834, Test Loss 0.2310, Train Acc: 0.6136, Test Acc: 0.6667\n",
      "Epoch: 006, Train Loss: 0.0823, Test Loss 0.2301, Train Acc: 0.6477, Test Acc: 0.3333\n",
      "Epoch: 007, Train Loss: 0.0840, Test Loss 0.1881, Train Acc: 0.5114, Test Acc: 0.6667\n",
      "Epoch: 008, Train Loss: 0.0818, Test Loss 0.2449, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 009, Train Loss: 0.0889, Test Loss 0.1850, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 010, Train Loss: 0.0828, Test Loss 0.2392, Train Acc: 0.5795, Test Acc: 0.3333\n",
      "Epoch: 011, Train Loss: 0.0821, Test Loss 0.1974, Train Acc: 0.5227, Test Acc: 0.6667\n",
      "Epoch: 012, Train Loss: 0.0820, Test Loss 0.1966, Train Acc: 0.5227, Test Acc: 0.6667\n",
      "Epoch: 013, Train Loss: 0.0808, Test Loss 0.2012, Train Acc: 0.5795, Test Acc: 0.6667\n",
      "Epoch: 014, Train Loss: 0.0800, Test Loss 0.2063, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 015, Train Loss: 0.0797, Test Loss 0.2071, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 016, Train Loss: 0.0794, Test Loss 0.2048, Train Acc: 0.7727, Test Acc: 1.0000\n",
      "Epoch: 017, Train Loss: 0.0792, Test Loss 0.2037, Train Acc: 0.7727, Test Acc: 1.0000\n",
      "Epoch: 018, Train Loss: 0.0789, Test Loss 0.2008, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 019, Train Loss: 0.0787, Test Loss 0.1982, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 020, Train Loss: 0.0781, Test Loss 0.2007, Train Acc: 0.8068, Test Acc: 1.0000\n",
      "Epoch: 021, Train Loss: 0.0779, Test Loss 0.1967, Train Acc: 0.8068, Test Acc: 1.0000\n",
      "Epoch: 022, Train Loss: 0.0776, Test Loss 0.1934, Train Acc: 0.8068, Test Acc: 1.0000\n",
      "Epoch: 023, Train Loss: 0.0775, Test Loss 0.1889, Train Acc: 0.7727, Test Acc: 1.0000\n",
      "Epoch: 024, Train Loss: 0.0774, Test Loss 0.1891, Train Acc: 0.7727, Test Acc: 1.0000\n",
      "Epoch: 025, Train Loss: 0.0774, Test Loss 0.1888, Train Acc: 0.7727, Test Acc: 1.0000\n",
      "Epoch: 026, Train Loss: 0.0773, Test Loss 0.1890, Train Acc: 0.7727, Test Acc: 1.0000\n",
      "Epoch: 027, Train Loss: 0.0773, Test Loss 0.1887, Train Acc: 0.7727, Test Acc: 1.0000\n",
      "Epoch: 028, Train Loss: 0.0773, Test Loss 0.1883, Train Acc: 0.7727, Test Acc: 1.0000\n",
      "Epoch: 029, Train Loss: 0.0772, Test Loss 0.1883, Train Acc: 0.7727, Test Acc: 1.0000\n",
      "Epoch: 030, Train Loss: 0.0772, Test Loss 0.1882, Train Acc: 0.7727, Test Acc: 1.0000\n",
      "Epoch: 031, Train Loss: 0.0771, Test Loss 0.1882, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 032, Train Loss: 0.0771, Test Loss 0.1880, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 033, Train Loss: 0.0771, Test Loss 0.1878, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 034, Train Loss: 0.0770, Test Loss 0.1876, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 035, Train Loss: 0.0770, Test Loss 0.1876, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 036, Train Loss: 0.0770, Test Loss 0.1876, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 037, Train Loss: 0.0769, Test Loss 0.1875, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 038, Train Loss: 0.0769, Test Loss 0.1875, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 039, Train Loss: 0.0769, Test Loss 0.1875, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 040, Train Loss: 0.0769, Test Loss 0.1875, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 041, Train Loss: 0.0769, Test Loss 0.1874, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 042, Train Loss: 0.0769, Test Loss 0.1874, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 043, Train Loss: 0.0769, Test Loss 0.1874, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 044, Train Loss: 0.0769, Test Loss 0.1874, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 045, Train Loss: 0.0769, Test Loss 0.1873, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 046, Train Loss: 0.0769, Test Loss 0.1873, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 047, Train Loss: 0.0769, Test Loss 0.1873, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 048, Train Loss: 0.0769, Test Loss 0.1873, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 049, Train Loss: 0.0769, Test Loss 0.1873, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "7\n",
      "Epoch: 000, Train Loss: 0.0949, Test Loss 0.1830, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 001, Train Loss: 0.0863, Test Loss 0.2440, Train Acc: 0.5114, Test Acc: 0.3333\n",
      "Epoch: 002, Train Loss: 0.0854, Test Loss 0.2221, Train Acc: 0.5568, Test Acc: 0.6667\n",
      "Epoch: 003, Train Loss: 0.0867, Test Loss 0.2579, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 004, Train Loss: 0.0847, Test Loss 0.2116, Train Acc: 0.5114, Test Acc: 0.6667\n",
      "Epoch: 005, Train Loss: 0.0837, Test Loss 0.2280, Train Acc: 0.6932, Test Acc: 0.6667\n",
      "Epoch: 006, Train Loss: 0.0849, Test Loss 0.2436, Train Acc: 0.5227, Test Acc: 0.3333\n",
      "Epoch: 007, Train Loss: 0.0845, Test Loss 0.2076, Train Acc: 0.5341, Test Acc: 0.6667\n",
      "Epoch: 008, Train Loss: 0.0844, Test Loss 0.2468, Train Acc: 0.5227, Test Acc: 0.3333\n",
      "Epoch: 009, Train Loss: 0.0822, Test Loss 0.2183, Train Acc: 0.6932, Test Acc: 0.6667\n",
      "Epoch: 010, Train Loss: 0.0817, Test Loss 0.2298, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 011, Train Loss: 0.0800, Test Loss 0.2046, Train Acc: 0.6364, Test Acc: 0.6667\n",
      "Epoch: 012, Train Loss: 0.0797, Test Loss 0.2059, Train Acc: 0.6477, Test Acc: 1.0000\n",
      "Epoch: 013, Train Loss: 0.0793, Test Loss 0.2103, Train Acc: 0.7045, Test Acc: 0.6667\n",
      "Epoch: 014, Train Loss: 0.0791, Test Loss 0.2129, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 015, Train Loss: 0.0788, Test Loss 0.2119, Train Acc: 0.6932, Test Acc: 0.6667\n",
      "Epoch: 016, Train Loss: 0.0789, Test Loss 0.2193, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 017, Train Loss: 0.0784, Test Loss 0.2079, Train Acc: 0.7159, Test Acc: 0.6667\n",
      "Epoch: 018, Train Loss: 0.0782, Test Loss 0.2069, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 019, Train Loss: 0.0780, Test Loss 0.2035, Train Acc: 0.7159, Test Acc: 0.6667\n",
      "Epoch: 020, Train Loss: 0.0778, Test Loss 0.2094, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 021, Train Loss: 0.0776, Test Loss 0.2027, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 022, Train Loss: 0.0773, Test Loss 0.2045, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 023, Train Loss: 0.0771, Test Loss 0.2088, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 024, Train Loss: 0.0771, Test Loss 0.2082, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 025, Train Loss: 0.0771, Test Loss 0.2073, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 026, Train Loss: 0.0770, Test Loss 0.2062, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 027, Train Loss: 0.0770, Test Loss 0.2053, Train Acc: 0.7159, Test Acc: 0.6667\n",
      "Epoch: 028, Train Loss: 0.0770, Test Loss 0.2047, Train Acc: 0.7159, Test Acc: 0.6667\n",
      "Epoch: 029, Train Loss: 0.0770, Test Loss 0.2044, Train Acc: 0.7159, Test Acc: 0.6667\n",
      "Epoch: 030, Train Loss: 0.0769, Test Loss 0.2039, Train Acc: 0.7159, Test Acc: 0.6667\n",
      "Epoch: 031, Train Loss: 0.0769, Test Loss 0.2034, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 032, Train Loss: 0.0769, Test Loss 0.2036, Train Acc: 0.7159, Test Acc: 0.6667\n",
      "Epoch: 033, Train Loss: 0.0769, Test Loss 0.2019, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 034, Train Loss: 0.0769, Test Loss 0.2019, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 035, Train Loss: 0.0769, Test Loss 0.2014, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 036, Train Loss: 0.0768, Test Loss 0.2014, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 037, Train Loss: 0.0768, Test Loss 0.2015, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 038, Train Loss: 0.0768, Test Loss 0.2014, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 039, Train Loss: 0.0768, Test Loss 0.2014, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 040, Train Loss: 0.0768, Test Loss 0.2014, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 041, Train Loss: 0.0768, Test Loss 0.2015, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 042, Train Loss: 0.0768, Test Loss 0.2014, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 043, Train Loss: 0.0768, Test Loss 0.2014, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 044, Train Loss: 0.0768, Test Loss 0.2014, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 045, Train Loss: 0.0768, Test Loss 0.2013, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 046, Train Loss: 0.0768, Test Loss 0.2013, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 047, Train Loss: 0.0768, Test Loss 0.2014, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 048, Train Loss: 0.0768, Test Loss 0.2014, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 049, Train Loss: 0.0768, Test Loss 0.2014, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "8\n",
      "Epoch: 000, Train Loss: 0.0903, Test Loss 0.2060, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 001, Train Loss: 0.0880, Test Loss 0.2748, Train Acc: 0.4773, Test Acc: 0.3333\n",
      "Epoch: 002, Train Loss: 0.0828, Test Loss 0.2338, Train Acc: 0.6591, Test Acc: 0.3333\n",
      "Epoch: 003, Train Loss: 0.0861, Test Loss 0.1975, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 004, Train Loss: 0.0807, Test Loss 0.2373, Train Acc: 0.6136, Test Acc: 0.6667\n",
      "Epoch: 005, Train Loss: 0.0824, Test Loss 0.2593, Train Acc: 0.5227, Test Acc: 0.3333\n",
      "Epoch: 006, Train Loss: 0.0762, Test Loss 0.2062, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 007, Train Loss: 0.0748, Test Loss 0.1800, Train Acc: 0.7614, Test Acc: 1.0000\n",
      "Epoch: 008, Train Loss: 0.0778, Test Loss 0.1640, Train Acc: 0.5909, Test Acc: 0.6667\n",
      "Epoch: 009, Train Loss: 0.0976, Test Loss 0.1683, Train Acc: 0.5227, Test Acc: 0.6667\n",
      "Epoch: 010, Train Loss: 0.0864, Test Loss 0.2791, Train Acc: 0.5114, Test Acc: 0.3333\n",
      "Epoch: 011, Train Loss: 0.0842, Test Loss 0.1581, Train Acc: 0.5682, Test Acc: 0.6667\n",
      "Epoch: 012, Train Loss: 0.0731, Test Loss 0.1593, Train Acc: 0.6818, Test Acc: 1.0000\n",
      "Epoch: 013, Train Loss: 0.0681, Test Loss 0.1666, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 014, Train Loss: 0.0675, Test Loss 0.1658, Train Acc: 0.8068, Test Acc: 1.0000\n",
      "Epoch: 015, Train Loss: 0.0671, Test Loss 0.1644, Train Acc: 0.8068, Test Acc: 1.0000\n",
      "Epoch: 016, Train Loss: 0.0683, Test Loss 0.1590, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 017, Train Loss: 0.0667, Test Loss 0.1617, Train Acc: 0.8182, Test Acc: 1.0000\n",
      "Epoch: 018, Train Loss: 0.0669, Test Loss 0.1593, Train Acc: 0.8182, Test Acc: 1.0000\n",
      "Epoch: 019, Train Loss: 0.0664, Test Loss 0.1595, Train Acc: 0.8068, Test Acc: 1.0000\n",
      "Epoch: 020, Train Loss: 0.0666, Test Loss 0.1566, Train Acc: 0.8068, Test Acc: 1.0000\n",
      "Epoch: 021, Train Loss: 0.0666, Test Loss 0.1554, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 022, Train Loss: 0.0663, Test Loss 0.1544, Train Acc: 0.8068, Test Acc: 1.0000\n",
      "Epoch: 023, Train Loss: 0.0666, Test Loss 0.1526, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 024, Train Loss: 0.0664, Test Loss 0.1527, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 025, Train Loss: 0.0664, Test Loss 0.1527, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 026, Train Loss: 0.0662, Test Loss 0.1530, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 027, Train Loss: 0.0661, Test Loss 0.1530, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 028, Train Loss: 0.0662, Test Loss 0.1528, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 029, Train Loss: 0.0659, Test Loss 0.1531, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 030, Train Loss: 0.0660, Test Loss 0.1528, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 031, Train Loss: 0.0659, Test Loss 0.1528, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 032, Train Loss: 0.0659, Test Loss 0.1527, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 033, Train Loss: 0.0660, Test Loss 0.1526, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 034, Train Loss: 0.0658, Test Loss 0.1528, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 035, Train Loss: 0.0658, Test Loss 0.1526, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 036, Train Loss: 0.0658, Test Loss 0.1526, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 037, Train Loss: 0.0658, Test Loss 0.1526, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 038, Train Loss: 0.0658, Test Loss 0.1526, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 039, Train Loss: 0.0658, Test Loss 0.1526, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 040, Train Loss: 0.0658, Test Loss 0.1526, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 041, Train Loss: 0.0658, Test Loss 0.1526, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 042, Train Loss: 0.0658, Test Loss 0.1525, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 043, Train Loss: 0.0658, Test Loss 0.1525, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 044, Train Loss: 0.0658, Test Loss 0.1526, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 045, Train Loss: 0.0658, Test Loss 0.1525, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 046, Train Loss: 0.0658, Test Loss 0.1525, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 047, Train Loss: 0.0658, Test Loss 0.1525, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 048, Train Loss: 0.0658, Test Loss 0.1525, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 049, Train Loss: 0.0658, Test Loss 0.1525, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "9\n",
      "Epoch: 000, Train Loss: 0.0907, Test Loss 0.2022, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 001, Train Loss: 0.0862, Test Loss 0.2312, Train Acc: 0.5568, Test Acc: 0.3333\n",
      "Epoch: 002, Train Loss: 0.0864, Test Loss 0.2203, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 003, Train Loss: 0.0861, Test Loss 0.2203, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 004, Train Loss: 0.0855, Test Loss 0.2289, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 005, Train Loss: 0.0850, Test Loss 0.2238, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 006, Train Loss: 0.0845, Test Loss 0.2184, Train Acc: 0.6250, Test Acc: 1.0000\n",
      "Epoch: 007, Train Loss: 0.0852, Test Loss 0.2086, Train Acc: 0.5341, Test Acc: 0.6667\n",
      "Epoch: 008, Train Loss: 0.0832, Test Loss 0.2183, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 009, Train Loss: 0.0828, Test Loss 0.2092, Train Acc: 0.6477, Test Acc: 1.0000\n",
      "Epoch: 010, Train Loss: 0.0823, Test Loss 0.2006, Train Acc: 0.6250, Test Acc: 1.0000\n",
      "Epoch: 011, Train Loss: 0.0802, Test Loss 0.2000, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 012, Train Loss: 0.0803, Test Loss 0.1964, Train Acc: 0.6705, Test Acc: 1.0000\n",
      "Epoch: 013, Train Loss: 0.0800, Test Loss 0.1953, Train Acc: 0.6932, Test Acc: 1.0000\n",
      "Epoch: 014, Train Loss: 0.0794, Test Loss 0.1946, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 015, Train Loss: 0.0789, Test Loss 0.1929, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 016, Train Loss: 0.0785, Test Loss 0.1906, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 017, Train Loss: 0.0781, Test Loss 0.1857, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 018, Train Loss: 0.0774, Test Loss 0.1847, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 019, Train Loss: 0.0775, Test Loss 0.1783, Train Acc: 0.6932, Test Acc: 1.0000\n",
      "Epoch: 020, Train Loss: 0.0765, Test Loss 0.1780, Train Acc: 0.6932, Test Acc: 1.0000\n",
      "Epoch: 021, Train Loss: 0.0759, Test Loss 0.1755, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 022, Train Loss: 0.0749, Test Loss 0.1751, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 023, Train Loss: 0.0755, Test Loss 0.1685, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 024, Train Loss: 0.0753, Test Loss 0.1688, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 025, Train Loss: 0.0753, Test Loss 0.1683, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 026, Train Loss: 0.0751, Test Loss 0.1686, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 027, Train Loss: 0.0749, Test Loss 0.1690, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 028, Train Loss: 0.0747, Test Loss 0.1692, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 029, Train Loss: 0.0748, Test Loss 0.1687, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 030, Train Loss: 0.0746, Test Loss 0.1691, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 031, Train Loss: 0.0745, Test Loss 0.1691, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 032, Train Loss: 0.0746, Test Loss 0.1681, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 033, Train Loss: 0.0745, Test Loss 0.1681, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 034, Train Loss: 0.0745, Test Loss 0.1679, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 035, Train Loss: 0.0744, Test Loss 0.1679, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 036, Train Loss: 0.0744, Test Loss 0.1679, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 037, Train Loss: 0.0744, Test Loss 0.1679, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 038, Train Loss: 0.0744, Test Loss 0.1679, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 039, Train Loss: 0.0744, Test Loss 0.1679, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 040, Train Loss: 0.0744, Test Loss 0.1679, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 041, Train Loss: 0.0744, Test Loss 0.1678, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 042, Train Loss: 0.0744, Test Loss 0.1678, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 043, Train Loss: 0.0744, Test Loss 0.1678, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 044, Train Loss: 0.0744, Test Loss 0.1678, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 045, Train Loss: 0.0744, Test Loss 0.1678, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 046, Train Loss: 0.0744, Test Loss 0.1677, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 047, Train Loss: 0.0744, Test Loss 0.1677, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 048, Train Loss: 0.0744, Test Loss 0.1677, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 049, Train Loss: 0.0744, Test Loss 0.1677, Train Acc: 0.7386, Test Acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "eval_metrics = np.zeros((skf.n_splits, 3))\n",
    "\n",
    "labels = [full_dataset[i].y for i in range(len(full_dataset))]\n",
    "\n",
    "\n",
    "for n_fold, (train_idx, test_idx) in tqdm(enumerate(skf.split(labels, labels))):\n",
    "    model = GATv2(full_dataset.num_features, 128, 8).to(device())\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=50//4, gamma=0.1, last_epoch=-1, verbose=False)\n",
    "\n",
    "    train_loader_ = DataLoader(full_dataset[list(train_idx)], batch_size=8, shuffle=True)\n",
    "    test_loader_ = DataLoader(full_dataset[list(test_idx)], batch_size=8, shuffle=True)\n",
    "    min_v_loss = np.inf\n",
    "    print(n_fold)\n",
    "    pr, rc, acc = [], [], []\n",
    "    for epoch in range(50):\n",
    "        train_epoch(train_loader, model, criterion, optimizer)\n",
    "        train_loss, train_acc, _, _ = eval_epoch(train_loader, model, criterion)\n",
    "        val_loss, test_acc, _, _ = eval_epoch(val_loader, model, criterion)\n",
    "        scheduler.step()\n",
    "        print(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.4f}, Test Loss {val_loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "        #print(f'Train Prec: {train_pr:.3f}, Train Rec: {train_rc:.3f}, Test Prec: {val_pr:.3f}, Test Rec: {val_rc:.3f}')\n",
    "        #rc.append(val_rc)\n",
    "        #pr.append(val_pr)\n",
    "        acc.append(test_acc)\n",
    "        if min_v_loss > val_loss:\n",
    "            min_v_loss = val_loss\n",
    "            best_test_acc = test_acc\n",
    "\n",
    "    eval_metrics[n_fold, 0] = best_test_acc\n",
    "    eval_metrics[n_fold, 1] = np.mean(acc)\n",
    "    eval_metrics[n_fold, 2] = np.std(acc)\n",
    "### eval_metrics[n_fold, 3] ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.66666667, 0.65333333, 0.11469767],\n",
       "       [1.        , 0.85333333, 0.25086517],\n",
       "       [0.66666667, 0.61333333, 0.16812694],\n",
       "       [1.        , 0.93333333, 0.18856181],\n",
       "       [0.66666667, 0.64666667, 0.07916228],\n",
       "       [1.        , 0.89333333, 0.21540659],\n",
       "       [0.66666667, 0.87333333, 0.2096558 ],\n",
       "       [0.66666667, 0.64666667, 0.1034945 ],\n",
       "       [1.        , 0.9       , 0.20275875],\n",
       "       [1.        , 0.96      , 0.12719189]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8333333333333333, 0.16666666666666669)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(eval_metrics[:, 0]), np.std(eval_metrics[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7973333333333333, 0.13176409897152477)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(eval_metrics[:, 1]), np.std(eval_metrics[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from captum.attr import Saliency, IntegratedGradients\n",
    "\n",
    "def model_forward(edge_mask, data):\n",
    "    batch = torch.zeros(data.x.shape[0], dtype=int).to(device())\n",
    "    out = model(data) # .x, data.edge_index, batch, edge_mask\n",
    "    return out\n",
    "\n",
    "\n",
    "def explain(method, data, target=0):\n",
    "    input_mask = torch.ones(data.edge_index.shape[1]).requires_grad_(True).to(device)\n",
    "    if method == 'ig':\n",
    "        ig = IntegratedGradients(model_forward)\n",
    "        mask = ig.attribute(input_mask, target=target,\n",
    "                            additional_forward_args=(data,),\n",
    "                            internal_batch_size=data.edge_index.shape[1])\n",
    "    elif method == 'saliency':\n",
    "        saliency = Saliency(model_forward)\n",
    "        mask = saliency.attribute(input_mask, target=target,\n",
    "                                  additional_forward_args=(data,))\n",
    "    else:\n",
    "        raise Exception('Unknown explanation method')\n",
    "\n",
    "    edge_mask = np.abs(mask.cpu().detach().numpy())\n",
    "    if edge_mask.max() > 0:  # avoid division by zero\n",
    "        edge_mask = edge_mask / edge_mask.max()\n",
    "    return edge_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Open_Close_GNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "266da1a1721e4588a8feaf69449a5145239fab947ce8997260eacc776b5422ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
