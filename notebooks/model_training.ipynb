{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.16 (default, Jan 17 2023, 16:06:28) [MSC v.1916 64 bit (AMD64)] on win32\n"
     ]
    }
   ],
   "source": [
    "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
    "sys.path.extend(['C:\\\\Users\\\\user\\\\PycharmProjects\\\\Open_Close_GNN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from model.dataset import OpenCloseDataset\n",
    "from model.gnn_model import GCN, GATv2\n",
    "import os\n",
    "from model.utils import train, device, train_epoch, eval_epoch, cross_val\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import lr_scheduler\n",
    "from torch_geometric.loader import DataLoader\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import global_mean_pool, GCNConv, GATConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset = OpenCloseDataset(datafolder='../data', reload=False, k_degree=10).shuffle()\n",
    "len(os.listdir('../data/processed'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_size = int(0.94 * len(full_dataset))\n",
    "train_dataset, val_dataset = full_dataset[:train_size], full_dataset[train_size:]\n",
    "#val_dataset, test_dataset = val_dataset[:-3], val_dataset[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-03.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "116858"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GCN(full_dataset.num_features, channels=[256, 32, 8], dropout=0.1).to(device())\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "epochs = 20\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=epochs//3, gamma=0.1, last_epoch=-1, verbose=True)\n",
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a271b2d87df4494b900f24654ce299e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Loss: 0.0238, Test Loss 0.1088, Train Acc: 0.4896, Test Acc: 0.6667\n",
      "Test precision: 0.0000, Test recall: 0.0000\n",
      "Epoch: 002, Train Loss: 0.0234, Test Loss 0.1109, Train Acc: 0.5035, Test Acc: 0.8333\n",
      "Test precision: 1.0000, Test recall: 0.5000\n",
      "Epoch: 003, Train Loss: 0.0231, Test Loss 0.1085, Train Acc: 0.5139, Test Acc: 0.8333\n",
      "Test precision: 1.0000, Test recall: 0.5000\n",
      "Epoch: 004, Train Loss: 0.0229, Test Loss 0.1033, Train Acc: 0.5208, Test Acc: 0.8333\n",
      "Test precision: 1.0000, Test recall: 0.5000\n",
      "Epoch: 005, Train Loss: 0.0221, Test Loss 0.1036, Train Acc: 0.7118, Test Acc: 0.8333\n",
      "Test precision: 1.0000, Test recall: 0.5000\n",
      "Epoch: 006, Train Loss: 0.0211, Test Loss 0.1168, Train Acc: 0.8264, Test Acc: 0.6667\n",
      "Test precision: 0.5000, Test recall: 1.0000\n",
      "Epoch: 007, Train Loss: 0.0200, Test Loss 0.1000, Train Acc: 0.8750, Test Acc: 0.8333\n",
      "Test precision: 0.6667, Test recall: 1.0000\n",
      "Epoch: 008, Train Loss: 0.0190, Test Loss 0.0970, Train Acc: 0.9340, Test Acc: 0.8333\n",
      "Test precision: 0.6667, Test recall: 1.0000\n",
      "Epoch: 009, Train Loss: 0.0176, Test Loss 0.1177, Train Acc: 0.8438, Test Acc: 0.6667\n",
      "Test precision: 0.5000, Test recall: 1.0000\n",
      "Epoch: 010, Train Loss: 0.0169, Test Loss 0.1064, Train Acc: 0.9792, Test Acc: 0.6667\n",
      "Test precision: 0.5000, Test recall: 1.0000\n",
      "Epoch: 011, Train Loss: 0.0156, Test Loss 0.1049, Train Acc: 0.9861, Test Acc: 0.6667\n",
      "Test precision: 0.5000, Test recall: 1.0000\n",
      "Epoch: 012, Train Loss: 0.0147, Test Loss 0.1452, Train Acc: 0.8229, Test Acc: 0.6667\n",
      "Test precision: 0.5000, Test recall: 1.0000\n",
      "Epoch: 013, Train Loss: 0.0133, Test Loss 0.0845, Train Acc: 0.9792, Test Acc: 0.6667\n",
      "Test precision: 0.5000, Test recall: 1.0000\n",
      "Epoch: 014, Train Loss: 0.0110, Test Loss 0.1135, Train Acc: 0.9896, Test Acc: 0.6667\n",
      "Test precision: 0.5000, Test recall: 1.0000\n",
      "Epoch: 015, Train Loss: 0.0104, Test Loss 0.1204, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Test precision: 0.5000, Test recall: 1.0000\n",
      "Epoch: 016, Train Loss: 0.0102, Test Loss 0.0855, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Test precision: 0.5000, Test recall: 1.0000\n",
      "Epoch: 017, Train Loss: 0.0078, Test Loss 0.1246, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Test precision: 0.5000, Test recall: 1.0000\n",
      "Epoch: 018, Train Loss: 0.0076, Test Loss 0.1632, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Test precision: 0.5000, Test recall: 1.0000\n",
      "Epoch: 019, Train Loss: 0.0064, Test Loss 0.1148, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Test precision: 0.5000, Test recall: 1.0000\n",
      "Epoch: 020, Train Loss: 0.0053, Test Loss 0.1496, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Test precision: 0.5000, Test recall: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history = train(model, epochs, train_loader, val_loader, loss, optimizer, scheduler=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loss, val_loss, train_acc, test_acc = zip(*history)\n",
    "# k = None no dropuot\n",
    "plt.plot(train_loss)\n",
    "plt.plot(val_loss);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220722"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GATv2(full_dataset.num_features, 128, 8).to(device())\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "epochs = 40\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=epochs//4, gamma=0.1, last_epoch=-1, verbose=False)\n",
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd684b0d9b454d3daaa118d4d64b114b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Loss: 0.0235, Test Loss 0.1103, Train Acc: 0.4886, Test Acc: 0.6667\n",
      "Epoch: 002, Train Loss: 0.0226, Test Loss 0.1197, Train Acc: 0.6591, Test Acc: 0.3333\n",
      "Epoch: 003, Train Loss: 0.0217, Test Loss 0.1306, Train Acc: 0.6477, Test Acc: 0.3333\n",
      "Epoch: 004, Train Loss: 0.0205, Test Loss 0.1211, Train Acc: 0.7159, Test Acc: 0.5000\n",
      "Epoch: 005, Train Loss: 0.0209, Test Loss 0.1129, Train Acc: 0.5795, Test Acc: 0.6667\n",
      "Epoch: 006, Train Loss: 0.0171, Test Loss 0.1398, Train Acc: 0.8750, Test Acc: 0.3333\n",
      "Epoch: 007, Train Loss: 0.0167, Test Loss 0.1238, Train Acc: 0.7386, Test Acc: 0.5000\n",
      "Epoch: 008, Train Loss: 0.0152, Test Loss 0.1208, Train Acc: 0.7955, Test Acc: 0.5000\n",
      "Epoch: 009, Train Loss: 0.0124, Test Loss 0.1614, Train Acc: 0.9205, Test Acc: 0.3333\n",
      "Epoch: 010, Train Loss: 0.0105, Test Loss 0.1592, Train Acc: 0.9659, Test Acc: 0.5000\n",
      "Epoch: 011, Train Loss: 0.0084, Test Loss 0.1703, Train Acc: 0.9886, Test Acc: 0.5000\n",
      "Epoch: 012, Train Loss: 0.0071, Test Loss 0.1517, Train Acc: 0.9659, Test Acc: 0.6667\n",
      "Epoch: 013, Train Loss: 0.0045, Test Loss 0.1826, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 014, Train Loss: 0.0032, Test Loss 0.2240, Train Acc: 1.0000, Test Acc: 0.5000\n",
      "Epoch: 015, Train Loss: 0.0028, Test Loss 0.1990, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 016, Train Loss: 0.0019, Test Loss 0.2210, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 017, Train Loss: 0.0016, Test Loss 0.2919, Train Acc: 1.0000, Test Acc: 0.5000\n",
      "Epoch: 018, Train Loss: 0.0019, Test Loss 0.3517, Train Acc: 1.0000, Test Acc: 0.5000\n",
      "Epoch: 019, Train Loss: 0.0008, Test Loss 0.3261, Train Acc: 1.0000, Test Acc: 0.5000\n",
      "Epoch: 020, Train Loss: 0.0007, Test Loss 0.2985, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 021, Train Loss: 0.0005, Test Loss 0.3158, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 022, Train Loss: 0.0005, Test Loss 0.3789, Train Acc: 1.0000, Test Acc: 0.5000\n",
      "Epoch: 023, Train Loss: 0.0013, Test Loss 0.4599, Train Acc: 1.0000, Test Acc: 0.5000\n",
      "Epoch: 024, Train Loss: 0.0003, Test Loss 0.4063, Train Acc: 1.0000, Test Acc: 0.5000\n",
      "Epoch: 025, Train Loss: 0.0004, Test Loss 0.3520, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 026, Train Loss: 0.0003, Test Loss 0.3682, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 027, Train Loss: 0.0002, Test Loss 0.4074, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 028, Train Loss: 0.0002, Test Loss 0.4326, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 029, Train Loss: 0.0001, Test Loss 0.4364, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 030, Train Loss: 0.0001, Test Loss 0.4230, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 031, Train Loss: 0.0002, Test Loss 0.4087, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 032, Train Loss: 0.0002, Test Loss 0.4036, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 033, Train Loss: 0.0001, Test Loss 0.4097, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 034, Train Loss: 0.0001, Test Loss 0.4259, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 035, Train Loss: 0.0001, Test Loss 0.4392, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 036, Train Loss: 0.0001, Test Loss 0.4370, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 037, Train Loss: 0.0001, Test Loss 0.4389, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 038, Train Loss: 0.0001, Test Loss 0.4398, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 039, Train Loss: 0.0001, Test Loss 0.4334, Train Acc: 1.0000, Test Acc: 0.6667\n",
      "Epoch: 040, Train Loss: 0.0001, Test Loss 0.4350, Train Acc: 1.0000, Test Acc: 0.6667\n"
     ]
    }
   ],
   "source": [
    "history = train(model, epochs, train_loader, val_loader, loss, optimizer, scheduler=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loss, val_loss = zip(*history)\n",
    "\n",
    "plt.plot(train_loss)\n",
    "plt.plot(val_loss);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '../weights/gat_2heads_128_8_knn30.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b301190b54244c4b410d148fbadcd6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'lr'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17000\\2619165759.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m cross_val(full_dataset, GCN, lr=0.001, step_size=50//4, gamma=0.1, last_epoch=-1, verbose=False,\n\u001b[1;32m----> 2\u001b[1;33m           num_features=full_dataset.num_features, channels=[256, 32, 8], dropout=0.3)\n\u001b[0m",
      "\u001b[1;32m~\\PycharmProjects\\Open_Close_GNN\\model\\utils.py\u001b[0m in \u001b[0;36mcross_val\u001b[1;34m(data, model_name, n_splits, epochs, batch_size, **kwargs)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mn_fold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_idx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mskf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m         \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[0mcriterion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'lr'"
     ]
    }
   ],
   "source": [
    "cross_val(full_dataset, GCN, lr=0.001, step_size=50//4, gamma=0.1, last_epoch=-1, verbose=False,\n",
    "          num_features=full_dataset.num_features, channels=[256, 32, 8], dropout=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e79c08e398d24141950bddd238dbc5f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Train Loss: 0.0865, Test Loss 0.2112, Train Acc: 0.4545, Test Acc: 0.6667\n",
      "Epoch: 001, Train Loss: 0.0863, Test Loss 0.2283, Train Acc: 0.4886, Test Acc: 0.3333\n",
      "Epoch: 002, Train Loss: 0.0859, Test Loss 0.2202, Train Acc: 0.5114, Test Acc: 0.6667\n",
      "Epoch: 003, Train Loss: 0.0856, Test Loss 0.2265, Train Acc: 0.5227, Test Acc: 0.6667\n",
      "Epoch: 004, Train Loss: 0.0854, Test Loss 0.2065, Train Acc: 0.5568, Test Acc: 1.0000\n",
      "Epoch: 005, Train Loss: 0.0848, Test Loss 0.2246, Train Acc: 0.5455, Test Acc: 0.6667\n",
      "Epoch: 006, Train Loss: 0.0840, Test Loss 0.2156, Train Acc: 0.6136, Test Acc: 0.6667\n",
      "Epoch: 007, Train Loss: 0.0831, Test Loss 0.2139, Train Acc: 0.6364, Test Acc: 0.6667\n",
      "Epoch: 008, Train Loss: 0.0820, Test Loss 0.2101, Train Acc: 0.6477, Test Acc: 0.6667\n",
      "Epoch: 009, Train Loss: 0.0819, Test Loss 0.2072, Train Acc: 0.6364, Test Acc: 0.6667\n",
      "Epoch: 010, Train Loss: 0.0820, Test Loss 0.1909, Train Acc: 0.6136, Test Acc: 1.0000\n",
      "Epoch: 011, Train Loss: 0.0778, Test Loss 0.2059, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 012, Train Loss: 0.0778, Test Loss 0.1972, Train Acc: 0.7045, Test Acc: 0.6667\n",
      "Epoch: 013, Train Loss: 0.0768, Test Loss 0.2073, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 014, Train Loss: 0.0763, Test Loss 0.2080, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 015, Train Loss: 0.0753, Test Loss 0.2019, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 016, Train Loss: 0.0746, Test Loss 0.1985, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 017, Train Loss: 0.0740, Test Loss 0.2023, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 018, Train Loss: 0.0733, Test Loss 0.1936, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 019, Train Loss: 0.0727, Test Loss 0.1914, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 020, Train Loss: 0.0720, Test Loss 0.1976, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 021, Train Loss: 0.0717, Test Loss 0.2040, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 022, Train Loss: 0.0710, Test Loss 0.1873, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 023, Train Loss: 0.0704, Test Loss 0.1975, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 024, Train Loss: 0.0703, Test Loss 0.1954, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 025, Train Loss: 0.0702, Test Loss 0.1941, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 026, Train Loss: 0.0702, Test Loss 0.1934, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 027, Train Loss: 0.0701, Test Loss 0.1940, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 028, Train Loss: 0.0701, Test Loss 0.1928, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 029, Train Loss: 0.0700, Test Loss 0.1917, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 030, Train Loss: 0.0700, Test Loss 0.1913, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 031, Train Loss: 0.0699, Test Loss 0.1918, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 032, Train Loss: 0.0699, Test Loss 0.1906, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 033, Train Loss: 0.0698, Test Loss 0.1903, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 034, Train Loss: 0.0698, Test Loss 0.1894, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 035, Train Loss: 0.0697, Test Loss 0.1898, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 036, Train Loss: 0.0697, Test Loss 0.1898, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 037, Train Loss: 0.0697, Test Loss 0.1898, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 038, Train Loss: 0.0697, Test Loss 0.1899, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 039, Train Loss: 0.0697, Test Loss 0.1899, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 040, Train Loss: 0.0697, Test Loss 0.1899, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 041, Train Loss: 0.0697, Test Loss 0.1900, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 042, Train Loss: 0.0697, Test Loss 0.1900, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 043, Train Loss: 0.0697, Test Loss 0.1900, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 044, Train Loss: 0.0697, Test Loss 0.1899, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 045, Train Loss: 0.0697, Test Loss 0.1900, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 046, Train Loss: 0.0697, Test Loss 0.1899, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 047, Train Loss: 0.0697, Test Loss 0.1899, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 048, Train Loss: 0.0697, Test Loss 0.1899, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 049, Train Loss: 0.0697, Test Loss 0.1900, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 000, Train Loss: 0.0863, Test Loss 0.2153, Train Acc: 0.5000, Test Acc: 1.0000\n",
      "Epoch: 001, Train Loss: 0.0860, Test Loss 0.2284, Train Acc: 0.5455, Test Acc: 0.3333\n",
      "Epoch: 002, Train Loss: 0.0859, Test Loss 0.2244, Train Acc: 0.5682, Test Acc: 0.6667\n",
      "Epoch: 003, Train Loss: 0.0857, Test Loss 0.2200, Train Acc: 0.5455, Test Acc: 0.6667\n",
      "Epoch: 004, Train Loss: 0.0855, Test Loss 0.2143, Train Acc: 0.5455, Test Acc: 1.0000\n",
      "Epoch: 005, Train Loss: 0.0853, Test Loss 0.2111, Train Acc: 0.6023, Test Acc: 1.0000\n",
      "Epoch: 006, Train Loss: 0.0847, Test Loss 0.2223, Train Acc: 0.6023, Test Acc: 0.6667\n",
      "Epoch: 007, Train Loss: 0.0842, Test Loss 0.2197, Train Acc: 0.6591, Test Acc: 0.6667\n",
      "Epoch: 008, Train Loss: 0.0838, Test Loss 0.2053, Train Acc: 0.6364, Test Acc: 1.0000\n",
      "Epoch: 009, Train Loss: 0.0824, Test Loss 0.2195, Train Acc: 0.6364, Test Acc: 0.6667\n",
      "Epoch: 010, Train Loss: 0.0825, Test Loss 0.1957, Train Acc: 0.6136, Test Acc: 1.0000\n",
      "Epoch: 011, Train Loss: 0.0801, Test Loss 0.2043, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 012, Train Loss: 0.0800, Test Loss 0.2017, Train Acc: 0.6818, Test Acc: 1.0000\n",
      "Epoch: 013, Train Loss: 0.0796, Test Loss 0.2037, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 014, Train Loss: 0.0793, Test Loss 0.2061, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 015, Train Loss: 0.0790, Test Loss 0.2098, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 016, Train Loss: 0.0784, Test Loss 0.2043, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 017, Train Loss: 0.0779, Test Loss 0.1996, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 018, Train Loss: 0.0775, Test Loss 0.2045, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 019, Train Loss: 0.0771, Test Loss 0.2047, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 020, Train Loss: 0.0767, Test Loss 0.2005, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 021, Train Loss: 0.0763, Test Loss 0.2012, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 022, Train Loss: 0.0765, Test Loss 0.1869, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 023, Train Loss: 0.0756, Test Loss 0.2018, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 024, Train Loss: 0.0755, Test Loss 0.2016, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 025, Train Loss: 0.0755, Test Loss 0.2011, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 026, Train Loss: 0.0754, Test Loss 0.1998, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 027, Train Loss: 0.0754, Test Loss 0.1999, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 028, Train Loss: 0.0753, Test Loss 0.1993, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 029, Train Loss: 0.0753, Test Loss 0.1992, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 030, Train Loss: 0.0752, Test Loss 0.1978, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 031, Train Loss: 0.0752, Test Loss 0.1982, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 032, Train Loss: 0.0752, Test Loss 0.1984, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 033, Train Loss: 0.0751, Test Loss 0.1975, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 034, Train Loss: 0.0751, Test Loss 0.1986, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 035, Train Loss: 0.0750, Test Loss 0.1981, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 036, Train Loss: 0.0750, Test Loss 0.1983, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 037, Train Loss: 0.0750, Test Loss 0.1982, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 038, Train Loss: 0.0750, Test Loss 0.1982, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 039, Train Loss: 0.0750, Test Loss 0.1982, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 040, Train Loss: 0.0750, Test Loss 0.1982, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 041, Train Loss: 0.0750, Test Loss 0.1981, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 042, Train Loss: 0.0750, Test Loss 0.1981, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 043, Train Loss: 0.0750, Test Loss 0.1981, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 044, Train Loss: 0.0750, Test Loss 0.1981, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 045, Train Loss: 0.0750, Test Loss 0.1981, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 046, Train Loss: 0.0750, Test Loss 0.1981, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 047, Train Loss: 0.0750, Test Loss 0.1980, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 048, Train Loss: 0.0750, Test Loss 0.1981, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 049, Train Loss: 0.0750, Test Loss 0.1980, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 000, Train Loss: 0.0869, Test Loss 0.2471, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 001, Train Loss: 0.0866, Test Loss 0.2323, Train Acc: 0.4432, Test Acc: 0.3333\n",
      "Epoch: 002, Train Loss: 0.0865, Test Loss 0.2306, Train Acc: 0.5568, Test Acc: 0.6667\n",
      "Epoch: 003, Train Loss: 0.0862, Test Loss 0.2260, Train Acc: 0.5114, Test Acc: 0.6667\n",
      "Epoch: 004, Train Loss: 0.0861, Test Loss 0.2290, Train Acc: 0.6364, Test Acc: 0.6667\n",
      "Epoch: 005, Train Loss: 0.0863, Test Loss 0.2177, Train Acc: 0.5114, Test Acc: 0.6667\n",
      "Epoch: 006, Train Loss: 0.0857, Test Loss 0.2294, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 007, Train Loss: 0.0851, Test Loss 0.2198, Train Acc: 0.5341, Test Acc: 0.6667\n",
      "Epoch: 008, Train Loss: 0.0844, Test Loss 0.2124, Train Acc: 0.5795, Test Acc: 1.0000\n",
      "Epoch: 009, Train Loss: 0.0841, Test Loss 0.2324, Train Acc: 0.6023, Test Acc: 0.3333\n",
      "Epoch: 010, Train Loss: 0.0852, Test Loss 0.2402, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 011, Train Loss: 0.0844, Test Loss 0.2322, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 012, Train Loss: 0.0843, Test Loss 0.2316, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 013, Train Loss: 0.0840, Test Loss 0.2297, Train Acc: 0.5114, Test Acc: 0.3333\n",
      "Epoch: 014, Train Loss: 0.0836, Test Loss 0.2273, Train Acc: 0.5227, Test Acc: 0.3333\n",
      "Epoch: 015, Train Loss: 0.0828, Test Loss 0.2249, Train Acc: 0.5227, Test Acc: 0.3333\n",
      "Epoch: 016, Train Loss: 0.0820, Test Loss 0.2288, Train Acc: 0.5455, Test Acc: 0.3333\n",
      "Epoch: 017, Train Loss: 0.0817, Test Loss 0.2318, Train Acc: 0.5455, Test Acc: 0.3333\n",
      "Epoch: 018, Train Loss: 0.0816, Test Loss 0.2173, Train Acc: 0.6818, Test Acc: 1.0000\n",
      "Epoch: 019, Train Loss: 0.0811, Test Loss 0.2178, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 020, Train Loss: 0.0805, Test Loss 0.2243, Train Acc: 0.6023, Test Acc: 0.3333\n",
      "Epoch: 021, Train Loss: 0.0802, Test Loss 0.2208, Train Acc: 0.6477, Test Acc: 0.3333\n",
      "Epoch: 022, Train Loss: 0.0799, Test Loss 0.2211, Train Acc: 0.6364, Test Acc: 0.3333\n",
      "Epoch: 023, Train Loss: 0.0798, Test Loss 0.2146, Train Acc: 0.6818, Test Acc: 0.6667\n",
      "Epoch: 024, Train Loss: 0.0797, Test Loss 0.2152, Train Acc: 0.6932, Test Acc: 0.6667\n",
      "Epoch: 025, Train Loss: 0.0796, Test Loss 0.2161, Train Acc: 0.6932, Test Acc: 0.6667\n",
      "Epoch: 026, Train Loss: 0.0796, Test Loss 0.2161, Train Acc: 0.6932, Test Acc: 0.6667\n",
      "Epoch: 027, Train Loss: 0.0795, Test Loss 0.2165, Train Acc: 0.6932, Test Acc: 0.6667\n",
      "Epoch: 028, Train Loss: 0.0795, Test Loss 0.2168, Train Acc: 0.6818, Test Acc: 0.6667\n",
      "Epoch: 029, Train Loss: 0.0794, Test Loss 0.2174, Train Acc: 0.6818, Test Acc: 0.6667\n",
      "Epoch: 030, Train Loss: 0.0794, Test Loss 0.2176, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 031, Train Loss: 0.0794, Test Loss 0.2173, Train Acc: 0.6818, Test Acc: 0.6667\n",
      "Epoch: 032, Train Loss: 0.0793, Test Loss 0.2172, Train Acc: 0.6818, Test Acc: 0.6667\n",
      "Epoch: 033, Train Loss: 0.0793, Test Loss 0.2175, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 034, Train Loss: 0.0793, Test Loss 0.2173, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 035, Train Loss: 0.0793, Test Loss 0.2175, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 036, Train Loss: 0.0793, Test Loss 0.2175, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 037, Train Loss: 0.0793, Test Loss 0.2175, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 038, Train Loss: 0.0793, Test Loss 0.2174, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 039, Train Loss: 0.0792, Test Loss 0.2174, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 040, Train Loss: 0.0792, Test Loss 0.2174, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 041, Train Loss: 0.0792, Test Loss 0.2174, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 042, Train Loss: 0.0792, Test Loss 0.2174, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 043, Train Loss: 0.0792, Test Loss 0.2174, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 044, Train Loss: 0.0792, Test Loss 0.2174, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 045, Train Loss: 0.0792, Test Loss 0.2174, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 046, Train Loss: 0.0792, Test Loss 0.2174, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 047, Train Loss: 0.0792, Test Loss 0.2173, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 048, Train Loss: 0.0792, Test Loss 0.2173, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 049, Train Loss: 0.0792, Test Loss 0.2173, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 000, Train Loss: 0.0867, Test Loss 0.2206, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 001, Train Loss: 0.0865, Test Loss 0.2273, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 002, Train Loss: 0.0863, Test Loss 0.2260, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 003, Train Loss: 0.0862, Test Loss 0.2211, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 004, Train Loss: 0.0860, Test Loss 0.2263, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 005, Train Loss: 0.0859, Test Loss 0.2191, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 006, Train Loss: 0.0863, Test Loss 0.2157, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 007, Train Loss: 0.0860, Test Loss 0.2184, Train Acc: 0.5114, Test Acc: 0.6667\n",
      "Epoch: 008, Train Loss: 0.0862, Test Loss 0.2302, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 009, Train Loss: 0.0857, Test Loss 0.2373, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 010, Train Loss: 0.0854, Test Loss 0.2252, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 011, Train Loss: 0.0853, Test Loss 0.2308, Train Acc: 0.6591, Test Acc: 0.6667\n",
      "Epoch: 012, Train Loss: 0.0851, Test Loss 0.2289, Train Acc: 0.6932, Test Acc: 0.6667\n",
      "Epoch: 013, Train Loss: 0.0847, Test Loss 0.2260, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 014, Train Loss: 0.0843, Test Loss 0.2246, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 015, Train Loss: 0.0839, Test Loss 0.2243, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 016, Train Loss: 0.0836, Test Loss 0.2198, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 017, Train Loss: 0.0833, Test Loss 0.2245, Train Acc: 0.6818, Test Acc: 0.6667\n",
      "Epoch: 018, Train Loss: 0.0830, Test Loss 0.2167, Train Acc: 0.7614, Test Acc: 1.0000\n",
      "Epoch: 019, Train Loss: 0.0828, Test Loss 0.2215, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 020, Train Loss: 0.0825, Test Loss 0.2209, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 021, Train Loss: 0.0823, Test Loss 0.2176, Train Acc: 0.7614, Test Acc: 1.0000\n",
      "Epoch: 022, Train Loss: 0.0821, Test Loss 0.2176, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 023, Train Loss: 0.0821, Test Loss 0.2079, Train Acc: 0.6818, Test Acc: 1.0000\n",
      "Epoch: 024, Train Loss: 0.0820, Test Loss 0.2085, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 025, Train Loss: 0.0819, Test Loss 0.2094, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 026, Train Loss: 0.0819, Test Loss 0.2102, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 027, Train Loss: 0.0818, Test Loss 0.2106, Train Acc: 0.7614, Test Acc: 1.0000\n",
      "Epoch: 028, Train Loss: 0.0818, Test Loss 0.2108, Train Acc: 0.7614, Test Acc: 1.0000\n",
      "Epoch: 029, Train Loss: 0.0818, Test Loss 0.2112, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 030, Train Loss: 0.0817, Test Loss 0.2117, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 031, Train Loss: 0.0817, Test Loss 0.2124, Train Acc: 0.8068, Test Acc: 1.0000\n",
      "Epoch: 032, Train Loss: 0.0817, Test Loss 0.2123, Train Acc: 0.8068, Test Acc: 1.0000\n",
      "Epoch: 033, Train Loss: 0.0817, Test Loss 0.2122, Train Acc: 0.8068, Test Acc: 1.0000\n",
      "Epoch: 034, Train Loss: 0.0816, Test Loss 0.2123, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 035, Train Loss: 0.0816, Test Loss 0.2130, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 036, Train Loss: 0.0816, Test Loss 0.2130, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 037, Train Loss: 0.0816, Test Loss 0.2130, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 038, Train Loss: 0.0816, Test Loss 0.2130, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 039, Train Loss: 0.0816, Test Loss 0.2130, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 040, Train Loss: 0.0816, Test Loss 0.2130, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 041, Train Loss: 0.0816, Test Loss 0.2130, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 042, Train Loss: 0.0816, Test Loss 0.2130, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 043, Train Loss: 0.0816, Test Loss 0.2130, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 044, Train Loss: 0.0816, Test Loss 0.2130, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 045, Train Loss: 0.0816, Test Loss 0.2130, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 046, Train Loss: 0.0816, Test Loss 0.2130, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 047, Train Loss: 0.0816, Test Loss 0.2130, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 048, Train Loss: 0.0816, Test Loss 0.2130, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 049, Train Loss: 0.0816, Test Loss 0.2130, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 000, Train Loss: 0.0864, Test Loss 0.2116, Train Acc: 0.5568, Test Acc: 1.0000\n",
      "Epoch: 001, Train Loss: 0.0860, Test Loss 0.2201, Train Acc: 0.5114, Test Acc: 0.6667\n",
      "Epoch: 002, Train Loss: 0.0861, Test Loss 0.2337, Train Acc: 0.5114, Test Acc: 0.3333\n",
      "Epoch: 003, Train Loss: 0.0854, Test Loss 0.2180, Train Acc: 0.5114, Test Acc: 0.6667\n",
      "Epoch: 004, Train Loss: 0.0852, Test Loss 0.2149, Train Acc: 0.5682, Test Acc: 0.6667\n",
      "Epoch: 005, Train Loss: 0.0847, Test Loss 0.2122, Train Acc: 0.5909, Test Acc: 1.0000\n",
      "Epoch: 006, Train Loss: 0.0845, Test Loss 0.2303, Train Acc: 0.5568, Test Acc: 0.3333\n",
      "Epoch: 007, Train Loss: 0.0836, Test Loss 0.2120, Train Acc: 0.6023, Test Acc: 0.6667\n",
      "Epoch: 008, Train Loss: 0.0832, Test Loss 0.2296, Train Acc: 0.5909, Test Acc: 0.3333\n",
      "Epoch: 009, Train Loss: 0.0820, Test Loss 0.2148, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 010, Train Loss: 0.0829, Test Loss 0.2136, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 011, Train Loss: 0.0822, Test Loss 0.2070, Train Acc: 0.6364, Test Acc: 1.0000\n",
      "Epoch: 012, Train Loss: 0.0820, Test Loss 0.2078, Train Acc: 0.6364, Test Acc: 0.6667\n",
      "Epoch: 013, Train Loss: 0.0818, Test Loss 0.2091, Train Acc: 0.6364, Test Acc: 0.6667\n",
      "Epoch: 014, Train Loss: 0.0815, Test Loss 0.2140, Train Acc: 0.7159, Test Acc: 0.6667\n",
      "Epoch: 015, Train Loss: 0.0813, Test Loss 0.2164, Train Acc: 0.7045, Test Acc: 0.6667\n",
      "Epoch: 016, Train Loss: 0.0811, Test Loss 0.2134, Train Acc: 0.7159, Test Acc: 0.6667\n",
      "Epoch: 017, Train Loss: 0.0809, Test Loss 0.2134, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 018, Train Loss: 0.0808, Test Loss 0.2163, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 019, Train Loss: 0.0805, Test Loss 0.2096, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 020, Train Loss: 0.0801, Test Loss 0.2099, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 021, Train Loss: 0.0799, Test Loss 0.2125, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 022, Train Loss: 0.0797, Test Loss 0.2099, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 023, Train Loss: 0.0795, Test Loss 0.2097, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 024, Train Loss: 0.0794, Test Loss 0.2091, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 025, Train Loss: 0.0794, Test Loss 0.2087, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 026, Train Loss: 0.0794, Test Loss 0.2089, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 027, Train Loss: 0.0794, Test Loss 0.2090, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 028, Train Loss: 0.0793, Test Loss 0.2092, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 029, Train Loss: 0.0793, Test Loss 0.2092, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 030, Train Loss: 0.0793, Test Loss 0.2088, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 031, Train Loss: 0.0793, Test Loss 0.2093, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 032, Train Loss: 0.0793, Test Loss 0.2094, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 033, Train Loss: 0.0792, Test Loss 0.2091, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 034, Train Loss: 0.0792, Test Loss 0.2088, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 035, Train Loss: 0.0792, Test Loss 0.2089, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 036, Train Loss: 0.0792, Test Loss 0.2088, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 037, Train Loss: 0.0792, Test Loss 0.2089, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 038, Train Loss: 0.0792, Test Loss 0.2088, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 039, Train Loss: 0.0792, Test Loss 0.2089, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 040, Train Loss: 0.0792, Test Loss 0.2089, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 041, Train Loss: 0.0792, Test Loss 0.2089, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 042, Train Loss: 0.0792, Test Loss 0.2089, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 043, Train Loss: 0.0792, Test Loss 0.2089, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 044, Train Loss: 0.0792, Test Loss 0.2089, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 045, Train Loss: 0.0792, Test Loss 0.2089, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 046, Train Loss: 0.0792, Test Loss 0.2089, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 047, Train Loss: 0.0792, Test Loss 0.2089, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 048, Train Loss: 0.0792, Test Loss 0.2089, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 049, Train Loss: 0.0792, Test Loss 0.2089, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 000, Train Loss: 0.0868, Test Loss 0.2340, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 001, Train Loss: 0.0862, Test Loss 0.2165, Train Acc: 0.5682, Test Acc: 1.0000\n",
      "Epoch: 002, Train Loss: 0.0856, Test Loss 0.2187, Train Acc: 0.5341, Test Acc: 1.0000\n",
      "Epoch: 003, Train Loss: 0.0851, Test Loss 0.2212, Train Acc: 0.5909, Test Acc: 0.6667\n",
      "Epoch: 004, Train Loss: 0.0845, Test Loss 0.2189, Train Acc: 0.6364, Test Acc: 0.6667\n",
      "Epoch: 005, Train Loss: 0.0838, Test Loss 0.2228, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 006, Train Loss: 0.0830, Test Loss 0.2043, Train Acc: 0.5909, Test Acc: 1.0000\n",
      "Epoch: 007, Train Loss: 0.0824, Test Loss 0.2260, Train Acc: 0.6364, Test Acc: 0.6667\n",
      "Epoch: 008, Train Loss: 0.0802, Test Loss 0.2207, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 009, Train Loss: 0.0771, Test Loss 0.2065, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 010, Train Loss: 0.0742, Test Loss 0.2101, Train Acc: 0.6477, Test Acc: 0.6667\n",
      "Epoch: 011, Train Loss: 0.0768, Test Loss 0.2059, Train Acc: 0.8068, Test Acc: 0.6667\n",
      "Epoch: 012, Train Loss: 0.0765, Test Loss 0.2093, Train Acc: 0.8182, Test Acc: 0.6667\n",
      "Epoch: 013, Train Loss: 0.0759, Test Loss 0.2012, Train Acc: 0.7955, Test Acc: 0.6667\n",
      "Epoch: 014, Train Loss: 0.0751, Test Loss 0.2021, Train Acc: 0.7955, Test Acc: 0.6667\n",
      "Epoch: 015, Train Loss: 0.0746, Test Loss 0.2096, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 016, Train Loss: 0.0736, Test Loss 0.1971, Train Acc: 0.7955, Test Acc: 0.6667\n",
      "Epoch: 017, Train Loss: 0.0729, Test Loss 0.1985, Train Acc: 0.8068, Test Acc: 0.6667\n",
      "Epoch: 018, Train Loss: 0.0724, Test Loss 0.1930, Train Acc: 0.7955, Test Acc: 0.6667\n",
      "Epoch: 019, Train Loss: 0.0718, Test Loss 0.1978, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 020, Train Loss: 0.0714, Test Loss 0.1983, Train Acc: 0.8182, Test Acc: 0.6667\n",
      "Epoch: 021, Train Loss: 0.0708, Test Loss 0.1967, Train Acc: 0.8409, Test Acc: 0.6667\n",
      "Epoch: 022, Train Loss: 0.0704, Test Loss 0.1847, Train Acc: 0.8182, Test Acc: 1.0000\n",
      "Epoch: 023, Train Loss: 0.0702, Test Loss 0.2003, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 024, Train Loss: 0.0700, Test Loss 0.1981, Train Acc: 0.8068, Test Acc: 0.6667\n",
      "Epoch: 025, Train Loss: 0.0699, Test Loss 0.1957, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 026, Train Loss: 0.0698, Test Loss 0.1949, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 027, Train Loss: 0.0697, Test Loss 0.1929, Train Acc: 0.8409, Test Acc: 0.6667\n",
      "Epoch: 028, Train Loss: 0.0696, Test Loss 0.1910, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 029, Train Loss: 0.0695, Test Loss 0.1904, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 030, Train Loss: 0.0695, Test Loss 0.1901, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 031, Train Loss: 0.0694, Test Loss 0.1897, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 032, Train Loss: 0.0694, Test Loss 0.1892, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 033, Train Loss: 0.0693, Test Loss 0.1904, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 034, Train Loss: 0.0693, Test Loss 0.1898, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 035, Train Loss: 0.0692, Test Loss 0.1891, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 036, Train Loss: 0.0692, Test Loss 0.1890, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 037, Train Loss: 0.0692, Test Loss 0.1889, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 038, Train Loss: 0.0692, Test Loss 0.1888, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 039, Train Loss: 0.0692, Test Loss 0.1889, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 040, Train Loss: 0.0692, Test Loss 0.1888, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 041, Train Loss: 0.0692, Test Loss 0.1889, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 042, Train Loss: 0.0692, Test Loss 0.1888, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 043, Train Loss: 0.0692, Test Loss 0.1887, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 044, Train Loss: 0.0692, Test Loss 0.1888, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 045, Train Loss: 0.0692, Test Loss 0.1887, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 046, Train Loss: 0.0692, Test Loss 0.1887, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 047, Train Loss: 0.0692, Test Loss 0.1887, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 048, Train Loss: 0.0692, Test Loss 0.1887, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 049, Train Loss: 0.0692, Test Loss 0.1887, Train Acc: 0.8295, Test Acc: 0.6667\n",
      "Epoch: 000, Train Loss: 0.0867, Test Loss 0.2148, Train Acc: 0.5114, Test Acc: 0.6667\n",
      "Epoch: 001, Train Loss: 0.0863, Test Loss 0.2289, Train Acc: 0.5227, Test Acc: 0.6667\n",
      "Epoch: 002, Train Loss: 0.0861, Test Loss 0.2261, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 003, Train Loss: 0.0860, Test Loss 0.2272, Train Acc: 0.5795, Test Acc: 0.6667\n",
      "Epoch: 004, Train Loss: 0.0855, Test Loss 0.2263, Train Acc: 0.5682, Test Acc: 0.6667\n",
      "Epoch: 005, Train Loss: 0.0851, Test Loss 0.2175, Train Acc: 0.6250, Test Acc: 1.0000\n",
      "Epoch: 006, Train Loss: 0.0851, Test Loss 0.2322, Train Acc: 0.5795, Test Acc: 0.3333\n",
      "Epoch: 007, Train Loss: 0.0844, Test Loss 0.2196, Train Acc: 0.6477, Test Acc: 0.6667\n",
      "Epoch: 008, Train Loss: 0.0841, Test Loss 0.2233, Train Acc: 0.7045, Test Acc: 0.6667\n",
      "Epoch: 009, Train Loss: 0.0836, Test Loss 0.2199, Train Acc: 0.6932, Test Acc: 0.6667\n",
      "Epoch: 010, Train Loss: 0.0837, Test Loss 0.2047, Train Acc: 0.5568, Test Acc: 0.6667\n",
      "Epoch: 011, Train Loss: 0.0824, Test Loss 0.2200, Train Acc: 0.6932, Test Acc: 0.6667\n",
      "Epoch: 012, Train Loss: 0.0821, Test Loss 0.2156, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 013, Train Loss: 0.0811, Test Loss 0.2128, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 014, Train Loss: 0.0793, Test Loss 0.2122, Train Acc: 0.6477, Test Acc: 0.6667\n",
      "Epoch: 015, Train Loss: 0.0786, Test Loss 0.2125, Train Acc: 0.6591, Test Acc: 0.6667\n",
      "Epoch: 016, Train Loss: 0.0779, Test Loss 0.2056, Train Acc: 0.7045, Test Acc: 0.6667\n",
      "Epoch: 017, Train Loss: 0.0773, Test Loss 0.2081, Train Acc: 0.6932, Test Acc: 0.6667\n",
      "Epoch: 018, Train Loss: 0.0769, Test Loss 0.1949, Train Acc: 0.7614, Test Acc: 1.0000\n",
      "Epoch: 019, Train Loss: 0.0758, Test Loss 0.2038, Train Acc: 0.7159, Test Acc: 0.6667\n",
      "Epoch: 020, Train Loss: 0.0752, Test Loss 0.1948, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 021, Train Loss: 0.0749, Test Loss 0.2101, Train Acc: 0.6477, Test Acc: 0.6667\n",
      "Epoch: 022, Train Loss: 0.0742, Test Loss 0.1874, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 023, Train Loss: 0.0733, Test Loss 0.1908, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 024, Train Loss: 0.0732, Test Loss 0.1912, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 025, Train Loss: 0.0731, Test Loss 0.1935, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 026, Train Loss: 0.0730, Test Loss 0.1933, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 027, Train Loss: 0.0730, Test Loss 0.1936, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 028, Train Loss: 0.0729, Test Loss 0.1935, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 029, Train Loss: 0.0729, Test Loss 0.1924, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 030, Train Loss: 0.0728, Test Loss 0.1920, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 031, Train Loss: 0.0727, Test Loss 0.1929, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 032, Train Loss: 0.0727, Test Loss 0.1932, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 033, Train Loss: 0.0726, Test Loss 0.1930, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 034, Train Loss: 0.0726, Test Loss 0.1920, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 035, Train Loss: 0.0725, Test Loss 0.1927, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 036, Train Loss: 0.0725, Test Loss 0.1927, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 037, Train Loss: 0.0725, Test Loss 0.1927, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 038, Train Loss: 0.0725, Test Loss 0.1928, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 039, Train Loss: 0.0725, Test Loss 0.1927, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 040, Train Loss: 0.0725, Test Loss 0.1927, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 041, Train Loss: 0.0725, Test Loss 0.1927, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 042, Train Loss: 0.0725, Test Loss 0.1927, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 043, Train Loss: 0.0725, Test Loss 0.1926, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 044, Train Loss: 0.0725, Test Loss 0.1926, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 045, Train Loss: 0.0725, Test Loss 0.1926, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 046, Train Loss: 0.0725, Test Loss 0.1926, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 047, Train Loss: 0.0725, Test Loss 0.1925, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 048, Train Loss: 0.0725, Test Loss 0.1925, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 049, Train Loss: 0.0725, Test Loss 0.1925, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 000, Train Loss: 0.0870, Test Loss 0.2216, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 001, Train Loss: 0.0865, Test Loss 0.2261, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 002, Train Loss: 0.0863, Test Loss 0.2395, Train Acc: 0.5455, Test Acc: 0.0000\n",
      "Epoch: 003, Train Loss: 0.0859, Test Loss 0.2246, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 004, Train Loss: 0.0857, Test Loss 0.2182, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 005, Train Loss: 0.0854, Test Loss 0.2493, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 006, Train Loss: 0.0859, Test Loss 0.2113, Train Acc: 0.5227, Test Acc: 0.6667\n",
      "Epoch: 007, Train Loss: 0.0844, Test Loss 0.2199, Train Acc: 0.5682, Test Acc: 0.6667\n",
      "Epoch: 008, Train Loss: 0.0834, Test Loss 0.2306, Train Acc: 0.6364, Test Acc: 0.3333\n",
      "Epoch: 009, Train Loss: 0.0823, Test Loss 0.2290, Train Acc: 0.6591, Test Acc: 0.6667\n",
      "Epoch: 010, Train Loss: 0.0814, Test Loss 0.2003, Train Acc: 0.6136, Test Acc: 0.6667\n",
      "Epoch: 011, Train Loss: 0.0804, Test Loss 0.2368, Train Acc: 0.5795, Test Acc: 0.3333\n",
      "Epoch: 012, Train Loss: 0.0793, Test Loss 0.2267, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 013, Train Loss: 0.0786, Test Loss 0.2153, Train Acc: 0.7045, Test Acc: 0.6667\n",
      "Epoch: 014, Train Loss: 0.0782, Test Loss 0.2060, Train Acc: 0.7614, Test Acc: 1.0000\n",
      "Epoch: 015, Train Loss: 0.0782, Test Loss 0.1991, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 016, Train Loss: 0.0776, Test Loss 0.2053, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 017, Train Loss: 0.0774, Test Loss 0.2108, Train Acc: 0.7045, Test Acc: 0.6667\n",
      "Epoch: 018, Train Loss: 0.0771, Test Loss 0.2098, Train Acc: 0.7045, Test Acc: 0.6667\n",
      "Epoch: 019, Train Loss: 0.0768, Test Loss 0.1988, Train Acc: 0.7614, Test Acc: 1.0000\n",
      "Epoch: 020, Train Loss: 0.0764, Test Loss 0.2039, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 021, Train Loss: 0.0761, Test Loss 0.2011, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 022, Train Loss: 0.0759, Test Loss 0.2064, Train Acc: 0.6818, Test Acc: 0.6667\n",
      "Epoch: 023, Train Loss: 0.0756, Test Loss 0.1975, Train Acc: 0.7727, Test Acc: 1.0000\n",
      "Epoch: 024, Train Loss: 0.0755, Test Loss 0.1982, Train Acc: 0.7614, Test Acc: 1.0000\n",
      "Epoch: 025, Train Loss: 0.0755, Test Loss 0.1984, Train Acc: 0.7614, Test Acc: 1.0000\n",
      "Epoch: 026, Train Loss: 0.0755, Test Loss 0.1984, Train Acc: 0.7614, Test Acc: 1.0000\n",
      "Epoch: 027, Train Loss: 0.0754, Test Loss 0.1990, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 028, Train Loss: 0.0754, Test Loss 0.1991, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 029, Train Loss: 0.0754, Test Loss 0.1994, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 030, Train Loss: 0.0753, Test Loss 0.2002, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 031, Train Loss: 0.0753, Test Loss 0.2002, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 032, Train Loss: 0.0753, Test Loss 0.2002, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 033, Train Loss: 0.0752, Test Loss 0.2000, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 034, Train Loss: 0.0752, Test Loss 0.1996, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 035, Train Loss: 0.0752, Test Loss 0.1993, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 036, Train Loss: 0.0752, Test Loss 0.1993, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 037, Train Loss: 0.0752, Test Loss 0.1994, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 038, Train Loss: 0.0752, Test Loss 0.1994, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 039, Train Loss: 0.0752, Test Loss 0.1994, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 040, Train Loss: 0.0752, Test Loss 0.1994, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 041, Train Loss: 0.0752, Test Loss 0.1994, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 042, Train Loss: 0.0752, Test Loss 0.1994, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 043, Train Loss: 0.0752, Test Loss 0.1994, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 044, Train Loss: 0.0752, Test Loss 0.1995, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 045, Train Loss: 0.0752, Test Loss 0.1994, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 046, Train Loss: 0.0752, Test Loss 0.1994, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 047, Train Loss: 0.0752, Test Loss 0.1994, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 048, Train Loss: 0.0752, Test Loss 0.1994, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 049, Train Loss: 0.0752, Test Loss 0.1994, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 000, Train Loss: 0.0910, Test Loss 0.2869, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 001, Train Loss: 0.0864, Test Loss 0.2362, Train Acc: 0.5909, Test Acc: 0.0000\n",
      "Epoch: 002, Train Loss: 0.0862, Test Loss 0.2256, Train Acc: 0.5114, Test Acc: 0.6667\n",
      "Epoch: 003, Train Loss: 0.0856, Test Loss 0.2410, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 004, Train Loss: 0.0851, Test Loss 0.2317, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 005, Train Loss: 0.0840, Test Loss 0.2217, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 006, Train Loss: 0.0823, Test Loss 0.2269, Train Acc: 0.5114, Test Acc: 0.3333\n",
      "Epoch: 007, Train Loss: 0.0818, Test Loss 0.2088, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "Epoch: 008, Train Loss: 0.0834, Test Loss 0.2110, Train Acc: 0.5682, Test Acc: 0.6667\n",
      "Epoch: 009, Train Loss: 0.0823, Test Loss 0.2382, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 010, Train Loss: 0.0828, Test Loss 0.1971, Train Acc: 0.5682, Test Acc: 0.6667\n",
      "Epoch: 011, Train Loss: 0.0820, Test Loss 0.2443, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 012, Train Loss: 0.0799, Test Loss 0.2295, Train Acc: 0.5341, Test Acc: 0.3333\n",
      "Epoch: 013, Train Loss: 0.0784, Test Loss 0.2128, Train Acc: 0.6932, Test Acc: 0.6667\n",
      "Epoch: 014, Train Loss: 0.0779, Test Loss 0.2018, Train Acc: 0.7727, Test Acc: 1.0000\n",
      "Epoch: 015, Train Loss: 0.0775, Test Loss 0.2041, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 016, Train Loss: 0.0773, Test Loss 0.2031, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 017, Train Loss: 0.0769, Test Loss 0.1984, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 018, Train Loss: 0.0766, Test Loss 0.1946, Train Acc: 0.7614, Test Acc: 1.0000\n",
      "Epoch: 019, Train Loss: 0.0763, Test Loss 0.1918, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 020, Train Loss: 0.0758, Test Loss 0.1945, Train Acc: 0.7614, Test Acc: 1.0000\n",
      "Epoch: 021, Train Loss: 0.0750, Test Loss 0.1909, Train Acc: 0.7614, Test Acc: 1.0000\n",
      "Epoch: 022, Train Loss: 0.0742, Test Loss 0.1907, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 023, Train Loss: 0.0735, Test Loss 0.1821, Train Acc: 0.7614, Test Acc: 1.0000\n",
      "Epoch: 024, Train Loss: 0.0735, Test Loss 0.1813, Train Acc: 0.7614, Test Acc: 1.0000\n",
      "Epoch: 025, Train Loss: 0.0734, Test Loss 0.1816, Train Acc: 0.7614, Test Acc: 1.0000\n",
      "Epoch: 026, Train Loss: 0.0733, Test Loss 0.1822, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 027, Train Loss: 0.0732, Test Loss 0.1822, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 028, Train Loss: 0.0732, Test Loss 0.1828, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 029, Train Loss: 0.0731, Test Loss 0.1832, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 030, Train Loss: 0.0731, Test Loss 0.1834, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 031, Train Loss: 0.0730, Test Loss 0.1835, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 032, Train Loss: 0.0730, Test Loss 0.1839, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 033, Train Loss: 0.0729, Test Loss 0.1837, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 034, Train Loss: 0.0729, Test Loss 0.1831, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 035, Train Loss: 0.0728, Test Loss 0.1827, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 036, Train Loss: 0.0728, Test Loss 0.1827, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 037, Train Loss: 0.0728, Test Loss 0.1827, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 038, Train Loss: 0.0728, Test Loss 0.1827, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 039, Train Loss: 0.0728, Test Loss 0.1827, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 040, Train Loss: 0.0728, Test Loss 0.1827, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 041, Train Loss: 0.0728, Test Loss 0.1827, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 042, Train Loss: 0.0728, Test Loss 0.1827, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 043, Train Loss: 0.0728, Test Loss 0.1826, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 044, Train Loss: 0.0728, Test Loss 0.1827, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 045, Train Loss: 0.0728, Test Loss 0.1827, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 046, Train Loss: 0.0728, Test Loss 0.1826, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 047, Train Loss: 0.0728, Test Loss 0.1826, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 048, Train Loss: 0.0728, Test Loss 0.1826, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 049, Train Loss: 0.0728, Test Loss 0.1826, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 000, Train Loss: 0.0865, Test Loss 0.2278, Train Acc: 0.5227, Test Acc: 0.6667\n",
      "Epoch: 001, Train Loss: 0.0865, Test Loss 0.2294, Train Acc: 0.5568, Test Acc: 1.0000\n",
      "Epoch: 002, Train Loss: 0.0864, Test Loss 0.2289, Train Acc: 0.5682, Test Acc: 1.0000\n",
      "Epoch: 003, Train Loss: 0.0862, Test Loss 0.2281, Train Acc: 0.5909, Test Acc: 1.0000\n",
      "Epoch: 004, Train Loss: 0.0860, Test Loss 0.2250, Train Acc: 0.5341, Test Acc: 0.6667\n",
      "Epoch: 005, Train Loss: 0.0857, Test Loss 0.2173, Train Acc: 0.5227, Test Acc: 0.6667\n",
      "Epoch: 006, Train Loss: 0.0862, Test Loss 0.2303, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 007, Train Loss: 0.0860, Test Loss 0.2197, Train Acc: 0.5227, Test Acc: 0.6667\n",
      "Epoch: 008, Train Loss: 0.0855, Test Loss 0.2192, Train Acc: 0.5227, Test Acc: 0.6667\n",
      "Epoch: 009, Train Loss: 0.0861, Test Loss 0.2300, Train Acc: 0.6477, Test Acc: 0.6667\n",
      "Epoch: 010, Train Loss: 0.0859, Test Loss 0.2240, Train Acc: 0.5227, Test Acc: 0.6667\n",
      "Epoch: 011, Train Loss: 0.0851, Test Loss 0.2225, Train Acc: 0.6023, Test Acc: 1.0000\n",
      "Epoch: 012, Train Loss: 0.0850, Test Loss 0.2242, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 013, Train Loss: 0.0848, Test Loss 0.2242, Train Acc: 0.7045, Test Acc: 0.6667\n",
      "Epoch: 014, Train Loss: 0.0847, Test Loss 0.2257, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 015, Train Loss: 0.0846, Test Loss 0.2261, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 016, Train Loss: 0.0845, Test Loss 0.2247, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 017, Train Loss: 0.0843, Test Loss 0.2260, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 018, Train Loss: 0.0842, Test Loss 0.2247, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 019, Train Loss: 0.0841, Test Loss 0.2262, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 020, Train Loss: 0.0839, Test Loss 0.2247, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 021, Train Loss: 0.0838, Test Loss 0.2252, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 022, Train Loss: 0.0837, Test Loss 0.2226, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 023, Train Loss: 0.0836, Test Loss 0.2235, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 024, Train Loss: 0.0836, Test Loss 0.2232, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 025, Train Loss: 0.0836, Test Loss 0.2231, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 026, Train Loss: 0.0836, Test Loss 0.2232, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 027, Train Loss: 0.0836, Test Loss 0.2229, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 028, Train Loss: 0.0836, Test Loss 0.2228, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 029, Train Loss: 0.0836, Test Loss 0.2229, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 030, Train Loss: 0.0835, Test Loss 0.2230, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 031, Train Loss: 0.0835, Test Loss 0.2228, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 032, Train Loss: 0.0835, Test Loss 0.2228, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 033, Train Loss: 0.0835, Test Loss 0.2230, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 034, Train Loss: 0.0835, Test Loss 0.2227, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 035, Train Loss: 0.0835, Test Loss 0.2227, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 036, Train Loss: 0.0835, Test Loss 0.2227, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 037, Train Loss: 0.0835, Test Loss 0.2227, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 038, Train Loss: 0.0835, Test Loss 0.2227, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 039, Train Loss: 0.0835, Test Loss 0.2227, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 040, Train Loss: 0.0835, Test Loss 0.2227, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 041, Train Loss: 0.0835, Test Loss 0.2227, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 042, Train Loss: 0.0835, Test Loss 0.2227, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 043, Train Loss: 0.0835, Test Loss 0.2227, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 044, Train Loss: 0.0835, Test Loss 0.2227, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 045, Train Loss: 0.0835, Test Loss 0.2227, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 046, Train Loss: 0.0835, Test Loss 0.2227, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 047, Train Loss: 0.0835, Test Loss 0.2227, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 048, Train Loss: 0.0835, Test Loss 0.2227, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 049, Train Loss: 0.0835, Test Loss 0.2227, Train Acc: 0.7273, Test Acc: 0.6667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "eval_metrics = np.zeros((skf.n_splits, 3))\n",
    "\n",
    "labels = [full_dataset[i].y for i in range(len(full_dataset))]\n",
    "\n",
    "\n",
    "for n_fold, (train_idx, test_idx) in tqdm(enumerate(skf.split(labels, labels))):\n",
    "    model = GCN(full_dataset.num_features, 2, channels=[256, 32, 8], dropout=0.3).to(device())\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=50//4, gamma=0.1, last_epoch=-1, verbose=False)\n",
    "\n",
    "    train_loader_ = DataLoader(full_dataset[list(train_idx)], batch_size=8, shuffle=True)\n",
    "    test_loader_ = DataLoader(full_dataset[list(test_idx)], batch_size=8, shuffle=True)\n",
    "    min_v_loss = np.inf\n",
    "    print(n_fold)\n",
    "    pr, rc, acc = [], [], []\n",
    "    for epoch in range(50):\n",
    "        train_epoch(train_loader, model, criterion, optimizer)\n",
    "        train_loss, train_acc, _, _ = eval_epoch(train_loader, model, criterion)\n",
    "        val_loss, test_acc, _, _ = eval_epoch(val_loader, model, criterion)\n",
    "        scheduler.step()\n",
    "        print(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.4f}, Test Loss {val_loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "        #print(f'Train Prec: {train_pr:.3f}, Train Rec: {train_rc:.3f}, Test Prec: {val_pr:.3f}, Test Rec: {val_rc:.3f}')\n",
    "        #rc.append(val_rc)\n",
    "        #pr.append(val_pr)\n",
    "        acc.append(test_acc)\n",
    "        if min_v_loss > val_loss:\n",
    "            min_v_loss = val_loss\n",
    "            best_test_acc = test_acc\n",
    "\n",
    "    eval_metrics[n_fold, 0] = best_test_acc\n",
    "    eval_metrics[n_fold, 1] = np.mean(acc)\n",
    "    eval_metrics[n_fold, 2] = np.std(acc)\n",
    "### eval_metrics[n_fold, 3] ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.66666667, 0.67333333, 0.08137704],\n",
       "       [1.        , 0.72      , 0.13920409],\n",
       "       [1.        , 0.59333333, 0.18      ],\n",
       "       [1.        , 0.86666667, 0.17638342],\n",
       "       [1.        , 0.66666667, 0.11547005],\n",
       "       [1.        , 0.68666667, 0.1034945 ],\n",
       "       [1.        , 0.68      , 0.09333333],\n",
       "       [1.        , 0.85333333, 0.23247461],\n",
       "       [1.        , 0.86666667, 0.25819889],\n",
       "       [0.66666667, 0.7       , 0.1       ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9333333333333332, 0.13333333333333336)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(eval_metrics[:, 0]), np.std(eval_metrics[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7306666666666666, 0.0915641851380768)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(eval_metrics[:, 1]), np.std(eval_metrics[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b0c61b80a07460b875fa182f812bd74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Epoch: 000, Train Loss: 0.0872, Test Loss 0.2138, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 001, Train Loss: 0.0851, Test Loss 0.2342, Train Acc: 0.5909, Test Acc: 0.3333\n",
      "Epoch: 002, Train Loss: 0.0846, Test Loss 0.2182, Train Acc: 0.5227, Test Acc: 0.6667\n",
      "Epoch: 003, Train Loss: 0.0813, Test Loss 0.2223, Train Acc: 0.6591, Test Acc: 0.6667\n",
      "Epoch: 004, Train Loss: 0.0795, Test Loss 0.2135, Train Acc: 0.6818, Test Acc: 0.6667\n",
      "Epoch: 005, Train Loss: 0.0826, Test Loss 0.2691, Train Acc: 0.5455, Test Acc: 0.3333\n",
      "Epoch: 006, Train Loss: 0.0795, Test Loss 0.1939, Train Acc: 0.6364, Test Acc: 1.0000\n",
      "Epoch: 007, Train Loss: 0.0734, Test Loss 0.2276, Train Acc: 0.7045, Test Acc: 0.6667\n",
      "Epoch: 008, Train Loss: 0.0764, Test Loss 0.2646, Train Acc: 0.6477, Test Acc: 0.3333\n",
      "Epoch: 009, Train Loss: 0.0801, Test Loss 0.2904, Train Acc: 0.5909, Test Acc: 0.3333\n",
      "Epoch: 010, Train Loss: 0.0687, Test Loss 0.1715, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 011, Train Loss: 0.0715, Test Loss 0.1550, Train Acc: 0.6591, Test Acc: 0.6667\n",
      "Epoch: 012, Train Loss: 0.0637, Test Loss 0.1778, Train Acc: 0.8523, Test Acc: 0.6667\n",
      "Epoch: 013, Train Loss: 0.0630, Test Loss 0.1929, Train Acc: 0.7955, Test Acc: 0.6667\n",
      "Epoch: 014, Train Loss: 0.0625, Test Loss 0.1859, Train Acc: 0.8409, Test Acc: 0.6667\n",
      "Epoch: 015, Train Loss: 0.0632, Test Loss 0.1701, Train Acc: 0.8409, Test Acc: 0.6667\n",
      "Epoch: 016, Train Loss: 0.0617, Test Loss 0.1828, Train Acc: 0.8523, Test Acc: 0.6667\n",
      "Epoch: 017, Train Loss: 0.0615, Test Loss 0.1756, Train Acc: 0.8523, Test Acc: 0.6667\n",
      "Epoch: 018, Train Loss: 0.0609, Test Loss 0.1907, Train Acc: 0.7955, Test Acc: 0.6667\n",
      "Epoch: 019, Train Loss: 0.0609, Test Loss 0.1707, Train Acc: 0.8409, Test Acc: 0.6667\n",
      "Epoch: 020, Train Loss: 0.0608, Test Loss 0.1671, Train Acc: 0.8409, Test Acc: 0.6667\n",
      "Epoch: 021, Train Loss: 0.0597, Test Loss 0.1706, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 022, Train Loss: 0.0591, Test Loss 0.1702, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 023, Train Loss: 0.0587, Test Loss 0.1673, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 024, Train Loss: 0.0587, Test Loss 0.1670, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 025, Train Loss: 0.0586, Test Loss 0.1674, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 026, Train Loss: 0.0586, Test Loss 0.1668, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 027, Train Loss: 0.0587, Test Loss 0.1652, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 028, Train Loss: 0.0587, Test Loss 0.1649, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 029, Train Loss: 0.0586, Test Loss 0.1654, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 030, Train Loss: 0.0589, Test Loss 0.1634, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 031, Train Loss: 0.0588, Test Loss 0.1634, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 032, Train Loss: 0.0587, Test Loss 0.1638, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 033, Train Loss: 0.0584, Test Loss 0.1653, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 034, Train Loss: 0.0586, Test Loss 0.1638, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 035, Train Loss: 0.0587, Test Loss 0.1624, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 036, Train Loss: 0.0587, Test Loss 0.1625, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 037, Train Loss: 0.0587, Test Loss 0.1624, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 038, Train Loss: 0.0587, Test Loss 0.1625, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 039, Train Loss: 0.0587, Test Loss 0.1626, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 040, Train Loss: 0.0587, Test Loss 0.1624, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 041, Train Loss: 0.0587, Test Loss 0.1625, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 042, Train Loss: 0.0587, Test Loss 0.1626, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 043, Train Loss: 0.0587, Test Loss 0.1625, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 044, Train Loss: 0.0587, Test Loss 0.1624, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 045, Train Loss: 0.0587, Test Loss 0.1625, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 046, Train Loss: 0.0587, Test Loss 0.1626, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 047, Train Loss: 0.0586, Test Loss 0.1626, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 048, Train Loss: 0.0586, Test Loss 0.1626, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "Epoch: 049, Train Loss: 0.0586, Test Loss 0.1626, Train Acc: 0.8636, Test Acc: 0.6667\n",
      "1\n",
      "Epoch: 000, Train Loss: 0.0868, Test Loss 0.2403, Train Acc: 0.5341, Test Acc: 0.0000\n",
      "Epoch: 001, Train Loss: 0.0876, Test Loss 0.2195, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 002, Train Loss: 0.0865, Test Loss 0.2339, Train Acc: 0.5568, Test Acc: 0.6667\n",
      "Epoch: 003, Train Loss: 0.0863, Test Loss 0.2308, Train Acc: 0.5227, Test Acc: 0.6667\n",
      "Epoch: 004, Train Loss: 0.0863, Test Loss 0.2238, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 005, Train Loss: 0.0858, Test Loss 0.2343, Train Acc: 0.6136, Test Acc: 0.0000\n",
      "Epoch: 006, Train Loss: 0.0855, Test Loss 0.2252, Train Acc: 0.5341, Test Acc: 0.6667\n",
      "Epoch: 007, Train Loss: 0.0856, Test Loss 0.2205, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 008, Train Loss: 0.0851, Test Loss 0.2417, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 009, Train Loss: 0.0859, Test Loss 0.2139, Train Acc: 0.5114, Test Acc: 0.6667\n",
      "Epoch: 010, Train Loss: 0.0838, Test Loss 0.2309, Train Acc: 0.5682, Test Acc: 0.3333\n",
      "Epoch: 011, Train Loss: 0.0844, Test Loss 0.2097, Train Acc: 0.5568, Test Acc: 0.6667\n",
      "Epoch: 012, Train Loss: 0.0835, Test Loss 0.2101, Train Acc: 0.5682, Test Acc: 0.6667\n",
      "Epoch: 013, Train Loss: 0.0826, Test Loss 0.2116, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "Epoch: 014, Train Loss: 0.0821, Test Loss 0.2136, Train Acc: 0.7614, Test Acc: 1.0000\n",
      "Epoch: 015, Train Loss: 0.0819, Test Loss 0.2099, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 016, Train Loss: 0.0814, Test Loss 0.2078, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 017, Train Loss: 0.0810, Test Loss 0.2065, Train Acc: 0.7159, Test Acc: 0.6667\n",
      "Epoch: 018, Train Loss: 0.0807, Test Loss 0.2033, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 019, Train Loss: 0.0808, Test Loss 0.1998, Train Acc: 0.6932, Test Acc: 1.0000\n",
      "Epoch: 020, Train Loss: 0.0807, Test Loss 0.1979, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "Epoch: 021, Train Loss: 0.0798, Test Loss 0.1967, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 022, Train Loss: 0.0791, Test Loss 0.1962, Train Acc: 0.7159, Test Acc: 0.6667\n",
      "Epoch: 023, Train Loss: 0.0789, Test Loss 0.1928, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 024, Train Loss: 0.0788, Test Loss 0.1926, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 025, Train Loss: 0.0788, Test Loss 0.1925, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 026, Train Loss: 0.0786, Test Loss 0.1926, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 027, Train Loss: 0.0786, Test Loss 0.1924, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 028, Train Loss: 0.0786, Test Loss 0.1921, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 029, Train Loss: 0.0785, Test Loss 0.1917, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 030, Train Loss: 0.0785, Test Loss 0.1913, Train Acc: 0.6818, Test Acc: 1.0000\n",
      "Epoch: 031, Train Loss: 0.0785, Test Loss 0.1911, Train Acc: 0.6818, Test Acc: 1.0000\n",
      "Epoch: 032, Train Loss: 0.0784, Test Loss 0.1912, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 033, Train Loss: 0.0784, Test Loss 0.1909, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 034, Train Loss: 0.0783, Test Loss 0.1909, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 035, Train Loss: 0.0782, Test Loss 0.1906, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 036, Train Loss: 0.0782, Test Loss 0.1906, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 037, Train Loss: 0.0782, Test Loss 0.1906, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 038, Train Loss: 0.0782, Test Loss 0.1906, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 039, Train Loss: 0.0782, Test Loss 0.1906, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 040, Train Loss: 0.0782, Test Loss 0.1905, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 041, Train Loss: 0.0782, Test Loss 0.1906, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 042, Train Loss: 0.0782, Test Loss 0.1905, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 043, Train Loss: 0.0782, Test Loss 0.1905, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 044, Train Loss: 0.0782, Test Loss 0.1905, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 045, Train Loss: 0.0782, Test Loss 0.1905, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 046, Train Loss: 0.0782, Test Loss 0.1905, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 047, Train Loss: 0.0782, Test Loss 0.1905, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 048, Train Loss: 0.0782, Test Loss 0.1905, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 049, Train Loss: 0.0782, Test Loss 0.1905, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "2\n",
      "Epoch: 000, Train Loss: 0.0895, Test Loss 0.2098, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 001, Train Loss: 0.0868, Test Loss 0.2271, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 002, Train Loss: 0.0867, Test Loss 0.2283, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 003, Train Loss: 0.0866, Test Loss 0.2278, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 004, Train Loss: 0.0864, Test Loss 0.2344, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 005, Train Loss: 0.0866, Test Loss 0.2429, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 006, Train Loss: 0.0861, Test Loss 0.2278, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "Epoch: 007, Train Loss: 0.0858, Test Loss 0.2281, Train Acc: 0.6818, Test Acc: 0.6667\n",
      "Epoch: 008, Train Loss: 0.0857, Test Loss 0.2410, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 009, Train Loss: 0.0848, Test Loss 0.2272, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 010, Train Loss: 0.0842, Test Loss 0.2338, Train Acc: 0.5227, Test Acc: 0.3333\n",
      "Epoch: 011, Train Loss: 0.0836, Test Loss 0.2384, Train Acc: 0.5227, Test Acc: 0.3333\n",
      "Epoch: 012, Train Loss: 0.0833, Test Loss 0.2360, Train Acc: 0.5455, Test Acc: 0.3333\n",
      "Epoch: 013, Train Loss: 0.0830, Test Loss 0.2357, Train Acc: 0.5568, Test Acc: 0.3333\n",
      "Epoch: 014, Train Loss: 0.0821, Test Loss 0.2210, Train Acc: 0.7614, Test Acc: 1.0000\n",
      "Epoch: 015, Train Loss: 0.0820, Test Loss 0.2262, Train Acc: 0.6932, Test Acc: 0.6667\n",
      "Epoch: 016, Train Loss: 0.0824, Test Loss 0.2362, Train Acc: 0.5795, Test Acc: 0.3333\n",
      "Epoch: 017, Train Loss: 0.0817, Test Loss 0.2279, Train Acc: 0.6818, Test Acc: 0.3333\n",
      "Epoch: 018, Train Loss: 0.0816, Test Loss 0.2284, Train Acc: 0.6705, Test Acc: 0.3333\n",
      "Epoch: 019, Train Loss: 0.0811, Test Loss 0.2203, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 020, Train Loss: 0.0809, Test Loss 0.2211, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 021, Train Loss: 0.0809, Test Loss 0.2248, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 022, Train Loss: 0.0810, Test Loss 0.2293, Train Acc: 0.6705, Test Acc: 0.3333\n",
      "Epoch: 023, Train Loss: 0.0804, Test Loss 0.2214, Train Acc: 0.7159, Test Acc: 0.6667\n",
      "Epoch: 024, Train Loss: 0.0804, Test Loss 0.2214, Train Acc: 0.7159, Test Acc: 0.6667\n",
      "Epoch: 025, Train Loss: 0.0804, Test Loss 0.2212, Train Acc: 0.7159, Test Acc: 0.6667\n",
      "Epoch: 026, Train Loss: 0.0803, Test Loss 0.2207, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 027, Train Loss: 0.0803, Test Loss 0.2214, Train Acc: 0.7159, Test Acc: 0.6667\n",
      "Epoch: 028, Train Loss: 0.0803, Test Loss 0.2224, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 029, Train Loss: 0.0803, Test Loss 0.2219, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 030, Train Loss: 0.0803, Test Loss 0.2225, Train Acc: 0.7159, Test Acc: 0.6667\n",
      "Epoch: 031, Train Loss: 0.0803, Test Loss 0.2223, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 032, Train Loss: 0.0803, Test Loss 0.2219, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 033, Train Loss: 0.0802, Test Loss 0.2215, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 034, Train Loss: 0.0802, Test Loss 0.2218, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 035, Train Loss: 0.0802, Test Loss 0.2220, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 036, Train Loss: 0.0802, Test Loss 0.2221, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 037, Train Loss: 0.0802, Test Loss 0.2221, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 038, Train Loss: 0.0802, Test Loss 0.2221, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 039, Train Loss: 0.0802, Test Loss 0.2221, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 040, Train Loss: 0.0802, Test Loss 0.2221, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 041, Train Loss: 0.0802, Test Loss 0.2221, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 042, Train Loss: 0.0802, Test Loss 0.2220, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 043, Train Loss: 0.0802, Test Loss 0.2220, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 044, Train Loss: 0.0802, Test Loss 0.2220, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 045, Train Loss: 0.0802, Test Loss 0.2220, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 046, Train Loss: 0.0802, Test Loss 0.2220, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 047, Train Loss: 0.0802, Test Loss 0.2220, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 048, Train Loss: 0.0802, Test Loss 0.2220, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 049, Train Loss: 0.0802, Test Loss 0.2220, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "3\n",
      "Epoch: 000, Train Loss: 0.0949, Test Loss 0.2967, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 001, Train Loss: 0.0873, Test Loss 0.2368, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 002, Train Loss: 0.0888, Test Loss 0.1947, Train Acc: 0.5114, Test Acc: 0.6667\n",
      "Epoch: 003, Train Loss: 0.0861, Test Loss 0.2290, Train Acc: 0.5227, Test Acc: 0.3333\n",
      "Epoch: 004, Train Loss: 0.0857, Test Loss 0.2129, Train Acc: 0.5795, Test Acc: 1.0000\n",
      "Epoch: 005, Train Loss: 0.0854, Test Loss 0.2105, Train Acc: 0.5795, Test Acc: 1.0000\n",
      "Epoch: 006, Train Loss: 0.0848, Test Loss 0.2142, Train Acc: 0.6250, Test Acc: 1.0000\n",
      "Epoch: 007, Train Loss: 0.0847, Test Loss 0.2037, Train Acc: 0.5568, Test Acc: 1.0000\n",
      "Epoch: 008, Train Loss: 0.0831, Test Loss 0.2161, Train Acc: 0.5909, Test Acc: 0.6667\n",
      "Epoch: 009, Train Loss: 0.0841, Test Loss 0.1977, Train Acc: 0.5455, Test Acc: 1.0000\n",
      "Epoch: 010, Train Loss: 0.0834, Test Loss 0.2259, Train Acc: 0.6136, Test Acc: 0.3333\n",
      "Epoch: 011, Train Loss: 0.0834, Test Loss 0.1921, Train Acc: 0.5341, Test Acc: 1.0000\n",
      "Epoch: 012, Train Loss: 0.0826, Test Loss 0.1971, Train Acc: 0.5795, Test Acc: 1.0000\n",
      "Epoch: 013, Train Loss: 0.0824, Test Loss 0.1978, Train Acc: 0.5909, Test Acc: 1.0000\n",
      "Epoch: 014, Train Loss: 0.0820, Test Loss 0.2008, Train Acc: 0.6250, Test Acc: 1.0000\n",
      "Epoch: 015, Train Loss: 0.0817, Test Loss 0.2024, Train Acc: 0.6818, Test Acc: 1.0000\n",
      "Epoch: 016, Train Loss: 0.0816, Test Loss 0.2001, Train Acc: 0.6477, Test Acc: 1.0000\n",
      "Epoch: 017, Train Loss: 0.0816, Test Loss 0.1972, Train Acc: 0.6136, Test Acc: 1.0000\n",
      "Epoch: 018, Train Loss: 0.0813, Test Loss 0.1981, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "Epoch: 019, Train Loss: 0.0810, Test Loss 0.1984, Train Acc: 0.6818, Test Acc: 1.0000\n",
      "Epoch: 020, Train Loss: 0.0811, Test Loss 0.1931, Train Acc: 0.6136, Test Acc: 1.0000\n",
      "Epoch: 021, Train Loss: 0.0806, Test Loss 0.1952, Train Acc: 0.6818, Test Acc: 1.0000\n",
      "Epoch: 022, Train Loss: 0.0803, Test Loss 0.1926, Train Acc: 0.6818, Test Acc: 1.0000\n",
      "Epoch: 023, Train Loss: 0.0804, Test Loss 0.1844, Train Acc: 0.6364, Test Acc: 1.0000\n",
      "Epoch: 024, Train Loss: 0.0802, Test Loss 0.1853, Train Acc: 0.6364, Test Acc: 1.0000\n",
      "Epoch: 025, Train Loss: 0.0801, Test Loss 0.1855, Train Acc: 0.6477, Test Acc: 1.0000\n",
      "Epoch: 026, Train Loss: 0.0799, Test Loss 0.1856, Train Acc: 0.6705, Test Acc: 1.0000\n",
      "Epoch: 027, Train Loss: 0.0798, Test Loss 0.1855, Train Acc: 0.6705, Test Acc: 1.0000\n",
      "Epoch: 028, Train Loss: 0.0797, Test Loss 0.1854, Train Acc: 0.6705, Test Acc: 1.0000\n",
      "Epoch: 029, Train Loss: 0.0796, Test Loss 0.1855, Train Acc: 0.6705, Test Acc: 1.0000\n",
      "Epoch: 030, Train Loss: 0.0795, Test Loss 0.1850, Train Acc: 0.6705, Test Acc: 1.0000\n",
      "Epoch: 031, Train Loss: 0.0793, Test Loss 0.1855, Train Acc: 0.6705, Test Acc: 1.0000\n",
      "Epoch: 032, Train Loss: 0.0792, Test Loss 0.1849, Train Acc: 0.6705, Test Acc: 1.0000\n",
      "Epoch: 033, Train Loss: 0.0791, Test Loss 0.1847, Train Acc: 0.6705, Test Acc: 1.0000\n",
      "Epoch: 034, Train Loss: 0.0790, Test Loss 0.1842, Train Acc: 0.6705, Test Acc: 1.0000\n",
      "Epoch: 035, Train Loss: 0.0789, Test Loss 0.1842, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "Epoch: 036, Train Loss: 0.0789, Test Loss 0.1841, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "Epoch: 037, Train Loss: 0.0789, Test Loss 0.1841, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "Epoch: 038, Train Loss: 0.0789, Test Loss 0.1840, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "Epoch: 039, Train Loss: 0.0789, Test Loss 0.1839, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "Epoch: 040, Train Loss: 0.0788, Test Loss 0.1839, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "Epoch: 041, Train Loss: 0.0788, Test Loss 0.1838, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "Epoch: 042, Train Loss: 0.0788, Test Loss 0.1838, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "Epoch: 043, Train Loss: 0.0788, Test Loss 0.1838, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "Epoch: 044, Train Loss: 0.0788, Test Loss 0.1837, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "Epoch: 045, Train Loss: 0.0788, Test Loss 0.1837, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "Epoch: 046, Train Loss: 0.0788, Test Loss 0.1836, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "Epoch: 047, Train Loss: 0.0788, Test Loss 0.1836, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "Epoch: 048, Train Loss: 0.0788, Test Loss 0.1836, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "Epoch: 049, Train Loss: 0.0788, Test Loss 0.1836, Train Acc: 0.6591, Test Acc: 1.0000\n",
      "4\n",
      "Epoch: 000, Train Loss: 0.0866, Test Loss 0.2417, Train Acc: 0.5227, Test Acc: 0.3333\n",
      "Epoch: 001, Train Loss: 0.0850, Test Loss 0.2202, Train Acc: 0.5341, Test Acc: 0.6667\n",
      "Epoch: 002, Train Loss: 0.0835, Test Loss 0.2207, Train Acc: 0.6477, Test Acc: 0.6667\n",
      "Epoch: 003, Train Loss: 0.0826, Test Loss 0.2195, Train Acc: 0.6591, Test Acc: 0.6667\n",
      "Epoch: 004, Train Loss: 0.0855, Test Loss 0.2622, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 005, Train Loss: 0.0832, Test Loss 0.2266, Train Acc: 0.6023, Test Acc: 0.6667\n",
      "Epoch: 006, Train Loss: 0.0817, Test Loss 0.2126, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 007, Train Loss: 0.0871, Test Loss 0.1793, Train Acc: 0.5341, Test Acc: 0.6667\n",
      "Epoch: 008, Train Loss: 0.0908, Test Loss 0.3034, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 009, Train Loss: 0.0823, Test Loss 0.2126, Train Acc: 0.6136, Test Acc: 0.6667\n",
      "Epoch: 010, Train Loss: 0.0864, Test Loss 0.1866, Train Acc: 0.5341, Test Acc: 0.6667\n",
      "Epoch: 011, Train Loss: 0.0822, Test Loss 0.2368, Train Acc: 0.6250, Test Acc: 0.6667\n",
      "Epoch: 012, Train Loss: 0.0815, Test Loss 0.2314, Train Acc: 0.6591, Test Acc: 0.6667\n",
      "Epoch: 013, Train Loss: 0.0808, Test Loss 0.2246, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 014, Train Loss: 0.0802, Test Loss 0.2159, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 015, Train Loss: 0.0799, Test Loss 0.2151, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 016, Train Loss: 0.0795, Test Loss 0.2132, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 017, Train Loss: 0.0793, Test Loss 0.2135, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 018, Train Loss: 0.0790, Test Loss 0.2099, Train Acc: 0.7045, Test Acc: 0.6667\n",
      "Epoch: 019, Train Loss: 0.0787, Test Loss 0.2139, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 020, Train Loss: 0.0785, Test Loss 0.2180, Train Acc: 0.7159, Test Acc: 0.6667\n",
      "Epoch: 021, Train Loss: 0.0781, Test Loss 0.2117, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 022, Train Loss: 0.0778, Test Loss 0.2127, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 023, Train Loss: 0.0775, Test Loss 0.2121, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 024, Train Loss: 0.0775, Test Loss 0.2116, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 025, Train Loss: 0.0775, Test Loss 0.2121, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 026, Train Loss: 0.0775, Test Loss 0.2130, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 027, Train Loss: 0.0774, Test Loss 0.2127, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 028, Train Loss: 0.0774, Test Loss 0.2122, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 029, Train Loss: 0.0774, Test Loss 0.2119, Train Acc: 0.7841, Test Acc: 0.6667\n",
      "Epoch: 030, Train Loss: 0.0773, Test Loss 0.2126, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 031, Train Loss: 0.0773, Test Loss 0.2129, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 032, Train Loss: 0.0773, Test Loss 0.2124, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 033, Train Loss: 0.0772, Test Loss 0.2123, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 034, Train Loss: 0.0772, Test Loss 0.2125, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 035, Train Loss: 0.0772, Test Loss 0.2126, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 036, Train Loss: 0.0772, Test Loss 0.2126, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 037, Train Loss: 0.0772, Test Loss 0.2126, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 038, Train Loss: 0.0772, Test Loss 0.2126, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 039, Train Loss: 0.0772, Test Loss 0.2127, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 040, Train Loss: 0.0772, Test Loss 0.2126, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 041, Train Loss: 0.0772, Test Loss 0.2126, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 042, Train Loss: 0.0772, Test Loss 0.2127, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 043, Train Loss: 0.0772, Test Loss 0.2127, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 044, Train Loss: 0.0771, Test Loss 0.2126, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 045, Train Loss: 0.0771, Test Loss 0.2127, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 046, Train Loss: 0.0771, Test Loss 0.2126, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 047, Train Loss: 0.0771, Test Loss 0.2127, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 048, Train Loss: 0.0771, Test Loss 0.2127, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 049, Train Loss: 0.0771, Test Loss 0.2127, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "5\n",
      "Epoch: 000, Train Loss: 0.0864, Test Loss 0.2392, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 001, Train Loss: 0.0864, Test Loss 0.2169, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 002, Train Loss: 0.0857, Test Loss 0.2215, Train Acc: 0.5568, Test Acc: 0.6667\n",
      "Epoch: 003, Train Loss: 0.0863, Test Loss 0.2115, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 004, Train Loss: 0.0847, Test Loss 0.2298, Train Acc: 0.5568, Test Acc: 0.3333\n",
      "Epoch: 005, Train Loss: 0.0840, Test Loss 0.2143, Train Acc: 0.6250, Test Acc: 1.0000\n",
      "Epoch: 006, Train Loss: 0.0832, Test Loss 0.2042, Train Acc: 0.6364, Test Acc: 1.0000\n",
      "Epoch: 007, Train Loss: 0.0852, Test Loss 0.1925, Train Acc: 0.5227, Test Acc: 0.6667\n",
      "Epoch: 008, Train Loss: 0.0849, Test Loss 0.1939, Train Acc: 0.5341, Test Acc: 0.6667\n",
      "Epoch: 009, Train Loss: 0.0914, Test Loss 0.3045, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 010, Train Loss: 0.0824, Test Loss 0.1972, Train Acc: 0.6023, Test Acc: 1.0000\n",
      "Epoch: 011, Train Loss: 0.0855, Test Loss 0.2658, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 012, Train Loss: 0.0806, Test Loss 0.2330, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 013, Train Loss: 0.0781, Test Loss 0.1984, Train Acc: 0.7500, Test Acc: 1.0000\n",
      "Epoch: 014, Train Loss: 0.0777, Test Loss 0.1969, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 015, Train Loss: 0.0771, Test Loss 0.1929, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 016, Train Loss: 0.0765, Test Loss 0.1886, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 017, Train Loss: 0.0760, Test Loss 0.1825, Train Acc: 0.7273, Test Acc: 1.0000\n",
      "Epoch: 018, Train Loss: 0.0755, Test Loss 0.1765, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 019, Train Loss: 0.0751, Test Loss 0.1722, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 020, Train Loss: 0.0740, Test Loss 0.1727, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 021, Train Loss: 0.0751, Test Loss 0.1647, Train Acc: 0.6705, Test Acc: 1.0000\n",
      "Epoch: 022, Train Loss: 0.0737, Test Loss 0.1652, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 023, Train Loss: 0.0733, Test Loss 0.1634, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 024, Train Loss: 0.0733, Test Loss 0.1632, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 025, Train Loss: 0.0733, Test Loss 0.1629, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 026, Train Loss: 0.0733, Test Loss 0.1627, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 027, Train Loss: 0.0732, Test Loss 0.1627, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 028, Train Loss: 0.0733, Test Loss 0.1623, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 029, Train Loss: 0.0732, Test Loss 0.1622, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 030, Train Loss: 0.0734, Test Loss 0.1618, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 031, Train Loss: 0.0734, Test Loss 0.1615, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 032, Train Loss: 0.0732, Test Loss 0.1616, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 033, Train Loss: 0.0731, Test Loss 0.1615, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 034, Train Loss: 0.0730, Test Loss 0.1615, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 035, Train Loss: 0.0731, Test Loss 0.1612, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 036, Train Loss: 0.0731, Test Loss 0.1611, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 037, Train Loss: 0.0731, Test Loss 0.1611, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 038, Train Loss: 0.0731, Test Loss 0.1611, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 039, Train Loss: 0.0731, Test Loss 0.1611, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 040, Train Loss: 0.0731, Test Loss 0.1612, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 041, Train Loss: 0.0731, Test Loss 0.1611, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 042, Train Loss: 0.0731, Test Loss 0.1611, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 043, Train Loss: 0.0731, Test Loss 0.1611, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 044, Train Loss: 0.0731, Test Loss 0.1611, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 045, Train Loss: 0.0730, Test Loss 0.1611, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 046, Train Loss: 0.0730, Test Loss 0.1611, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 047, Train Loss: 0.0730, Test Loss 0.1611, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 048, Train Loss: 0.0730, Test Loss 0.1611, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 049, Train Loss: 0.0730, Test Loss 0.1611, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "6\n",
      "Epoch: 000, Train Loss: 0.0870, Test Loss 0.2201, Train Acc: 0.4886, Test Acc: 0.6667\n",
      "Epoch: 001, Train Loss: 0.0871, Test Loss 0.2289, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 002, Train Loss: 0.0862, Test Loss 0.2299, Train Acc: 0.5455, Test Acc: 0.6667\n",
      "Epoch: 003, Train Loss: 0.0867, Test Loss 0.2167, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 004, Train Loss: 0.0847, Test Loss 0.2400, Train Acc: 0.5455, Test Acc: 0.3333\n",
      "Epoch: 005, Train Loss: 0.0834, Test Loss 0.2310, Train Acc: 0.6136, Test Acc: 0.6667\n",
      "Epoch: 006, Train Loss: 0.0823, Test Loss 0.2301, Train Acc: 0.6477, Test Acc: 0.3333\n",
      "Epoch: 007, Train Loss: 0.0840, Test Loss 0.1881, Train Acc: 0.5114, Test Acc: 0.6667\n",
      "Epoch: 008, Train Loss: 0.0818, Test Loss 0.2449, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 009, Train Loss: 0.0889, Test Loss 0.1850, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 010, Train Loss: 0.0828, Test Loss 0.2392, Train Acc: 0.5795, Test Acc: 0.3333\n",
      "Epoch: 011, Train Loss: 0.0821, Test Loss 0.1974, Train Acc: 0.5227, Test Acc: 0.6667\n",
      "Epoch: 012, Train Loss: 0.0820, Test Loss 0.1966, Train Acc: 0.5227, Test Acc: 0.6667\n",
      "Epoch: 013, Train Loss: 0.0808, Test Loss 0.2012, Train Acc: 0.5795, Test Acc: 0.6667\n",
      "Epoch: 014, Train Loss: 0.0800, Test Loss 0.2063, Train Acc: 0.7727, Test Acc: 0.6667\n",
      "Epoch: 015, Train Loss: 0.0797, Test Loss 0.2071, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 016, Train Loss: 0.0794, Test Loss 0.2048, Train Acc: 0.7727, Test Acc: 1.0000\n",
      "Epoch: 017, Train Loss: 0.0792, Test Loss 0.2037, Train Acc: 0.7727, Test Acc: 1.0000\n",
      "Epoch: 018, Train Loss: 0.0789, Test Loss 0.2008, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 019, Train Loss: 0.0787, Test Loss 0.1982, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 020, Train Loss: 0.0781, Test Loss 0.2007, Train Acc: 0.8068, Test Acc: 1.0000\n",
      "Epoch: 021, Train Loss: 0.0779, Test Loss 0.1967, Train Acc: 0.8068, Test Acc: 1.0000\n",
      "Epoch: 022, Train Loss: 0.0776, Test Loss 0.1934, Train Acc: 0.8068, Test Acc: 1.0000\n",
      "Epoch: 023, Train Loss: 0.0775, Test Loss 0.1889, Train Acc: 0.7727, Test Acc: 1.0000\n",
      "Epoch: 024, Train Loss: 0.0774, Test Loss 0.1891, Train Acc: 0.7727, Test Acc: 1.0000\n",
      "Epoch: 025, Train Loss: 0.0774, Test Loss 0.1888, Train Acc: 0.7727, Test Acc: 1.0000\n",
      "Epoch: 026, Train Loss: 0.0773, Test Loss 0.1890, Train Acc: 0.7727, Test Acc: 1.0000\n",
      "Epoch: 027, Train Loss: 0.0773, Test Loss 0.1887, Train Acc: 0.7727, Test Acc: 1.0000\n",
      "Epoch: 028, Train Loss: 0.0773, Test Loss 0.1883, Train Acc: 0.7727, Test Acc: 1.0000\n",
      "Epoch: 029, Train Loss: 0.0772, Test Loss 0.1883, Train Acc: 0.7727, Test Acc: 1.0000\n",
      "Epoch: 030, Train Loss: 0.0772, Test Loss 0.1882, Train Acc: 0.7727, Test Acc: 1.0000\n",
      "Epoch: 031, Train Loss: 0.0771, Test Loss 0.1882, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 032, Train Loss: 0.0771, Test Loss 0.1880, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 033, Train Loss: 0.0771, Test Loss 0.1878, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 034, Train Loss: 0.0770, Test Loss 0.1876, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 035, Train Loss: 0.0770, Test Loss 0.1876, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 036, Train Loss: 0.0770, Test Loss 0.1876, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 037, Train Loss: 0.0769, Test Loss 0.1875, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 038, Train Loss: 0.0769, Test Loss 0.1875, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 039, Train Loss: 0.0769, Test Loss 0.1875, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 040, Train Loss: 0.0769, Test Loss 0.1875, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 041, Train Loss: 0.0769, Test Loss 0.1874, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 042, Train Loss: 0.0769, Test Loss 0.1874, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 043, Train Loss: 0.0769, Test Loss 0.1874, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 044, Train Loss: 0.0769, Test Loss 0.1874, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 045, Train Loss: 0.0769, Test Loss 0.1873, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 046, Train Loss: 0.0769, Test Loss 0.1873, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 047, Train Loss: 0.0769, Test Loss 0.1873, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 048, Train Loss: 0.0769, Test Loss 0.1873, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 049, Train Loss: 0.0769, Test Loss 0.1873, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "7\n",
      "Epoch: 000, Train Loss: 0.0949, Test Loss 0.1830, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 001, Train Loss: 0.0863, Test Loss 0.2440, Train Acc: 0.5114, Test Acc: 0.3333\n",
      "Epoch: 002, Train Loss: 0.0854, Test Loss 0.2221, Train Acc: 0.5568, Test Acc: 0.6667\n",
      "Epoch: 003, Train Loss: 0.0867, Test Loss 0.2579, Train Acc: 0.5000, Test Acc: 0.3333\n",
      "Epoch: 004, Train Loss: 0.0847, Test Loss 0.2116, Train Acc: 0.5114, Test Acc: 0.6667\n",
      "Epoch: 005, Train Loss: 0.0837, Test Loss 0.2280, Train Acc: 0.6932, Test Acc: 0.6667\n",
      "Epoch: 006, Train Loss: 0.0849, Test Loss 0.2436, Train Acc: 0.5227, Test Acc: 0.3333\n",
      "Epoch: 007, Train Loss: 0.0845, Test Loss 0.2076, Train Acc: 0.5341, Test Acc: 0.6667\n",
      "Epoch: 008, Train Loss: 0.0844, Test Loss 0.2468, Train Acc: 0.5227, Test Acc: 0.3333\n",
      "Epoch: 009, Train Loss: 0.0822, Test Loss 0.2183, Train Acc: 0.6932, Test Acc: 0.6667\n",
      "Epoch: 010, Train Loss: 0.0817, Test Loss 0.2298, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 011, Train Loss: 0.0800, Test Loss 0.2046, Train Acc: 0.6364, Test Acc: 0.6667\n",
      "Epoch: 012, Train Loss: 0.0797, Test Loss 0.2059, Train Acc: 0.6477, Test Acc: 1.0000\n",
      "Epoch: 013, Train Loss: 0.0793, Test Loss 0.2103, Train Acc: 0.7045, Test Acc: 0.6667\n",
      "Epoch: 014, Train Loss: 0.0791, Test Loss 0.2129, Train Acc: 0.6705, Test Acc: 0.6667\n",
      "Epoch: 015, Train Loss: 0.0788, Test Loss 0.2119, Train Acc: 0.6932, Test Acc: 0.6667\n",
      "Epoch: 016, Train Loss: 0.0789, Test Loss 0.2193, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 017, Train Loss: 0.0784, Test Loss 0.2079, Train Acc: 0.7159, Test Acc: 0.6667\n",
      "Epoch: 018, Train Loss: 0.0782, Test Loss 0.2069, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 019, Train Loss: 0.0780, Test Loss 0.2035, Train Acc: 0.7159, Test Acc: 0.6667\n",
      "Epoch: 020, Train Loss: 0.0778, Test Loss 0.2094, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 021, Train Loss: 0.0776, Test Loss 0.2027, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 022, Train Loss: 0.0773, Test Loss 0.2045, Train Acc: 0.7500, Test Acc: 0.6667\n",
      "Epoch: 023, Train Loss: 0.0771, Test Loss 0.2088, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 024, Train Loss: 0.0771, Test Loss 0.2082, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 025, Train Loss: 0.0771, Test Loss 0.2073, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 026, Train Loss: 0.0770, Test Loss 0.2062, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 027, Train Loss: 0.0770, Test Loss 0.2053, Train Acc: 0.7159, Test Acc: 0.6667\n",
      "Epoch: 028, Train Loss: 0.0770, Test Loss 0.2047, Train Acc: 0.7159, Test Acc: 0.6667\n",
      "Epoch: 029, Train Loss: 0.0770, Test Loss 0.2044, Train Acc: 0.7159, Test Acc: 0.6667\n",
      "Epoch: 030, Train Loss: 0.0769, Test Loss 0.2039, Train Acc: 0.7159, Test Acc: 0.6667\n",
      "Epoch: 031, Train Loss: 0.0769, Test Loss 0.2034, Train Acc: 0.7386, Test Acc: 0.6667\n",
      "Epoch: 032, Train Loss: 0.0769, Test Loss 0.2036, Train Acc: 0.7159, Test Acc: 0.6667\n",
      "Epoch: 033, Train Loss: 0.0769, Test Loss 0.2019, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 034, Train Loss: 0.0769, Test Loss 0.2019, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 035, Train Loss: 0.0769, Test Loss 0.2014, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 036, Train Loss: 0.0768, Test Loss 0.2014, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 037, Train Loss: 0.0768, Test Loss 0.2015, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 038, Train Loss: 0.0768, Test Loss 0.2014, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 039, Train Loss: 0.0768, Test Loss 0.2014, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 040, Train Loss: 0.0768, Test Loss 0.2014, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 041, Train Loss: 0.0768, Test Loss 0.2015, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 042, Train Loss: 0.0768, Test Loss 0.2014, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 043, Train Loss: 0.0768, Test Loss 0.2014, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 044, Train Loss: 0.0768, Test Loss 0.2014, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 045, Train Loss: 0.0768, Test Loss 0.2013, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 046, Train Loss: 0.0768, Test Loss 0.2013, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 047, Train Loss: 0.0768, Test Loss 0.2014, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 048, Train Loss: 0.0768, Test Loss 0.2014, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "Epoch: 049, Train Loss: 0.0768, Test Loss 0.2014, Train Acc: 0.7614, Test Acc: 0.6667\n",
      "8\n",
      "Epoch: 000, Train Loss: 0.0903, Test Loss 0.2060, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 001, Train Loss: 0.0880, Test Loss 0.2748, Train Acc: 0.4773, Test Acc: 0.3333\n",
      "Epoch: 002, Train Loss: 0.0828, Test Loss 0.2338, Train Acc: 0.6591, Test Acc: 0.3333\n",
      "Epoch: 003, Train Loss: 0.0861, Test Loss 0.1975, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 004, Train Loss: 0.0807, Test Loss 0.2373, Train Acc: 0.6136, Test Acc: 0.6667\n",
      "Epoch: 005, Train Loss: 0.0824, Test Loss 0.2593, Train Acc: 0.5227, Test Acc: 0.3333\n",
      "Epoch: 006, Train Loss: 0.0762, Test Loss 0.2062, Train Acc: 0.7273, Test Acc: 0.6667\n",
      "Epoch: 007, Train Loss: 0.0748, Test Loss 0.1800, Train Acc: 0.7614, Test Acc: 1.0000\n",
      "Epoch: 008, Train Loss: 0.0778, Test Loss 0.1640, Train Acc: 0.5909, Test Acc: 0.6667\n",
      "Epoch: 009, Train Loss: 0.0976, Test Loss 0.1683, Train Acc: 0.5227, Test Acc: 0.6667\n",
      "Epoch: 010, Train Loss: 0.0864, Test Loss 0.2791, Train Acc: 0.5114, Test Acc: 0.3333\n",
      "Epoch: 011, Train Loss: 0.0842, Test Loss 0.1581, Train Acc: 0.5682, Test Acc: 0.6667\n",
      "Epoch: 012, Train Loss: 0.0731, Test Loss 0.1593, Train Acc: 0.6818, Test Acc: 1.0000\n",
      "Epoch: 013, Train Loss: 0.0681, Test Loss 0.1666, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 014, Train Loss: 0.0675, Test Loss 0.1658, Train Acc: 0.8068, Test Acc: 1.0000\n",
      "Epoch: 015, Train Loss: 0.0671, Test Loss 0.1644, Train Acc: 0.8068, Test Acc: 1.0000\n",
      "Epoch: 016, Train Loss: 0.0683, Test Loss 0.1590, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 017, Train Loss: 0.0667, Test Loss 0.1617, Train Acc: 0.8182, Test Acc: 1.0000\n",
      "Epoch: 018, Train Loss: 0.0669, Test Loss 0.1593, Train Acc: 0.8182, Test Acc: 1.0000\n",
      "Epoch: 019, Train Loss: 0.0664, Test Loss 0.1595, Train Acc: 0.8068, Test Acc: 1.0000\n",
      "Epoch: 020, Train Loss: 0.0666, Test Loss 0.1566, Train Acc: 0.8068, Test Acc: 1.0000\n",
      "Epoch: 021, Train Loss: 0.0666, Test Loss 0.1554, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 022, Train Loss: 0.0663, Test Loss 0.1544, Train Acc: 0.8068, Test Acc: 1.0000\n",
      "Epoch: 023, Train Loss: 0.0666, Test Loss 0.1526, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 024, Train Loss: 0.0664, Test Loss 0.1527, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 025, Train Loss: 0.0664, Test Loss 0.1527, Train Acc: 0.7841, Test Acc: 1.0000\n",
      "Epoch: 026, Train Loss: 0.0662, Test Loss 0.1530, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 027, Train Loss: 0.0661, Test Loss 0.1530, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 028, Train Loss: 0.0662, Test Loss 0.1528, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 029, Train Loss: 0.0659, Test Loss 0.1531, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 030, Train Loss: 0.0660, Test Loss 0.1528, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 031, Train Loss: 0.0659, Test Loss 0.1528, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 032, Train Loss: 0.0659, Test Loss 0.1527, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 033, Train Loss: 0.0660, Test Loss 0.1526, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 034, Train Loss: 0.0658, Test Loss 0.1528, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 035, Train Loss: 0.0658, Test Loss 0.1526, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 036, Train Loss: 0.0658, Test Loss 0.1526, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 037, Train Loss: 0.0658, Test Loss 0.1526, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 038, Train Loss: 0.0658, Test Loss 0.1526, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 039, Train Loss: 0.0658, Test Loss 0.1526, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 040, Train Loss: 0.0658, Test Loss 0.1526, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 041, Train Loss: 0.0658, Test Loss 0.1526, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 042, Train Loss: 0.0658, Test Loss 0.1525, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 043, Train Loss: 0.0658, Test Loss 0.1525, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 044, Train Loss: 0.0658, Test Loss 0.1526, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 045, Train Loss: 0.0658, Test Loss 0.1525, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 046, Train Loss: 0.0658, Test Loss 0.1525, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 047, Train Loss: 0.0658, Test Loss 0.1525, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 048, Train Loss: 0.0658, Test Loss 0.1525, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "Epoch: 049, Train Loss: 0.0658, Test Loss 0.1525, Train Acc: 0.7955, Test Acc: 1.0000\n",
      "9\n",
      "Epoch: 000, Train Loss: 0.0907, Test Loss 0.2022, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 001, Train Loss: 0.0862, Test Loss 0.2312, Train Acc: 0.5568, Test Acc: 0.3333\n",
      "Epoch: 002, Train Loss: 0.0864, Test Loss 0.2203, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 003, Train Loss: 0.0861, Test Loss 0.2203, Train Acc: 0.5000, Test Acc: 0.6667\n",
      "Epoch: 004, Train Loss: 0.0855, Test Loss 0.2289, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 005, Train Loss: 0.0850, Test Loss 0.2238, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 006, Train Loss: 0.0845, Test Loss 0.2184, Train Acc: 0.6250, Test Acc: 1.0000\n",
      "Epoch: 007, Train Loss: 0.0852, Test Loss 0.2086, Train Acc: 0.5341, Test Acc: 0.6667\n",
      "Epoch: 008, Train Loss: 0.0832, Test Loss 0.2183, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 009, Train Loss: 0.0828, Test Loss 0.2092, Train Acc: 0.6477, Test Acc: 1.0000\n",
      "Epoch: 010, Train Loss: 0.0823, Test Loss 0.2006, Train Acc: 0.6250, Test Acc: 1.0000\n",
      "Epoch: 011, Train Loss: 0.0802, Test Loss 0.2000, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 012, Train Loss: 0.0803, Test Loss 0.1964, Train Acc: 0.6705, Test Acc: 1.0000\n",
      "Epoch: 013, Train Loss: 0.0800, Test Loss 0.1953, Train Acc: 0.6932, Test Acc: 1.0000\n",
      "Epoch: 014, Train Loss: 0.0794, Test Loss 0.1946, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 015, Train Loss: 0.0789, Test Loss 0.1929, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 016, Train Loss: 0.0785, Test Loss 0.1906, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 017, Train Loss: 0.0781, Test Loss 0.1857, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 018, Train Loss: 0.0774, Test Loss 0.1847, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 019, Train Loss: 0.0775, Test Loss 0.1783, Train Acc: 0.6932, Test Acc: 1.0000\n",
      "Epoch: 020, Train Loss: 0.0765, Test Loss 0.1780, Train Acc: 0.6932, Test Acc: 1.0000\n",
      "Epoch: 021, Train Loss: 0.0759, Test Loss 0.1755, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 022, Train Loss: 0.0749, Test Loss 0.1751, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 023, Train Loss: 0.0755, Test Loss 0.1685, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 024, Train Loss: 0.0753, Test Loss 0.1688, Train Acc: 0.7045, Test Acc: 1.0000\n",
      "Epoch: 025, Train Loss: 0.0753, Test Loss 0.1683, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 026, Train Loss: 0.0751, Test Loss 0.1686, Train Acc: 0.7159, Test Acc: 1.0000\n",
      "Epoch: 027, Train Loss: 0.0749, Test Loss 0.1690, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 028, Train Loss: 0.0747, Test Loss 0.1692, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 029, Train Loss: 0.0748, Test Loss 0.1687, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 030, Train Loss: 0.0746, Test Loss 0.1691, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 031, Train Loss: 0.0745, Test Loss 0.1691, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 032, Train Loss: 0.0746, Test Loss 0.1681, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 033, Train Loss: 0.0745, Test Loss 0.1681, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 034, Train Loss: 0.0745, Test Loss 0.1679, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 035, Train Loss: 0.0744, Test Loss 0.1679, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 036, Train Loss: 0.0744, Test Loss 0.1679, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 037, Train Loss: 0.0744, Test Loss 0.1679, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 038, Train Loss: 0.0744, Test Loss 0.1679, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 039, Train Loss: 0.0744, Test Loss 0.1679, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 040, Train Loss: 0.0744, Test Loss 0.1679, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 041, Train Loss: 0.0744, Test Loss 0.1678, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 042, Train Loss: 0.0744, Test Loss 0.1678, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 043, Train Loss: 0.0744, Test Loss 0.1678, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 044, Train Loss: 0.0744, Test Loss 0.1678, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 045, Train Loss: 0.0744, Test Loss 0.1678, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 046, Train Loss: 0.0744, Test Loss 0.1677, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 047, Train Loss: 0.0744, Test Loss 0.1677, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 048, Train Loss: 0.0744, Test Loss 0.1677, Train Acc: 0.7386, Test Acc: 1.0000\n",
      "Epoch: 049, Train Loss: 0.0744, Test Loss 0.1677, Train Acc: 0.7386, Test Acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "eval_metrics = np.zeros((skf.n_splits, 3))\n",
    "\n",
    "labels = [full_dataset[i].y for i in range(len(full_dataset))]\n",
    "\n",
    "\n",
    "for n_fold, (train_idx, test_idx) in tqdm(enumerate(skf.split(labels, labels))):\n",
    "    model = GATv2(full_dataset.num_features, 128, 8).to(device())\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=50//4, gamma=0.1, last_epoch=-1, verbose=False)\n",
    "\n",
    "    train_loader_ = DataLoader(full_dataset[list(train_idx)], batch_size=8, shuffle=True)\n",
    "    test_loader_ = DataLoader(full_dataset[list(test_idx)], batch_size=8, shuffle=True)\n",
    "    min_v_loss = np.inf\n",
    "    print(n_fold)\n",
    "    pr, rc, acc = [], [], []\n",
    "    for epoch in range(50):\n",
    "        train_epoch(train_loader, model, criterion, optimizer)\n",
    "        train_loss, train_acc, _, _ = eval_epoch(train_loader, model, criterion)\n",
    "        val_loss, test_acc, _, _ = eval_epoch(val_loader, model, criterion)\n",
    "        scheduler.step()\n",
    "        print(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.4f}, Test Loss {val_loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "        #print(f'Train Prec: {train_pr:.3f}, Train Rec: {train_rc:.3f}, Test Prec: {val_pr:.3f}, Test Rec: {val_rc:.3f}')\n",
    "        #rc.append(val_rc)\n",
    "        #pr.append(val_pr)\n",
    "        acc.append(test_acc)\n",
    "        if min_v_loss > val_loss:\n",
    "            min_v_loss = val_loss\n",
    "            best_test_acc = test_acc\n",
    "\n",
    "    eval_metrics[n_fold, 0] = best_test_acc\n",
    "    eval_metrics[n_fold, 1] = np.mean(acc)\n",
    "    eval_metrics[n_fold, 2] = np.std(acc)\n",
    "### eval_metrics[n_fold, 3] ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.66666667, 0.65333333, 0.11469767],\n",
       "       [1.        , 0.85333333, 0.25086517],\n",
       "       [0.66666667, 0.61333333, 0.16812694],\n",
       "       [1.        , 0.93333333, 0.18856181],\n",
       "       [0.66666667, 0.64666667, 0.07916228],\n",
       "       [1.        , 0.89333333, 0.21540659],\n",
       "       [0.66666667, 0.87333333, 0.2096558 ],\n",
       "       [0.66666667, 0.64666667, 0.1034945 ],\n",
       "       [1.        , 0.9       , 0.20275875],\n",
       "       [1.        , 0.96      , 0.12719189]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8333333333333333, 0.16666666666666669)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(eval_metrics[:, 0]), np.std(eval_metrics[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7973333333333333, 0.13176409897152477)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(eval_metrics[:, 1]), np.std(eval_metrics[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from captum.attr import Saliency, IntegratedGradients\n",
    "\n",
    "def model_forward(edge_mask, data):\n",
    "    batch = torch.zeros(data.x.shape[0], dtype=int).to(device())\n",
    "    out = model(data) # .x, data.edge_index, batch, edge_mask\n",
    "    return out\n",
    "\n",
    "\n",
    "def explain(method, data, target=0):\n",
    "    input_mask = torch.ones(data.edge_index.shape[1]).requires_grad_(True).to(device)\n",
    "    if method == 'ig':\n",
    "        ig = IntegratedGradients(model_forward)\n",
    "        mask = ig.attribute(input_mask, target=target,\n",
    "                            additional_forward_args=(data,),\n",
    "                            internal_batch_size=data.edge_index.shape[1])\n",
    "    elif method == 'saliency':\n",
    "        saliency = Saliency(model_forward)\n",
    "        mask = saliency.attribute(input_mask, target=target,\n",
    "                                  additional_forward_args=(data,))\n",
    "    else:\n",
    "        raise Exception('Unknown explanation method')\n",
    "\n",
    "    edge_mask = np.abs(mask.cpu().detach().numpy())\n",
    "    if edge_mask.max() > 0:  # avoid division by zero\n",
    "        edge_mask = edge_mask / edge_mask.max()\n",
    "    return edge_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Open_Close_GNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "266da1a1721e4588a8feaf69449a5145239fab947ce8997260eacc776b5422ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
